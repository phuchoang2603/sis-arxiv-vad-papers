<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/aligning-effective-tokens-with-video-anomaly-in-large-language-models/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/aligning-effective-tokens-with-video-anomaly-in-large-language-models/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/aligning-effective-tokens-with-video-anomaly-in-large-language-models\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "8317"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>8317 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">40 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">Aligning Effective Tokens with Video Anomaly in Large Language Models
    <div id="aligning-effective-tokens-with-video-anomaly-in-large-language-models" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#aligning-effective-tokens-with-video-anomaly-in-large-language-models" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Yingxian Chen 1∗ Jiahui Liu 1∗ Ruidi Fan 1 Yanwei Li 2 Chirui Chang 1 Shizhen Zhao 1 Wilton W.T.Fok 1 Xiaojuan Qi 1† Yik-Chung Wu 1† 1 2</p>
<p>The University of Hong Kong The Chinese University of Hong Kong</p>
<p>{chenyx, liujh, xjqi, <a
  href="mailto:ycwu%7d@eee.hku.hk">ycwu}@eee.hku.hk</a></p>
<p>∗ equal contributions †
c †
corresponding authors</p>

<h2 class="relative group">Abstract
    <div id="abstract" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#abstract" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Understanding abnormal events in videos is a vital and challenging task that has garnered significant attention in a wide range of applications. Although current video understanding Multi-modal Large Language Models (MLLMs) are capable of analyzing general videos, they often struggle to handle anomalies due to the spatial and temporal sparsity of abnormal events, where the redundant information always leads to suboptimal outcomes. To address these challenges, exploiting the representation and generalization capabilities of Vison Language Models (VLMs) and Large Language Models (LLMs), we propose VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in various videos. Our approach efficiently aligns effective tokens between visual encoders and LLMs through two key proposed modules: Spatial Effective Token Selection (SETS) and Temporal Effective Token Generation (TETG). These modules enable our model to effectively capture and analyze both spatial and temporal information associated with abnormal events, resulting in more accurate responses and interactions. Furthermore, we construct an instruction-following dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed method outperforms existing state-of-the-art methods on various benchmarks.</p>

<h2 class="relative group">1. Introduction
    <div id="1-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Detecting and summarizing abnormal events in videos is critical and challenging, and it has garnered considerable attention across multiple research domains and real-world applications, such as security monitoring, video analysis, and crime detection.</p>
<p>Although many traditional methods [4 , 16 , 37 , 43 , 47 , 73 , 86] have been widely explored for video anomaly detection, they exhibit substantial limitations in their effectiveness [22 , 51 , 60 , 66 , 71 , 84]. These limitations manifest in two aspects: 1) Traditional video anomaly detection</p>
<p>Figure 1. Baseline video understanding MLLM feeds forward every visual token (yellow squares) equally to participate in fine-tuning and inference (top row). Different from it, our method focuses on the effective area (unobstructed area in medium video frames) in each frame and select the Spatial Effective Tokens (orange squares) for the LLM (see Section 3.2) (filtered tokens are shown as gray squares). At the same time, we generate anomaly-aware Temporal Effective Tokens (green squares) (see Section 3.3) based on the assigned anomaly scores (denoted as s) of each frame from a pretrained classifier for better temporal localization of anomalies.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_fc678031b8a7251fd6bb4962023d6f9c8116acb1c77f521e2692f2ba7f4ee6f4.png"
    ></figure>
<p>methods [6 , 13 , 57 , 63 , 64 , 86] essentially approach the task as a closed-set detection and classification problem, inherently limiting their ability to achieve a comprehensive understanding and interpretation of anomalies; 2) These methods [2 , 19 , 23 , 65 , 68 , 78] are restricted by a limited vocabulary, making it difficult for them to handle unseen or novel situations effectively.</p>
<p>Recent advancements [1 , 28 , 34 , 36 , 45 , 58] in Vision Language Models (VLMs) and Large Language Models (LLMs) have demonstrated remarkable capabilities in scene understanding and comprehensive analysis. Multimodal Large Language Models (MLLMs), particularly those designed for video understanding [27 , 29 , 31 , 39 , 42 , 74], have achieved significant progress in general video analysis tasks. However, while these models exhibit strong performance in general video understanding, they fall short in accurately detecting and interpreting anomalies.</p>
<p>For mitigating the above challenges, some works [12 , 55 , 67 , 75 , 79] proposed anomaly-aware video MLLMs to better understand the anomalies in videos. Although these models work well for detecting obvious abnormal events, such as fighting or fire, they typically struggle to effectively align abnormal regions with relevant captions which requires addressing spatial redundancy, and accurately identifying abnormal time intervals by mitigating temporal redundancy. This is because these methods treat all latent tokens with equal priority across spatial and temporal dimensions. This leads to performance degradation caused by redundant tokens unrelated to anomalies. However, in most cases, only small regions within a few frames contain the essential information help to identify an anomaly (as shown in Figure 1). Thus, we explore: How can multimodal architectures evolve selective token generation and processing mechanisms to dynamically prioritize anomaly-salient information while preserving comprehensive scene understanding capabilities?</p>
<p>To address the aforementioned issues, we propose a new model named VA-GPT for analyzing various Videos for Abnormal events by aligning effective and accurate tokens with LLMs across both spatial and temporal dimensions. VA-GPT integrates two key components to identify effective visual tokens for alignment while eliminating redundant tokens that could hinder anomaly analysis and distract model from extracting useful information: 1) we develop the Spatial Effective Token Selection (SETS) module for identifying tokens corresponding to regions with challenges for aligning them with LLMs, while filtering out tokens associated with minor dynamics to remove redundancy. This is because we find that abnormal events often result in different visual changes and variations in local areas (see Figure 1); and 2) we propose the Temporal Effective Token Generation (TETG) module which employs a lightweight pre-trained classifier to assign a confidence score to each frame indicating the possibility of containing abnormal events. Then TETG generates efficient tokens with prior knowledge of the temporal information of abnormal events directly in the language space as additional input to the LLMs, effectively enhancing the model&rsquo;s temporal reasoning and understanding abilities about abnormal events.</p>
<p>Furthermore, beyond conventional benchmarks (indomain benchmark), we establish a new cross-domain evaluation protocol that systematically evaluates model robustness with domain shifts. Based on a novel video dataset, XD-Violence [64], we design comprehensive QAs about abnormal events which include different visual contents from our training data and integrate it as a new cross-domain benchmark. Meanwhile, we design temporal-informationoriented QAs on both in- and cross- domain benckmarks for evaluating temporal localization abilities. Comprehensive experiments demonstrate VA-GPT&rsquo;s superiority, achieving state-of-the-art performance in both in-domain anomaly localization and cross-domain generalization scenarios.</p>
<p>The main contributions are summarized as follows:</p>
<ul>
<li>We propose VA-GPT, a video-anomaly-aware MLLM for detecting and summarizing anomalies in various videos, which introduces the MLLM to the specific domain of video anomaly understanding.</li>
<li>We introduce the SETS and TETG, which enable our MLLM to effectively capture both spatial and temporal information in video sequences, resulting in accurate understanding and localization of abnormal events. Meanwhile, we propose a new instruct-following dataset for video anomaly analysis and a comprehensive cross-domain evaluation benchmark for better evaluating the generalization abilities of MLLMs on video anomalies.</li>
<li>Our extensive experiments demonstrate that our method outperforms existing state-of-the-art methods in various benchmarks, highlighting its effectiveness and potential for practical applications in video anomaly understanding.</li>
</ul>

<h2 class="relative group">2. Related Work
    <div id="2-related-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-related-work" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">2.1. Large Language Models (LLMs)
    <div id="21-large-language-models-llms" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#21-large-language-models-llms" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The domain of Natural Language Processing (NLP) has experienced significant progress, particularly with the emergence of Large Language Models (LLMs). The introduction of the Transformer architecture [3 , 18 , 59] is a critical turning point, followed by other influential language models [3 , 10 , 25 , 80] that exhibited remarkable proficiency. Generative Pre-trained Transformers (GPT) [49] brought about a revolution in NLP by employing auto-regressive prediction, establishing itself as a powerful language modeling approach. More recent groundbreaking contributions, such as ChatGPT [45], GPT-4 [1], LaMDA [56] and LLaMA [58], have expanded the horizon even further. These models, trained on extensive textual data, display extraordinary performance in intricate linguistic tasks.</p>

<h2 class="relative group">2.2. Vision Language Models (VLMs)
    <div id="22-vision-language-models-vlms" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#22-vision-language-models-vlms" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Progress in the fields of computer vision and natural language processing has given rise to the development of vision-language models (VLMs) [14 , 21 , 33 , 34 , 50 , 61 , 69]. These models combine visual and linguistic systems to facilitate cross-modal comprehension and reasoning. Notable examples include CLIP [50] which pairs BERT [10] with ViT [11]; BLIP-2 [28] incorporating Vision Transformer features into Flan-T5 [8]; MiniGPT4 [85], connecting BLIP-2 with Vicuna [28 , 48]; PandaGPT [53] , bridging ImageBind [15] with Vicuna. These models excel in tasks like image classification, captioning, and object detection [24 , 53 , 83]. Recent developments in vision-language models have extended into video processing with models</p>
<p>Figure 2. Detailed illustration of our proposed model. When a video is fed into the model, patch embeddings and class embeddings (c.ebd) are extracted from all frames. 1) Based on the difference in patch embeddings between current frame and its neighbour frame, we can get a filter mask to filter out unimportant visual tokens (dashed square ) from current frame&rsquo;s visual tokens , thereby selecting Spatial Effective Tokens that are compressed with a projector with pooling into aligned content token for each frame, meanwhile take attention with text input from users for resulting aligned context token for each frame; 2) Based on class embeddings (c.ebd) of all frames, we use a pre-trained Anomaly-aware Classifier to localize the time period of abnormal events, thereby generating Temporal Effective Tokens to feed forward into the LLM. All of the resulting aligned tokens are fed into the LLM for reasoning and inference of the whole model.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_d7378ced629908a60a96ebf4ec21aacd834817c73e8f87cfa6f788a80743b4f8.png"
    ></figure>
<p>like Video-Chat [29], Video-ChatGPT [42], Otter [26], Valley [39], mPLUG [27], Video-LLaMA [74], and LLaMAVID [31]. These systems enable interactive video querying, enhance comprehension through audio-visual-text alignment, and support comprehensive video analysis. In this paper, we leverage VLMs and LLMs to develop a novel approach for video anomaly understanding.</p>

<h2 class="relative group">2.3. Video Anomaly Understanding (VAU)
    <div id="23-video-anomaly-understanding-vau" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#23-video-anomaly-understanding-vau" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Annotating surveillance video frames is labor-intensive, prompting researchers to explore alternatives: one-class learning [72], unsupervised anomaly detection without annotations [4 , 5 , 16 , 20 , 37 , 38 , 47], or weakly supervised methods using only video-level annotations [6 , 13 , 30 , 57 , 60 , 62 , 63 , 71 , 76]. In one-class learning, Luo et al. developed a ConvLSTM network for normal segment learning [40]. Several researchers employed Auto-Encoder networks to reconstruct normal frame features [5 , 20 , 81], while others implemented memory mechanisms [4 , 16 , 37 , 47] or meta-learning [38] to enhance generalizability. For weakly supervised learning, Tian [57] sed multiple-instance learning to localize anomalous clips. Zhong et al. uutilized graph convolution networks, though with limited generalization capability. [82]. To address this, Ramachandra et al. developed a Siamese network for normal feature learning. Wan et al. and Zaheer et al. [60 , 71]proposed clustering-based frameworks for anomaly identification. Recent studies have introduced new architectures for spatial-temporal feature ensemble learning [6 , 13 , 30 , 57 , 62 , 63 , 76]. However, these methods merely supply anomaly scores during inference, necessitating empirical threshold establishment on test sets to differentiate abnormal events. Recent research has begun exploring MLLMs to enhance models&rsquo; capabilities in identifying and describing anomalies [12 , 55 , 67 , 75 , 79].</p>

<h2 class="relative group">3. Method
    <div id="3-method" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-method" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">3.1. Overview
    <div id="31-overview" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#31-overview" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Task. Video anomaly understanding MLLMs aims to determine whether an input video contains abnormal events, meanwhile describing and interacting with the temporal localization and the entire process of the detected abnormal events (if has). We train the model with an instruct-following dataset based on abnormal videos [54], so that the model can better align the tokens between visual encoders and LLMs for presenting and generalizing information about abnormal events.</p>
<p>Pipeline. Considering the video understanding MLLM framework as shown in Figure 2, taking a video (contains T frames) as input, a frozen ViT-based [11] visual encoder (CLIP [52]) extracts visual tokens X t from each video frame V t (t = 1, &hellip;, T). For X t = {x t i } i=1,&hellip;,N of the current frame, there are N visual tokens corresponding to equal amounts of image patches. Modality alignment converts the processed visual tokens X t into the semantic space of LLMs. At the same time, text prompts are processed and encoded as text tokens into the same semantic space and serve as a part of input to LLMs. Our key design on models consists of (1) selecting Spatial Effective Tokens X ∗t (SET) from X t for each frame participating in fine-tuning and inference instead of X t (see Section 3.2); and (2) generating Temporal Effective Tokens S ∗t (TET) as anomaly-aware temporal priors, participating in inference to facilitate the temporal localization of abnormal events for LLMs (see Section 3.3). In addition, we produce high-quality instruct-following data on abnormal videos and develop a training strategy for it to maximize the effectiveness of our proposed method (see Section 3.4).</p>

<h2 class="relative group">3.2. Spatial Effective Token Selection (SETS)
    <div id="32-spatial-effective-token-selection-sets" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#32-spatial-effective-token-selection-sets" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In classical video classification tasks, context and relationships are critical. However, in our MLLM setting, beyond leveraging contextual information, the most crucial problem is aligning the visual and language modalities. Therefore, the key aspect of our design is to extract useful information for effectively aligning visual tokens with the LLM. Since text captions primarily describe anomaly events, which occupy only a small portion of the entire video, aligning all visual patterns with text tokens would be unreasonable and computationally heavy. Thus, we are the first to propose a novel token selection method SETS to achieve efficient and effective alignment.</p>
<p>Inter-frame Difference. For a video, we believe that areas with large changes in adjacent frames are more worthy of attention. As illustrated in Figure 2, for each frame V t of the video, we can regard its previous frame V t − 1 as the reference frame for investigating the difference between current timestamp and previous timestamp. Employing DINOv2 [46] as the feature extractor, denoted as FE, we can extract patch embeddings:</p>
<!-- formula-not-decoded -->
<p>where F t , F t − 1 ∈ R N×C are the extracted embeddings (N indicates the number of image patches and C indicates the channels). Thanks to the distinction and stability of the extracted features, we calculate their patch-wise distances as the inter-frame difference map of the current frame:</p>
<!-- formula-not-decoded -->
<p>where dis(·) indicates Manhattan distance [17] and D t ∈ R N indicates the distances between corresponding patch pairs in neighbour frames.</p>
<p>Select Spatial Effective Tokens. According to the interframe difference map D t , we can set up a vector M t = [m t t
1
, m t t
2
, &hellip;, m t N ] to record the difference of each patch, where top K ratio of elements with the largest distance are assigned with the value of 1, and the rest are assigned as 0. Thus we get a mask for filtering and updating the visual tokens as:</p>
<!-- formula-not-decoded -->
<p>where X ∗t contains the selected Spatial Effective Tokens (SET) which are fed into subsequent processing instead of</p>
<p>X t as shown in Figure 2. SET can efficiently isolate the regions highly related to the abnormal events to participate in both fine-tuning and inference.</p>

<h2 class="relative group">3.3. Temporal Effective Token Generation (TETG)
    <div id="33-temporal-effective-token-generation-tetg" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#33-temporal-effective-token-generation-tetg" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Anomaly-aware Classifier. We design a simple but effective MLP FA FA for learning whether each frame is related to an abnormal event. For the class embeddings (denoted as z) extracted from the feature encoder, we can split them based on training video caption into normal and anomaly embeddings, denoted as z n and z a , respectively. Thus we can optimize FA using a binary classification loss:</p>
<!-- formula-not-decoded -->
<p>The anomaly-aware classifier predicts whether each frame is related to anomalies in a video, which can bring additional important prior knowledge to LLMs at a very low cost to facilitate its inference.</p>
<p>Generate Temporal Effective Tokens. Since the information drawn from the anomaly-aware classifier is explicit, we can easily project it to LLMs&rsquo; text token space through natural languages. Based on the prediction results of the anomaly-aware classifier, we select the first and last frames&rsquo; timestamps with high confidence of containing abnormal events, denoted as &lt;a-start&gt; and &lt;a-end&gt;, respectively. Then we tokenize them with a template as: &ldquo;Known common crime types are: &lsquo;Shooting&rsquo;,&lsquo;Arson&rsquo;,&lsquo;Arrest&rsquo;, &hellip; There is one of the crime types occurring from &lt;a-start&gt; to &lt;a-end&gt;&rdquo;, resulting Temporal Effective Tokens (TET) in the text token space of LLMs. During inference, with the well-trained lightweight anomaly-aware classifier, TET is used as an additional input to participate in the forward process of the LLM to provide prior knowledge about abnormal events temporally (as shown in Figure 2).</p>

<h2 class="relative group">3.4. Training Strategy
    <div id="34-training-strategy" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#34-training-strategy" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>For modality alignment and instruction tuning, we follow the baseline [31] to ensure visual features are well aligned with the language space. In this work, the training strategy can be divided into two stages: 1) Stage One: Fine-tuning with anomaly video data, and 2) Stage Two: Aligning Spatial Effective Tokens with LLMs.</p>
<p>Fine-tuning with Video Anomaly Data. For enhancing the abnormal scene understanding of LLMs, we construct the UCF-crime [70] Question-Answer pairs for finetuning. We also mix different instruction pairs from various sources [32], including text conversations, single/multi-turn visual Question-Answer pairs, and video Question-Answer pairs. Different formats for text, image, and video inputs are adopted, and the image token is randomly inserted at the beginning or end of user input during training. All modules,</p>
<p>Table 1. Comparing on in-domain (UCF-Crime [54]) and the proposed cross-domain (XD-Violence [64]) benchmarks, our method significantly outperforms other models and achieve the state-of-the-art performance (accuracy is detonated as Acc.) on anomaly video understanding and temporal localization with LLMs (Best results are shown in bold).</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>LLM</th>
          <th>n-domain</th>
          <th>n-domain</th>
          <th>Cross-domain</th>
          <th>Cross-domain</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Method</td>
          <td>LLM</td>
          <td>Total Acc.(%)</td>
          <td>Temporal Acc.(%</td>
          <td>Total Acc.(%)</td>
          <td>Temporal Acc.(%)</td>
      </tr>
      <tr>
          <td>Video-Chat [29]</td>
          <td>Vicuna-7B</td>
          <td>22.41</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td>Video-ChatGPT [42]</td>
          <td>Vicuna-7B</td>
          <td>24.13</td>
          <td>28.51</td>
          <td>24.00</td>
          <td>29.10</td>
      </tr>
      <tr>
          <td>Otter [26]</td>
          <td>LLaMa-7B</td>
          <td>22.41</td>
          <td>22.17</td>
          <td>25.20</td>
          <td>23.80</td>
      </tr>
      <tr>
          <td>Valley [39]</td>
          <td>Vicuna-7B</td>
          <td>20.34</td>
          <td>14.48</td>
          <td>21.00</td>
          <td>20.20</td>
      </tr>
      <tr>
          <td>mPLUG [27]</td>
          <td>LLaMa-7B</td>
          <td>22.76</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td>Video-LLaMA2 [7]</td>
          <td>Vicuna-7B</td>
          <td>21.38</td>
          <td>26.62</td>
          <td>24.20</td>
          <td>23.00</td>
      </tr>
      <tr>
          <td>Hawkeye [79]</td>
          <td>LLaVA-7B</td>
          <td>28.60</td>
          <td>30.00</td>
          <td>25.30</td>
          <td>28.50</td>
      </tr>
      <tr>
          <td>LLaMA-VID [31]</td>
          <td>Vicuna-7B</td>
          <td>14.83</td>
          <td>26.70</td>
          <td>18.80</td>
          <td>23.60</td>
      </tr>
      <tr>
          <td>VA-GPT (Ours)</td>
          <td>Vicuna-7B</td>
          <td>30.69</td>
          <td>35.00</td>
          <td>26.20</td>
          <td>31.02</td>
      </tr>
  </tbody>
</table>
<p>Table 2. We evaluate our method on the MMEval [12] benchmark and show that our method outperforms the related previous method in different aspects.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Description</th>
          <th>Causes</th>
          <th>Effect</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>A-Guardian [12]</td>
          <td>79.65</td>
          <td>58.92</td>
          <td>50.64</td>
      </tr>
      <tr>
          <td>Ours</td>
          <td>80.83</td>
          <td>59.55</td>
          <td>51.08</td>
      </tr>
  </tbody>
</table>
<p>except the frozen visual encoder, are optimized in this stage. After fine-tuning, the LLM has a preferred perception of anomalies, thus ensuring the effectiveness of the temporal effective tokens (see Section 3.3) during inference. More dataset details are illustrated in Section 4 .</p>
<p>Aligning Spatial Effective Tokens with LLMs. For abnormal video scenes, most areas would not be aligned with languages. Therefore, we implement an additional fine-tuning step. This step involves utilizing Spatial Effective Tokens (see Section 3.2) derived from each video frame within the UCF-Crime dataset. By incorporating these tokens, we aim to provide the model with a more refined understanding of the spatial context of anomalies. It also brings efficient optimization, and the alignment here is only designed for very short-term fine-tuning, which can greatly improve the model&rsquo;s ability to detect and understand anomalies.</p>

<h2 class="relative group">4. Experiments
    <div id="4-experiments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#4-experiments" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Datasets. We fine-tune our model on our proposed instructfollowing format [34] training dataset including 4077 videos and 7730 images based on UCF-Crime [54] dataset. We evaluate the models on two anomaly video understanding benchmarks: UCF-Crime [54] for in-domain evaluation and a proposed benchmark designed based on XD-Violence [64] dataset for cross-domain evaluation, respectively. More details are shown in the supplementary.</p>
<p>Benchmarks and Metrics. To evaluate the ability to review videos and identify anomalies, we utilize a video anomaly understanding evaluation from Video-Bench [44] to assess the temporal comprehensive ability, which contains nature language based Question-Answer pairs from UCF-Crime [54] dataset. Meanwhile, in order to evaluate the model&rsquo;s crossdomain video anomaly understanding ability, we contribute Question-Answer pairs as an extra benchmark based on the XD-Violence dataset. These Question-Answer pairs encompass four options, with each presenting the anomaly category and the respective time intervals during which the anomalies transpire. For each benchmark, we design different sets of questions for two evaluations: one is an overall evaluation of abnormal event detection and understanding, and the other is a special evaluation focusing on temporal localization ability, measured by question answering accuracy (denoted as Total Acc. and Temporal Acc., respectively, higher is better).</p>
<p>Implementation Details. For network structure, we incorporate the pre-trained CLIP [52] and DINOv2 [46] as the visual encoder and Qformer [9] as the text decoder. We follow [31] to freeze the encoder during the modality alignment and to optimize the trainable parameters using anomaly videos and instructions for instruction tuning. During the training process, our model utilizes PyTorch on four NVIDIA A100 GPUs. We employ the AdamW optimizer with a batch size of 64. The learning schedule is set to cosine decay, with a learning rate of 2e-5 and a total of 1 epoch.</p>

<h2 class="relative group">4.1. Main Results
    <div id="41-main-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#41-main-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Results on In-domain Dataset. We first evaluate our method on the in-domain dataset, where the test set belongs to the same style and recording mode as the data used for training in Section 3.4. As shown in Table 1, compared with the baseline [31], with fewer visual embedding tokens and temporal effective tokens, our method brings more than double the performance improvement on total accuracy, also brings a significant increase in temporal localization. Driven by our proposed training strategy and designed effective tokens, more pure and effective visual-semantic information</p>
<p>Figure 3. Qualitative results in Question-Answer diagrams, the red circles in the figures correspond to the bold text in the answers. From short video of only a dozen seconds to medium video of longer than one minute and long video of about half an hour, our model can reason well and understand the content.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_d9fae28b86f6484911947aa7384d4981d71147a651c667a19139ecb331e860d2.png"
    ></figure>
<p>Table 3. Ablation studies on Spatial Effective Token Selection (SETS), Temporal Effective Tokens Generation (TETG), and progressive training strategies. At different model training stages, starting from the baseline (w/o fine-tuning, w/o both), we compare the performance of only using SETS (w.SETS), only using TETG (w.TETG), and using both (w.Both) on the UCF-Crime benchmark. Stage One: Only anomaly video fine-tuning. Stage Two: Anomaly video fine-tuning + Fine-tuning with SETS (Best results are shown in bold).</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Baseline</th>
          <th>Baseline</th>
          <th>Baseline</th>
          <th>Baseline</th>
          <th>Stage One Fine-tuning Sta</th>
          <th>Stage One Fine-tuning Sta</th>
          <th>Stage One Fine-tuning Sta</th>
          <th>Stage Two Fine-tuning</th>
          <th>Stage Two Fine-tuning</th>
          <th>Stage Two Fine-tuning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td>w/o Both</td>
          <td>w.SETS</td>
          <td>w.TETG</td>
          <td>w.Both</td>
          <td>w.SETS</td>
          <td>w.TETG</td>
          <td>w.Both</td>
          <td>w.SETS</td>
          <td>w.TETG</td>
          <td>w.Both</td>
      </tr>
      <tr>
          <td>Total Acc. (%)↑</td>
          <td>14.83</td>
          <td>24.83</td>
          <td>23.79</td>
          <td>25.12</td>
          <td>25.86</td>
          <td>26.10</td>
          <td>27.50</td>
          <td>29.31</td>
          <td>28.96</td>
          <td>30.69</td>
      </tr>
      <tr>
          <td>Temporal Acc. (%)↑</td>
          <td>26.70</td>
          <td>27.20</td>
          <td>27.76</td>
          <td>28.81</td>
          <td>29.68</td>
          <td>30.02</td>
          <td>30.77</td>
          <td>31.60</td>
          <td>33.58</td>
          <td>35.00</td>
      </tr>
  </tbody>
</table>
<p>of abnormal events is efficiently aligned with LLMs and exhibits powerful anomaly video understanding capabilities. At the same time, we conduct fair comparisons with existing video understanding models [26 , 27 , 29 , 39 , 42 , 74] (see Table 1), and we demonstrate competitive performance. It is worth noting that we use the fewest tokens among all methods to achieve the state-of-the-art results on both total and temporal accuracies.</p>
<p>Results on Cross-domain Dataset. For evaluating the robustness and generalization of the models, we additionally design a cross-domain benchmark. We conduct a fair comparison of our method with the baseline [31] and the existing in-domain methods on the proposed cross-domain benchmark. The results presented in Table 1 showcase a substantial performance improvement over existing methods on the cross-domain dataset, underscoring the exceptional generalization and temporal localization capabilities of our methodology. This clear superiority in performance serves as a compelling validation of the robustness and adaptability of our approach across diverse domains.</p>
<p>Interaction with the Model. We take some interactions with our well-trained model for better evaluation. As shown in Figure 3, we demonstrate the performance of our model in addressing various video anomaly understanding challenges. To evaluate the model&rsquo;s effectiveness, we select videos of different durations: short (0 to 1 minute), medium (1 to 30 minutes), and long (over 30 minutes). This variety allows us to thoroughly assess the model&rsquo;s capabilities in handling diverse anomaly understanding scenarios. In the Road Accident video (left side in Figure 3), our method successfully identifies a car driving at high speed and detects people falling, even in low-resolution footage. For the Explosion video (middle in Figure 3), the model accurately predicts the scene and the anomaly in a medium-length video depicting an explosion. In a normal video exceeding 30 minutes (right side in Figure 3), we demonstrate the model&rsquo;s ability</p>
<p>Table 4. We fine-tune comparison models with our proposed UCF instruct-following data and evaluate the performance of these models before and after fine-tuning on the UCF benchmark.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>LLM</th>
          <th>Fine-tuned</th>
          <th>Total Acc.(%)↑</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Video-ChatGPT</td>
          <td>Vicuna-7B</td>
          <td>-</td>
          <td>24.13</td>
      </tr>
      <tr>
          <td>Video-ChatGPT</td>
          <td>Vicuna-7B</td>
          <td>✓</td>
          <td>26.23</td>
      </tr>
      <tr>
          <td>LLaMA-VID (Baseline)</td>
          <td>Vicuna-7B</td>
          <td>-</td>
          <td>14.83</td>
      </tr>
      <tr>
          <td>LLaMA-VID (Baseline)</td>
          <td>Vicuna-7B</td>
          <td>✓</td>
          <td>23.1</td>
      </tr>
      <tr>
          <td>Ours</td>
          <td>Vicuna-7B</td>
          <td>-</td>
          <td>25.12</td>
      </tr>
      <tr>
          <td>Ours</td>
          <td>Vicuna-7B</td>
          <td>✓</td>
          <td>30.69</td>
      </tr>
  </tbody>
</table>
<p>to focus on both global and local information by asking it to summarize the content.</p>
<p>Comparison on Other Benchmark. We additionally compare our method on another benchmark (MMEval [12]) about anomaly video understanding with LLMs from different aspects. We follow a fair evaluation on our proposed model and obtain the quantity results as shown in Table 2 , which shows the superiority of our method.</p>

<h2 class="relative group">4.2. Ablation Studies
    <div id="42-ablation-studies" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#42-ablation-studies" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We conduct extensive ablation studies to validate the effectiveness of the key components in our method: Spatial Effective Token Selection (SETS, Section 3.2) and Temporal Effective Tokens Generation (TETG, Section 3.3) on progressive training strategies in Section 3.4 .</p>
<p>Fine-tuning Stages. The utilization of our high-quality UCF instruct-following data has proven to enhance the model&rsquo;s performance. Fine-tuning with this dataset has effectively contributed to a notable accuracy compared with the baseline. As evidenced in Table 3, with our model designing (both SETS and TETG), the total accuracy for anomaly detection only achieves 25.12% without any fine-tuning (denoted as Baseline). With anomaly video fine-tuning (denoted as Stage One Fine-tuning), the accuracy increases to 27.5%. Furthermore, an efficient tuning with SETS (denoted as Stage Two Fine-tuning) can achieve our final performance of 30.69% total accuracy. Temporal accuracy also shows similar increasing patterns with the scaling tuning stages.</p>
<p>Effectiveness of Fine-tuning Data. For a fair comparison, we fine-tune some high-performance comparison models [31 , 42] and compare them with our proposed UCF instructfollowing data. As shown in Table 4, the performance of these comparison models has increased after fine-tuning, which proves the effectiveness of our proposed data. However, their performance is still not as good as our method, which proves the effectiveness of our proposed model.</p>
<p>Effectiveness of SETS. Our proposed SETS demonstrates efficiency in extracting useful abnormal information, leading to performance enhancement. As illustrated in Table 3 , with the SETS, the accuracy reaches 24.83%, 25.86% and 29.31% without fine-tuning, anomaly video fine-tuning and</p>
<p>Table 5. For the sample rate of tokens, K ratios in SETS, we sample the patch tokens with ordered distance at a fixed sample rate (0.5). The ablation indicates too large sampling rates cause too much context noise, and too small sampling rates lose visual information.</p>
<table>
  <thead>
      <tr>
          <th># Sample Rate K</th>
          <th>0.1</th>
          <th>0.3</th>
          <th>0.5</th>
          <th>0.7</th>
          <th>0.9</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Total Acc.(%)↑</td>
          <td>23.61</td>
          <td>24.83</td>
          <td>30.69</td>
          <td>28.67</td>
          <td>27.27</td>
      </tr>
      <tr>
          <td>Temporal Acc.(%)↑</td>
          <td>29.03</td>
          <td>29.93</td>
          <td>35</td>
          <td>31.23</td>
          <td>31.03</td>
      </tr>
  </tbody>
</table>
<p>fine-tuning with SETS, respectively, which far exceed the accuracy of the baseline. Its intuitive mechanism for information filtering can be further analyzed with reference to Figure 4. The initial video is often cluttered with irrelevant and deceptive data. For example, in Figure 4, case one illustrates a scenario where the overall structure is complex, yet only a small segment requires attention. The SETS effectively filters out the dynamic features that do not require attention. Similarly, as in case two, the abnormal area is quite small. Our proposed SETS mechanism effectively filters out redundant and irrelevant information, significantly enhancing the model&rsquo;s ability to accurately pinpoint and recognize abnormal situations.</p>
<p>We also conduct the ablation studies on K ratios about SETS as shown in Tabel 5. Too small or too large K ratios will cause performance degradation in both total and temporal accuracy. If K ratios is too small, redundant information will affect the effectiveness of aligning abnormal event information with corresponding captions. In contrast, if K ratios is too large, some important areas will be filtered out, resulting in information loss and suboptimal performance.</p>
<p>Effectiveness of TETG. Our proposed TETG generates tokens directly in the text token space of LLMs as priors for each video, offering robust priors on the temporal aspects of abnormal events. The provision results in performance improvement without the necessity for fine-tuning. As shown in Table 3, the accuracy rises from 14.83% to 23.79%. Fine-tuning with anomaly video and SETS, the results even achieve 26.10% and 30.69% independently, which manifests the effectiveness of TETG. Besides, the integration of SETS and TETG highlights the importance of leveraging spatial and temporal information effectively in anomaly detection systems, boosting the results to 25.15%, 27.50% and 30.69%, respectively.</p>

<h2 class="relative group">5. Discussion
    <div id="5-discussion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#5-discussion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Key tokens play key roles. To the best of our knowledge, we are the first to explore how to assign different learnable knowledge to different tokens for better alignment with LLMs on visual contents, thus promoting video anomaly detection and understanding (see Table 1 and Figure 3). We assign the most effective roles to different tokens in both spatial and temporal dimensions, enabling the model to handle</p>
<p>Figure 4. Visualization of the initial videos and our masked results. These two cases illustrate road accident scenarios: one occurring in a bustling street and the other in an empty suburb. Our SETS effectively filters redundant and irrelevant regions (with black patch-level masks).</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_698c2444aa874ca7c6210c9bdeaaca4f84cd27bc63b9d0c3234f1cfd2ae96ce9.png"
    ></figure>
<p>various tokens more efficiently. The video contains abundant but redundant information. Our proposed SETS and TETG effectively compress the spatial and temporal information of abnormal events respectively, and utilize the existing alignment mechanism of MLLMs at a very low cost to participate in LLMs&rsquo; reasoning (see Table 3). Our exploration inspires more representation learning of MLLMs to facilitate downstream tasks.</p>
<p>Data matters a lot. We construct instruct-following data for anomaly videos, containing approximately 4,000 videos, which is significantly less than the amount of baseline finetuning video data (e.g., over 90k videos for fine-tuning in baseline model [31]). We still achieve promising performance on both in-domain and cross-domain benchmarks (see Table 1). This relies on the high-quality Question-Answer pairs in our instruct-following data. Meanwhile, SETS also improves data quality during fine-tuning: visual areas irrelevant to Question-Answer pairs are filtered out (see Figure 4), which allows for significant performance improvements in the second stage of fine-tuning (see Section 3.4) with very few steps (less than 150 iterations).</p>
<p>Broader impacts. Video anomaly understanding has farreaching implications across various sectors, including security, healthcare, industrial safety, and so on. By enhancing the ability to automatically identify and respond to unusual or suspicious activities in real-time, LLMs can significantly improve public safety, crime prevention, patient monitoring, hazard detection, loss prevention, traffic management, and urban planning. These systems offer substantial benefits in terms of operational efficiency and safety.</p>
<p>Limitations. Although our model adeptly portrays the occurrence, type, and area of video abnormal events, it still faces challenges in detecting and describing certain complex scenes. Our strategy represents an early successful validation and investigation of large models for video anomaly identification and localization. Consequently, our method possesses significant potential for enhancement in recognizing diverse abnormal video scenes. These insights motivate us to continue pursuing more powerful and efficient video anomaly understanding technologies in the future, aiming to address more challenges in the real world [35 , 41 , 77].</p>

<h2 class="relative group">6. Conclusions
    <div id="6-conclusions" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#6-conclusions" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this paper, we propose a novel MLLM for understanding anomalies in videos with LLMs by aligning effective tokens in both temporal and spatial space. The proposed method includes Spatial Effective Token Selection (SETS) for identifying abnormal events in small areas of large scenes and Temporal Effective Tokens Generation (TETG) for addressing the sparseness of abnormal events in video time sequences. We also develop instruct-following data of video anomaly detection to fine-tune the model. Besides, evaluation on the video anomaly understanding benchmark and a proposed cross-domain benchmark demonstrates the effectiveness of the proposed method. It further presents a promising approach for video anomaly understanding using MLLMs, showcasing the potential of effective tokens for enhancing video understanding tasks.</p>

<h2 class="relative group">Acknowledgements
    <div id="acknowledgements" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#acknowledgements" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This work has been supported in part by Hong Kong Research Grant Council - Early Career Scheme (Grant No.27209621), General Research Fund Scheme (Grant No.17202422, 17212923, 17215025), Theme-based Research (Grant No.T45-701/22-R) and Shenzhen Science and Technology Innovation Commission (SGDX20220530111405040). Part of the described research work is conducted in the JC STEM Lab of Robotics for Soft Materials funded by The Hong Kong Jockey Club Charities Trust.</p>

<h2 class="relative group">References
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 , 2</li>
<li>[2] Borislav Antic and Bj ´ ´ orn Ommer. Video parsing for abnormal- ¨ ¨ ity detection. In 2011 International Conference on Computer Vision, pages 2415–2422, 2011. 1</li>
<li>[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. 2</li>
<li>[4] Ruichu Cai, Hao Zhang, Wen Liu, Shenghua Gao, and Zhifeng Hao. Appearance-motion memory consistency network for video anomaly detection. Proceedings of the AAAI Conference on Artificial Intelligence, 35(2):938–946, 2021. 1 , 3</li>
<li>[5] Yunpeng Chang, Zhigang Tu, Wei Xie, and Junsong Yuan. Clustering driven deep autoencoder for video anomaly detection. In Computer Vision – ECCV 2020, 16th European Conference, pages 329–345, 2022. 3</li>
<li>[6] Yingxian Chen, Zhengzhe Liu, Baoheng Zhang, Wilton Fok, Xiaojuan Qi, and Yik-Chung Wu. Mgfn: Magnitudecontrastive glance-and-focus network for weakly-supervised video anomaly detection. AAAI2023, 2022. 1 , 3</li>
<li>[7] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 5</li>
<li>[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai,</li>
</ul>
<ol start="9">
<li>Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. 2</li>
</ol>
<ul>
<li>[9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning, 2023. 5</li>
<li>[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. 2</li>
<li>[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2 , 3</li>
<li>[12] Hang Du, Sicheng Zhang, Binzhu Xie, Guoshun Nan, Jiayang Zhang, Junrui Xu, Hangyu Liu, Sicong Leng, Jiangming Liu, Hehe Fan, Dajiu Huang, Jing Feng, Linli Chen, Can Zhang, Xuhuan Li, Hao Zhang, Jianhang Chen, Qimei Cui, and Xiaofeng Tao. Uncovering what, why and how: A comprehensive benchmark for causation understanding of video anomaly, 2024. 2 , 3 , 5 , 7</li>
<li>[13] JiaChang Feng, FaTing Hong, and WeiShi Zheng. MIST: multiple instance self-training framework for video anomaly detection. In 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1 , 3</li>
<li>[14] Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, and Chunhua Shen. Pyramidclip: Hierarchical feature alignment for vision-language model pretraining, 2022. 2</li>
<li>[15] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all, 2023. 2</li>
<li>[16] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection. In IEEE International Conference on Computer Vision (ICCV), 2019. 1 , 3</li>
<li>[17] Rodolfo Gonzalez and Sandra Palais. A path-building procedure for iterative circuit computers. Technical report, 1962. 4</li>
<li>[18] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu, Z. Yang, Y. Zhang, and D. Tao. A survey on vision transformer. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, pages 1–1, 2020. 2</li>
<li>[19] Mahmudul Hasan, Jonghyun Choi, jan Neumann, Amit K Roy-Chowdhury, and Larry Davis. Learning temporal regularity in video sequences. In Proceedings of IEEE Computer Vision and Pattern Recognition, 2016. 1</li>
<li>[20] Radu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana Georgescu, and Ling Shao. Object-centric auto-encoders and dummy anomalies for abnormal event detection in video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3</li>
<li>[21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision, 2021. 2</li>
<li>[22] Hyekang Kevin Joo, Khoa Vo, Kashu Yamazaki, and Ngan Le. Clip-tsa: Clip-assisted temporal self-attention for weaklysupervised video anomaly detection, 2023. 1</li>
<li>[23] Louis Kratz and Ko Nishino. Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 1446–1453, 2009. 1</li>
<li>[24] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. F-vlm: Open-vocabulary object detection upon frozen vision and language models, 2023. 2</li>
<li>[25] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019. 2</li>
<li>[26] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning, 2023. 3 , 5 , 6</li>
<li>[27] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou, and Luo Si. mplug: Effective and efficient vision-language learning by cross-modal skip-connections, 2022. 1 , 3 , 5 , 6</li>
<li>[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. 1 , 2</li>
<li>[29] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding, 2024. 1 , 3 , 5 , 6</li>
<li>[30] Shuo Li, Fang Liu, and Licheng Jiao. Self-training multisequence learning with transformer for weakly supervised video anomaly detection. Proceedings of the AAAI Conference on Artificial Intelligence, 36(2):1395–1403, 2022. 3</li>
<li>[31] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models, 2023. 1 , 3 , 4 , 5 , 6 , 7 , 8</li>
<li>[32] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection, 2023. 4</li>
<li>[33] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Munan Ning, and Li Yuan. Moellava: Mixture of experts for large vision-language models, 2024. 2</li>
<li>[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 1 , 2 , 5</li>
<li>[35] Jiahui Liu, Chirui Chang, Jianhui Liu, Xiaoyang Wu, Lan Ma, and Xiaojuan Qi. Mars3d: A plug-and-play motionaware model for semantic segmentation on multi-scan 3d point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9372–9381, 2023. 8</li>
<li>[36] Jiahui Liu, Xin Wen, Shizhen Zhao, Yingxian Chen, and Xiaojuan Qi. Can ood object detectors learn from foundation</li>
</ul>
<ol start="38">
<li>models? In European Conference on Computer Vision, pages 213–231. Springer, 2024. 1</li>
</ol>
<ul>
<li>[37] Zhian Liu, Yongwei Nie, Chengjiang Long, Qing Zhang, and Guiqing Li. A hybrid video anomaly detection framework via memory-augmented flow reconstruction and flow-guided frame prediction. In Proceedings of the IEEE International Conference on Computer Vision, 2021. 1 , 3</li>
<li>[38] Yiwei Lu, Frank Yu, Mahesh Kumar Krishna Reddy, and Yang Wang. Few-shot scene-adaptive anomaly detection. In European Conference on Computer Vision, 2020. 3</li>
<li>[39] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability, 2023. 1 , 3 , 5 , 6</li>
<li>[40] Weixin Luo, Wen Liu, and Shenghua Gao. Remembering history with convolutional lstm for anomaly detection. In 2017 IEEE International Conference on Multimedia and Expo (ICME), pages 439–444, 2017. 3</li>
<li>[41] Xiaoyang Lyu, Chirui Chang, Peng Dai, Yang-Tian Sun, and Xiaojuan Qi. Total-decom: decomposed 3d scene reconstruction with minimal interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20860–20869, 2024. 8</li>
<li>[42] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models, 2023. 1 , 3 , 5 , 6 , 7</li>
<li>[43] Trong-Nguyen Nguyen and Jean Meunier. Anomaly detection in video sequence with appearance-motion correspondence. CoRR, abs/1908.06351, 2019. 1</li>
<li>[44] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models, 2023. 5</li>
<li>[45] OpenAI. Introducing chatgpt. 2022. 1 , 2</li>
<li>[46] Maxime Oquab, Timothee Darcet, Th ´ ´ eo Moutakanni, Huy Vo, ´ ´ Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien ´ ´ Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. 4 , 5</li>
<li>[47] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning memory-guided normality for anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14372–14381, 2020. 1 , 3</li>
<li>[48] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4, 2023. 2</li>
<li>[49] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. 2</li>
<li>[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 2</li>
<li>[51] Bharathkumar Ramachandra, Michael J. Jones, and Ranga Raju Vatsavai. Learning a distance function with a siamese network to localize anomalies in videos. In 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), 2020. 1</li>
<li>[52] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, and Marco Fumero. Clip-forge: Towards zero-shot text-to-shape generation. In CVPR, 2022. 3 , 5</li>
<li>[53] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all, 2023. 2</li>
<li>[54] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 3 , 5</li>
<li>[55] Jiaqi Tang, Hao Lu, Ruizheng Wu, Xiaogang Xu, Ke Ma, Cheng Fang, Bin Guo, Jiangbo Lu, Qifeng Chen, and YingCong Chen. Hawk: Learning to understand open-world video anomalies, 2024. 2 , 3</li>
<li>[56] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022. 2</li>
<li>[57] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W. Verjans, and Gustavo Carneiro. Weakly-supervised video anomaly detection with robust temporal feature magnitude learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4975–4986, 2021. 1 , 3</li>
<li>[58] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste ´ ´ Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien <code> </code> Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. 1 , 2</li>
<li>[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017. 2</li>
<li>[60] Boyang Wan, Yuming Fang, Xue Xia, and Jiajie Mei. Weakly supervised video anomaly detection via center-guided discriminative learning. In Proceedings of the IEEE International Conference on Multimedia and Expo, 2020. 1 , 3</li>
<li>[61] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks, 2023. 2</li>
<li>[62] Jie Wu, Wei Zhang, Guanbin Li, Wenhao Wu, Xiao Tan, Yingying Li, Errui Ding, and Liang Lin. Weakly-supervised spatio-temporal anomaly detection in surveillance video. In he Thirtieth International Joint Conference on Artificial Intelligence, 2021. 3</li>
<li>[63] Peng Wu and Jing Liu. Learning causal temporal relation and feature discrimination for anomaly detection. IEEE Transactions on Image Processing, 30:3513–3527, 2021. 1 , 3</li>
<li>[64] Peng Wu, jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. Not only look, but also listen: Learning multimodal violence detection under weak supervision. In European Conference on Computer Vision (ECCV), 2020. 1 , 2 , 5</li>
<li>[65] Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng Wang, and Yanning Zhang. Vadclip: Adapting vision-language models for weakly supervised video anomaly detection, 2023. 1</li>
<li>[66] Peng Wu, Xuerong Zhou, Guansong Pang, Yujia Sun, Jing Liu, Peng Wang, and Yanning Zhang. Open-vocabulary video anomaly detection, 2024. 1</li>
<li>[67] Yuchen Yang, Kwonjoon Lee, Behzad Dariush, Yinzhi Cao, and Shao-Yuan Lo. Follow the rules: Reasoning for video anomaly detection with large language models, 2024. 2 , 3</li>
<li>[68] Zhiwei Yang, Jing Liu, and Peng Wu. Text prompt with normality guidance for weakly supervised video anomaly detection, 2024. 1</li>
<li>[69] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training, 2021. 2</li>
<li>[70] Tongtong Yuan, Xuange Zhang, Kun Liu, Bo Liu, Chen Chen, Jian Jin, and Zhenzhen Jiao. Towards surveillance videoand-language understanding: New dataset, baselines, and challenges, 2023. 4</li>
<li>[71] Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, and Seung-Ik Lee. Claws: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection. In European Conference on Computer Vision (ECCV), pages 358–376. Springer, 2020. 1 , 3</li>
<li>[72] Muhammad Zaigham Zaheer, Arif Mahmood, Muhammad Haris Khan, Mattia Segu, Fisher Yu, and Seung-Ik Lee. Generative cooperative learning for unsupervised video anomaly detection, 2022. 3</li>
<li>[73] Muhammad Zaigham Zaheer, Jin-Ha Lee, Marcella Astrid, Arif Mahmood, and Seung-Ik Lee. Cleaning label noise with clusters for minimally supervised anomaly detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, June 2020. 1</li>
<li>[74] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding, 2023. 1 , 3 , 6</li>
<li>[75] Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Xiaonan Huang, Changxin Gao, Shanjun Zhang, Li Yu, and</li>
</ul>
<ol start="78">
<li>Nong Sang. Holmes-vau: Towards long-term video anomaly understanding at any granularity, 2024. 2 , 3</li>
</ol>
<ul>
<li>[76] Jiangong Zhang, Laiyun Qing, and Jun Miao. Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection. In 2019 IEEE International Conference on Image Processing (ICIP), pages 4030–4034, 2019. 3</li>
<li>[77] Jiaqi Zhang, Yan Hu, Xiaojuan Qi, Ting Meng, Lihui Wang, Huazhu Fu, Mingming Yang, and Jiang Liu. Polar eyeball shape net for 3d posterior ocular shape representation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 180–190. Springer, 2023. 8</li>
<li>[78] Bin Zhao, Fei-Fei Li, and Eric P. Xing. Online detection of unusual events in videos via dynamic sparse coding. In CVPR 2011, pages 3313–3320, 2011. 1</li>
<li>[79] Jianing Zhao, Jingjing Wang, Yujie Jin, Jiamin Luo, and Guodong Zhou. Hawkeye: Discovering and grounding implicit anomalous sentiment in recon-videos via sceneenhanced video large language model. In Proceedings of ACM MM 2024, 2024. 2 , 3 , 5</li>
<li>[80] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models, 2023. 2</li>
<li>[81] Yiru Zhao, Bing Deng, Chen Shen, Yao Liu, Hongtao Lu, and Xian-Sheng Hua. Spatio-temporal autoencoder for video anomaly detection. In Proceedings of the 25th ACM International Conference on Multimedia, page 1933–1941, New York, NY, USA, 2017. Association for Computing Machinery. 3</li>
<li>[82] Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H. Li, and Ge Li. Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3</li>
<li>[83] Lanfeng Zhong, Xin Liao, Shaoting Zhang, Xiaofan Zhang, and Guotai Wang. Vlm-cpl: Consensus pseudo labels from vision-language models for human annotation-free pathological image classification, 2024. 2</li>
<li>[84] Hang Zhou, Junqing Yu, and Wei Yang. Dual memory units with uncertainty regulation for weakly supervised video anomaly detection, 2023. 1</li>
<li>[85] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023. 2</li>
<li>[86] Yi Zhu and Shawn D. Newsam. Motion-aware feature for improved video anomaly detection. In British Machine Vision Conference (BMVC), 2019. 1</li>
</ul>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/Aligning Effective Tokens with Video Anomaly in Large Language Models.md"
          data-oid-likes="likes_papers/Aligning Effective Tokens with Video Anomaly in Large Language Models.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/an-attribute-based-method-for-video-anomaly-detection/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/action-hints-semantic-typicality-and-context-uniqueness-for/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
