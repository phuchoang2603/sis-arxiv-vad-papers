<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/ye_vera_explainable_video_anomaly_detection_via_verbalized_learning_of_vision-language_cvpr_2025_paper/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/ye_vera_explainable_video_anomaly_detection_via_verbalized_learning_of_vision-language_cvpr_2025_paper/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/ye_vera_explainable_video_anomaly_detection_via_verbalized_learning_of_vision-language_cvpr_2025_paper\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "8183"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>8183 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">39 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          <figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_6d1e247b7d0a98b62a35558742715e9c9ca643c3fbfd736fe597c595aff4e11b.png"
    ></figure>
<p>This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version;</p>
<p>the final published version of the proceedings is available on IEEE Xplore.</p>

<h2 class="relative group">VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models
    <div id="vera-explainable-video-anomaly-detection-via-verbalized-learning-of-vision-language-models" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#vera-explainable-video-anomaly-detection-via-verbalized-learning-of-vision-language-models" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Muchao Ye 1*</p>
<p>Weiyang Liu 2</p>
<p>Pan He 3</p>
<p>1 The University of Iowa 2 Max Planck Institute for Intelligent Systems, Tubingen ¨ ¨ 3 Auburn University 1 <a
  href="mailto:muye@uiowa.edu">muye@uiowa.edu</a> 2 <a
  href="mailto:weiyang.liu@tuebingen.mpg.de">weiyang.liu@tuebingen.mpg.de</a> 3 <a
  href="mailto:pan.he@auburn.edu">pan.he@auburn.edu</a> ⇤ Corresponding Author <a
  href="https://vera-framework.github.io"
    target="_blank"
  >https://vera-framework.github.io</a></p>

<h2 class="relative group">Abstract
    <div id="abstract" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#abstract" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The rapid advancement of vision-language models (VLMs) has established a new paradigm in video anomaly detection (VAD): leveraging VLMs to simultaneously detect anomalies and provide comprehendible explanations for the decisions. Existing work in this direction often assumes the complex reasoning required for VAD exceeds the capabilities of pretrained VLMs. Consequently, these approaches either incorporate specialized reasoning modules during inference or rely on instruction tuning datasets through additional training to adapt VLMs for VAD. However, such strategies often incur substantial computational costs or data annotation overhead. To address these challenges in explainable VAD, we introduce a verbalized learning framework named VERA that enables VLMs to perform VAD without model parameter modifications. Specifically, VERA automatically decomposes the complex reasoning required for VAD into reflections on simpler, more focused guiding questions capturing distinct abnormal patterns. It treats these reflective questions as learnable parameters and optimizes them through data-driven verbal interactions between learner and optimizer VLMs, using coarsely labeled training data. During inference, VERA embeds the learned questions into model prompts to guide VLMs in generating segment-level anomaly scores, which are then refined into frame-level scores via the fusion of scene and temporal contexts. Experimental results on challenging benchmarks demonstrate that the learned questions of VERA are highly adaptable, significantly improving both detection performance and explainability of VLMs for VAD.</p>

<h2 class="relative group">1. Introduction
    <div id="1-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video anomaly detection (VAD) aims to automatically identify unexpected and abnormal events in video sequences, with broad applications ranging from autonomous driving [2] to industrial manufacturing [31]. While achieving good performance in VAD is essential, providing clear explanations for detected anomalies is even more crucial.</p>
<p>To this end, our work primarily focuses on explain-</p>
<p>Figure 1. VERA renders frozen VLMs to describe and reason with learnable guiding questions learned from coarsely labeled data.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_f6f6d340d1643b4c462777b50acf8b44f29669fcc6e8fb71dde433a03c0bc8d5.png"
    ></figure>
<p>able VAD, which requires both comprehensive visual understanding and the ability to generate human-interpretable predictions. The rapid advancement of vision-language models (VLMs) [7 , 18 , 21 , 58] enables us to address both requirements through their strong visual reasoning and language interaction capabilities. As multi-modal architectures that effectively combine the reasoning capabilities from large language models (LLMs) [4] and the visual understanding capabilities from pretrained vision encoders [8], VLMs are particularly well-suited for VAD for they can offer explainable predictions that clearly illustrate the rationale behind specific anomalies, making the results more interpretable to users. Recent research on VAD has consequently focused on how to effectively leverage the power of pretrained VLM. As shown in Fig. 1, existing approaches aim to address the misalignment problem between VLMs&rsquo; pretraining tasks and the VAD requirements through either additional reasoning modules or instruction tuning (IT):</p>
<ul>
<li>One line of research introduces external LLMs to assist frozen VLMs to reason in VAD [46 , 52]. It uses VLMs to caption what they see given a video, and the descriptions are then passed to an external LLM, e.g., GPT-4 [1], to reason whether an anomaly occurs.</li>
<li>Another line of research, instead, expands VLMs to generate explainable prediction via IT [26 , 55]. This research line creates additional VAD datasets with frame-</li>
</ul>
<p>level annotations and leverages exemplary instructions to fine-tune the VLM, enabling it to detect anomalies and generate human-interpretable explanations.</p>
<p>Key Observations and Research Question. While prior research demonstrates the potential of applying VLMs to VAD, we identify that this new paradigm is hindered by a shared critical issue: the use of additional reasoning modules or fine-grained labeled datasets incurs significant computational cost either in the inference or training phases. First, decoupling a VAD system into a frozen VLM and an extra LLM introduces more overhead in inference, because it separates the description generation and reasoning processes. Secondly, although IT-based methods enable VLMs to effectively integrate description and reasoning for VAD, they require additional manpower and computational resources for annotating and finetuning on fine-grained labeled instruction datasets, which is time-consuming and not scalable for large-scale datasets. In light of this, we investigate the following unexplored yet important question:</p>
<p>Can we enable a frozen VLM to integrate description and reasoning for VAD without instruction tuning?</p>
<p>Our Approach. This research question is nontrivial because the reasoning ability of a frozen VLM is limited in general visual tasks, and it struggles to handle complex reasoning tasks like VAD, which requires the understanding of subtle, context-dependent outliers. To illustrate, Table 1 shows that prompting frozen VLMs with simple VAD questions used in existing works leads to unsatisfactory results. Thus, instruction-tuning a VLM seems necessary to make it responsive to specific instructional cues and capture delicate visual variations. In this paper, we question the necessity of such an operation and propose a principled approach to tailor frozen VLMs for VAD.</p>
<p>Specifically, our solution is guided by the intuition that the reasoning ability of VLMs for VAD will improve if we find questions with suitable and concrete description of abnormal patterns rather than with abstract and general words like &ldquo;anomaly&rdquo; to prompt them. Our idea is to iteratively refine anomaly descriptions from abstract ones (e.g., &ldquo;is there any anomaly?&rdquo;) to detailed, specific characterizations.</p>
<p>Driven by such insight, we propose a framework, termed VERA, to explore verbalized learning (VL) for VAD. This framework considers the practical constraint that it is suboptimal to manually write down VAD guiding questions across VLMs, so it introduces a data-driven learning task to identify suitable anomaly-characterization questions containing concrete abnormal patterns for the frozen VLM using coarsely labeled datasets, eliminating the need for IT. Specifically, in the training phase, VERA treats the questions guiding the reasoning of VLMs in VAD as learnable parameters, improving them based on the verbal feedback from an optimizer VLM on the performance of a learner</p>
<table>
  <thead>
      <tr>
          <th>VAD Question for InternVL2-8B</th>
          <th>AUC (%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>“Describe the video and is there any anomaly?” [26]</td>
          <td>53.05</td>
      </tr>
      <tr>
          <td>“Are there any abnormal events in the video?” [</td>
          <td>65.03</td>
      </tr>
  </tbody>
</table>
<p>Table 1. Instructing a frozen VLM (InternVL2-8B [7]) with simple questions to perform VAD yields poor AUC on UCF-Crime [32] dataset.</p>
<p>VLM on an intermediate VAD subtask—binary video classification for each video in the VAD training set. This design is both efficient and appropriate for VAD, as it accounts for video-specific properties like temporality while relying solely on provided coarse video-level labels. After that, considering the large scale of video frames, VERA assigns a fine-grained anomaly score for each frame in a coarse-tofine manner in the inference phase. First, VERA generates segment-level anomaly scores by querying VLMs with the learned guiding questions. Next, VERA improves the initial score by incorporating scene context into each segment score via ensembling. Finally, VERA outputs frame-level scores by fusing temporal context via Gaussian smoothing and frame-level position weighting.</p>
<p>Contributions. To sum up, our contributions are:</p>
<ul>
<li>To our knowledge, we present the first approach, that is, VERA, to adapt frozen VLMs as an integrated system for VAD by learning detailed anomaly-characterization questions in prompts that decompose anomalies into concrete and recognizable patterns. VERA learns them directly from coarsely labeled datasets, eliminating the need for IT or external reasoning modules.</li>
<li>We introduce an effective VL-based algorithm for VLMs in VAD, allowing direct adaptation without modifying model parameters. With coarse labeled VAD datasets only, our approach obtains good guiding questions in VAD by relying on the verbal interaction between learner and optimizer VLMs in verbalized training. Additionally, we design a coarse-to-fine strategy to derive frame-level anomaly scores from verbally learned guiding questions in VAD, integrating both scene and temporal contexts for better VAD performance and reasoning.</li>
<li>The learned guiding questions from VERA are expressed in natural languages, providing a unified method to encode and transfer prior VAD knowledge seamlessly to other datasets or VLMs. In challenging VAD datasets like UCF-Crime [32] and XD-Violence [42], VERA achieves state-of-the-art explainable VAD performance and enjoys good generalization ability across models and datasets.</li>
</ul>

<h2 class="relative group">2. Related Work
    <div id="2-related-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-related-work" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video Anomaly Detection. VAD is the task of localizing frames that contain abnormal events in a given video. This task is challenging for anomalies cover a broad scope of events like accidents and criminal activities while training sets only offer coarse annotations. Modern VAD methods are based on deep neural networks (DNNs) for their superi-</p>
<p>ority and are going through a paradigm shift in using VLMs: (1) Early DNNs for VAD are task-specific, which often employ unsupervised (including one-class) or weakly supervised (WS) learning techniques for training. Most unsupervised learning methods [23 , 25 , 37 , 38 , 48 , 56] train DNNs on frame reconstruction/prediction tasks to establish representation spaces for normal/abnormal videos. WS learning methods [5 , 27 , 32 , 44 , 47 , 53] leverage both normal and abnormal videos to train a feature extractor that distinguishes anomalies from normalcy, typically using multiple instance learning [32] objectives. (2) Recent VAD methods adopt VLMs due to their remarkable success across core vision tasks [12 , 21 , 28 , 33]. Early research [26 , 46 , 52 , 55] has leveraged VLMs to generate textual descriptions of detected anomalies to enhance prediction explainability for VAD. However, current approaches incur high processing demands from external LLMs or require substantial effort and cost for fine-tuning on additional datasets, which are computationally inefficient in training or inference. Our work reduces the processing overhead by adapting frozen VLMs for VAD without model parameter modification or extra reasoning modules via learnable guiding questions, which elicit superior reasoning from frozen VLMs and significantly boost their performance in VAD.</p>
<p>Verbalized Learning for VLMs. The designed VL framework is inspired by a recent technique called verbalized machine learning (VML) [45]. The main idea of VML is to use LLMs to approximate functions and learn the verbal rules and descriptions of performing specific tasks, which casts traditional machine learning tasks as language-based learning tasks. This approach regards the language expressions that define classification rules and other task-specific criteria as learned parameters, and optimize them in a datadriven fashion through interactions between a learner and an optimizer modeled by LLMs or VLMs. However, the VML framework is limited to tasks involving regression on scalar values or classification for static images. A similar idea has also been explored in a concurrent method, TextGrad [49], which integrates the process of incorporating textual feedback from LLMs for improving prompts in PyTorch and further proves its effectiveness in coding, question answering, and optimization in chemistry and medicine. Compared to existing works, our work pioneers VL for the VAD task and video data, which remains unsolved for previous VL frameworks focus on static-data tasks and cannot handle the challenges of temporality and scene dynamics in videos. Specifically, VERA introduces a new learning paradigm for VAD: generating effective questions that encapsulate key abnormal patterns in videos to elicit the reasoning ability from VLMs for explainable VAD. Additionally, VERA works for any VAD dataset and supports WS learning. Unlike previous WS methods, VERA only needs to learn concise text but not millions of parameters, so the training is lightweight.</p>

<h2 class="relative group">3. The VERA Framework
    <div id="3-the-vera-framework" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-the-vera-framework" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Our approach adapts VLMs to detect video anomalies without additional reasoning modules or IT. We now formulate the VAD task and detail the design of VERA.</p>

<h2 class="relative group">3.1. Problem Formulation
    <div id="31-problem-formulation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#31-problem-formulation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video Anomaly Detection. Let V be a video with F frames, represented as V = {Ii} F i=1 , where Ii is the i-th frame (1  i  F). Our objective is to locate and detect the start and end of anomalous events within V . In standard labeling, any frame associated with an anomaly is labeled as 1, and normal frames are labeled as 0. Therefore, the ground truth label sequence for V is Y = [y1,&hellip;,yF ], where yi 2 {0 , 1} represents the fine-grained label for Ii. We aim to use a frozen VLM, fVLM, to generate anomaly score predictions across all frames, Y ˆ = [ˆy1 ,&hellip;, y ˆ F ], where y ˆ i 2 [0 , 1] is a continuous anomaly score for Ii .</p>
<p>Available Training Data for VAD. Typically, VAD datasets only provide coarsely labeled training sets [23 , 25 , 32 , 42]. We denote a VAD training set as D = {(V (j), Y (j))} N j=1 , where N is the total number of training videos, V (j) represents the j-th video (1  j  N) and Y (j) is the corresponding video-level label. Y (j) = 1 if V (j) contains any anomaly defined by the dataset annotators, e.g., abuse or arson activities, and Y (j) = 0 if V (j) has no anomalies. For V (j), we suppose it contains Fj Fj frames and denote the frames sequence as V (j) = {I
i (j) I
i } Fj Fj i=1 , where I
i (j) I
i is the i-th frame (1  i  Fj ) in V (j) .</p>

<h2 class="relative group">3.2. Training in VERA
    <div id="32-training-in-vera" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#32-training-in-vera" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Training Objective. We aim to learn guiding questions that break down a complex and ambiguous concept (i.e., what is an &ldquo;anomaly&rdquo;) into a set of identifiable anomalous patterns to unlock reasoning capabilities within frozen VLMs for VAD tasks. Those patterns vary among datasets, making manually designed descriptions ineffective for generalization. To address this, we propose a general VL framework shown in Fig. 2 to generate the desired guiding questions. We denote the guiding question set as Q = {q1,&hellip;,q m } , where qi is the i-th question (1  i  m) and m is the number of questions. The training framework considers Q as the learnable parameters, which are optimized through verbal interaction between a learner and an optimizer, modeled by VLMs through leveraging their ability to follow instructions with given prompts.</p>
<p>Training Data. The training data for learning Q consist of paired sampled video frames and video-level labels. Sampling is necessary because the amount of video frames is so huge that we cannot compute with every frame. We explore three types of sampling strategies and find that uniform sampling [54] yields the best results. That is, with any video V (j) 2 D, we first calculate the interval between</p>
<p>Figure 2. The overall training pipeline in VERA aims to optimize VAD guiding questions iteratively. In each iteration t, the optimization is verbalized by providing verbal instructions for the learner and optimizer to follow. They will generate predictions and new guiding questions, respectively.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_7d4b19a8f86a4e765e4ede218990d8f7b25c2d4e27531c0e002a7475a7f9e315.png"
    ></figure>
<p>sampled frames as l = floor(Fj/S), where S is the number of sampled frames, and floor denotes rounding down to the nearest integer. Given l, the uniformly sampled frames from V (j) are represented by V˜ ˜ (j) = [I (j) 1 , I
l (j) I
l+1 ,&hellip;,I (j) (S1)·l+1 ] . The label used for training is Y (j) only, resulting in training data pairs {(V ˜ (j) , Y (j))} N j=1 for VERA.</p>
<p>Updating Q via Learner and Optimizer. Since Q are verbal expressions for specific anomaly patterns, VERA inherits the idea of VML [45] in training: optimizing language-based parameters by verbal communication between a learner agent flearner and an optimizer agent fopt, rather than by numerical optimization algorithms like Adam [16]. W.l.o.g., we take an arbitrary iteration t when implementing the complete algorithm (detailed in Supplementary Material) for illustration. We denote any LLMbased model as f(x; ) where x represents the input data, and  denotes the natural language instructions for f to follow, which is considered as learnable parameters in our VL framework. Specifically, Q contains parameters to be learned in VERA. As depicted in Fig. 2, in each iteration t, the learner agent f
l (t) f
learner is modeled by the frozen VLM fVLM(·) used for VAD with a specific prompt template ✓ that guide fVLM(·) to conduct a learning task by pondering on current guiding questions Qt. We denote the learner agent as f
l (t) f
learner (x) = fVLM(x; (✓ , Qt)), where x is the input in a learning task, and Qt, the learnable guiding questions applied in each iteration t, constitutes the core parameters that distinguish the learner between iterations. Meanwhile, we introduce an optimizer f
o (t) f
opt to assess the quality of the</p>
<p>predictions of the learner and to optimize Qt. W.l.o.g., we use the same frozen VLM fVLM to model the optimizer. As demonstrated in Fig. 2, we provide another specific prompt template for the learner to follow to optimize Qt, so we denote the optimizer agent as f
o (t) f
opt (z) = fVLM(z; ( , Qt)) , where z is its input and is the instruction to improve Qt . It is important to note that f
l (t) f
learner 6= f
o (t) f
opt because f
l (t) f
learner follows (✓ , Qt) to conduct a learning task, while f
o (t) f
opt follows ( , Qt) to refine Qt .</p>
<p>Learning Task for flearner. The learner executes the &ldquo;forward pass&rdquo; and outputs a prediction. Recall that we only use the original coarsely labeled information for training. Thus, we design a binary classification task for flearner, which accounts for the temporal nature of video data, the sparsity of anomalies, and the weak supervision in VAD datasets. In this task, the job of the learner flearner is to produce a binary classification prediction Y ˆ (j) to determine whether there is an anomaly in the video based on the sampled frames V ˜ (j) . As shown in Fig. 2, we explain the task in natural language in the &ldquo;Model Description&rdquo; section in ✓. Guiding questions Qt are inserted in the &ldquo;Prompt Questions&rdquo; section in ✓ to elicit reasoning of the VLM. This template design is based on the prompt structures used in VML, with targeted modifications to help the learner effectively address this WS learning task. Given ✓ and a sampled frame set V ˜ (j)
, the learner will output a prediction as</p>
<!-- formula-not-decoded -->
<p>where Y ˆ (j) = 1 if the learner thinks there is an anomaly af-</p>
<p>ter skimming across the sampled frames V ˜ (j) and reasoning through the guiding questions Qt, and otherwise, Y
i ˆ Y
i = 0 . Optimization Step in fopt. The optimizer executes the &ldquo;backward pass&rdquo; to update the questions Qt via a mini-batch (batch size is n). Suppose the visual input in a batch is Vbatch = [V
b ˜ (1) V
batch, ··· , V
b ˜ (n) V
batch ] and the corresponding ground truths are Ybatch = [Y
b (1) Y
batch, ··· , Y
b (n) Y
batch ]. The learner generates prediction as Y
b ˆ Y
batch = [Y
b ˆ (1) Y
batch, ··· , Y
b ˆ (n) Y
batch ] with the current questions Qt by Eq. (1). The optimizer will output a new set of questions Qt+1 by following the prompt with batched data. We denote the optimization step as</p>
<!-- formula-not-decoded -->
<p>where Qt+1 is a new set of guiding questions constructed from f
o (t) f
opt owing to its text generation and instruction following abilities after reading .</p>

<h2 class="relative group">3.3. Inference in VERA
    <div id="33-inference-in-vera" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#33-inference-in-vera" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>During training, we denote the one with the largest validation accuracy as Q ⇤ . In inference, given Q ⇤ , VERA yields fine-grained anomaly score Y ˆ for a test video V via a coarse-to-fine process shown in Fig. 3 .</p>
<p>Step 1: Initial Anomaly Scores via Learned Guiding Questions. We divide the video into segments and analyze each segment independently first. Following [52], we perform equidistant frame sampling within V to obtain the set of segment centers C = {I1, Id+1 , ··· , I(h 1)·d+1 }, where d is the interval between centers and h = floor(F/d) is the total number of segments. For each center frame I( u 1)·d+1 (1  u  h), we define a 10-second window around it as the u -th segment, within which we uniformly sample 8 frames. We denote the sampled frame set in the u-th segment as Vu Vu. Next, we input Vu Vu in fVLM with the prompt (✓ , Q ⇤ ) to get the initial score</p>
<!-- formula-not-decoded -->
<p>where y˜ ˜ u = 1 if fVLM thinks the segment contains an anomaly after reasoning via Q ⇤ with Vu Vu , and otherwise, y ˜ u = 0. By repeating Eq. (3) for each segment, we have a segment-level initial anomaly score set Y ˜ = [˜y1 , ··· , y ˜ h ] . Step 2: Ensemble Segment-Level Anomaly Scores with Scene Context. Note that the scores derived above only examine a short moment in a long video without considering any context. To resolve it, we refine the initial segment-level score by incorporating scene context—defined as preceding and following segments that contain similar elements, such as actors and background, to those in the current segment.</p>
<p>We measure the relevance between different video segments by the cosine similarity of their feature representations [22], extracted by a pretrained vision feature extractor</p>
<p>Step 1: Initial Anomaly Scores via Learned Guiding Questions</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_cf37e2f632465dd4e8ea3691d3caaba7ebce4ce004c565367c0c069f275fcfd9.png"
    ></figure>
<p>Figure 3. VERA computes anomaly scores with Q ⇤ in three steps.</p>
<p>g , e.g., ImageBind [10]. For the u-th segment Vu Vu , its similarity with any segment Vw Vw (1  w  h) is sim(u, w) = cos ⇣ e u · e w ||e u ||·||e w || ⌘
, where cos denotes the cosine function, and e u = g(Vu Vu ) and e w = g(Vw Vw ) represent their features. Let  u = [ (1) u ,&hellip;,  (K) u ] denote the indices of the top-K segments similar to Vu Vu . We refine the anomaly score by</p>
<!-- formula-not-decoded -->
<p>where y¯ ¯ u is an ensemble of initial scores of top-K video segments relevant to Vu Vu . Here, the initial score of each retrieved segment is weighted by a factor derived from the cosine similarity and normalized by the Softmax function (with ⌧ as the temperature hyperparameter). Accordingly, scenes with greater similarity are assigned higher weights, making the ensemble score a more comprehensive reflection of anomalies with the video context. By applying Eq. (4) for all segments, we obtain Y ¯ = [¯y1 ,&hellip;, y ¯ h ] .</p>
<p>Step 3: Frame-level Anomaly Scoring with Temporal Context. Given Y ¯ , we aim to incorporate temporal context to capture how events evolve over time when computing frame-level anomaly scores, for the abnormality of an event often depends on the timing and progression of observed activities. To detail, we first apply Gaussian smoothing [11] to aggregate local temporal context into the segment-level anomaly scores. We denote the Gaussian kernel (suppose the filter size is !) as G(p) = exp( p 2 2 2 1 ) where p is the distance from the kernel center and 1 is the variance. We update segment-level scores as ¯ = Y ¯ ⇤ G = [¯1 , ··· , ¯ h ] , where ⇤ is the convolution operation. Next, we integrate global temporal context by position weighting. With ¯ , we flatten it into frame-level scores by assigning the score ¯ ¯ u to each frame in the u-th segment, i.e ., [I( u 1)·d+1, ··· , Iu Iu· d ] . We denote the frame-level score sequence after flattening as [⇢1 , ··· , ⇢F ]. We then apply the Gaussian function to encode position weights as w(i) = exp ⇣ (ic) 2 2 2 2 ⌘
, where</p>
<p>i (1  i  F) is any frame index, c = floor(F/2) is the center frame index, and 2 is the variance. The anomaly score for the i-th frame is:</p>
<!-- formula-not-decoded -->
<p>This operation scales the score ⇢i, diminishing the anomaly score for frames near the beginning and end of the event. This helps better capture the temporal progression of anomalies: the score gradually increases as the anomaly reaches its peak and decreases afterward. The final scores is denoted as Y ˆ = [ˆy1 ,&hellip;, y ˆ F ] after applying Eq. (5).</p>
<p>Explainable VAD by VERA. When using template ✓ embedded with Q ⇤ to compute Y ˆ , we ask the VLM to &ldquo;provide an explanation in one sentence&rdquo; when reasoning, and VLM will explain the anomaly score it assigns based on Q ⇤ .</p>

<h2 class="relative group">4. Experiments and Results
    <div id="4-experiments-and-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#4-experiments-and-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this section, we present an evaluation of VERA as follows, addressing key questions of interest including: (Q1) Does it enhance the effectiveness of frozen VLMs in VAD? (Q2) Is its design reasonable and well-structured? (Q3) How well does it generalize across different scenarios?</p>

<h2 class="relative group">4.1. Experimental Settings
    <div id="41-experimental-settings" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#41-experimental-settings" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Datasets. We conduct experiments on two large-scale VAD datasets: (1) UCF-Crime [32] collected from surveillance videos with 13 types of anomalies and 290 (140 abnormal) test videos (2.13 minutes long on average). (2) XDViolence [42] with 6 anomaly categories and 800 (500 abnormal) test videos (1.62 minutes long on average).</p>
<p>Metrics. Following approaches in [52 , 55], we mainly evaluate VAD performance using the Area Under the Curve (AUC) of the frame-level Receiver Operating Characteristic (ROC) curve, as it provides a comprehensive measure of model performance across all thresholds.</p>
<p>Baselines. We categorize baselines into non-explainable approaches and explainable ones as [55] does. Nonexplainable ones are obtained by WS learning [6 , 9 , 15 , 17 , 19 , 32 , 36 , 41 – 43 , 50 , 51 , 57] and unsupervised learning [13 , 25 , 34 , 35 , 37 , 38]. These non-explainable approaches cannot provide language-based explanations for VAD. For explainable approaches, we use LAVAD [52], Holmes-VAD [55], and VADor [26] as representatives of Pipeline 1 and Pipeline 2 shown in Fig. 1. It should be noted that [46] does not report performance on UCF-Crime and XD-Violence. Additionally, we include zero-shot (ZS) VAD by frozen VLMs designed by [52] as baselines.</p>
<p>Implementation of VERA. In our experiments, we choose a small VLM, InternVL2-8B [7], as the backbone fVLM for building VERA by default, if not otherwise specified. We also explore other backbones, such as Qwen2-VL-7B [40] and InternVL2-40B [7] for ablation. We train Q for no more than 10 epochs, with a validation accuracy calculated every 100 iterations to determine Q ⇤ . We set n as 2, S as 8, and m as 5 for training. The initial questions Q0 is &ldquo;1. Is there any suspicious person or object that looks unusual in this scene? 2. Is there any behavior that looks unusual in this scene?&rdquo;, inspired by previous VAD methods [13 , 43], which assume anomalies appear with unusual appearance or motions.</p>

<h2 class="relative group">4.2. Comparison to State-of-the-art Methods
    <div id="42-comparison-to-state-of-the-art-methods" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#42-comparison-to-state-of-the-art-methods" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We address Q1 by empirically comparing VERA to existing VAD methods. First, in Table 2, VERA achieves the highest AUC among explainable VAD methods on UCF-Crime, outperforming Holmes-VAD and VADor (without IT, as reported in their papers) in a fair comparison. Importantly, unlike these methods, VERA does not need to modify the model parameters, demonstrating its suitability to directly adapt VLM to the VAD task with minimal training requirements. Moreover, VERA surpasses LAVAD by 6% in AUC on UCF-Crime, uniquely integrating both description and reasoning capabilities in VAD. Compared to non-explainable methods, VERA achieves AUC performance that is comparable to one of the top-performing</p>
<p>Table 2. AUC (%) on UCF-Crime. No IT is used for Holmes-VAD and VADor.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>AUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Non-explainable VAD Methods</td>
          <td>Methods</td>
      </tr>
      <tr>
          <td>Wu et al. [42]</td>
          <td>82.44</td>
      </tr>
      <tr>
          <td>OVVAD [43]</td>
          <td>86.40</td>
      </tr>
      <tr>
          <td>S3R [41]</td>
          <td>85.99</td>
      </tr>
      <tr>
          <td>RTFM [36]</td>
          <td>84.30</td>
      </tr>
      <tr>
          <td>MSL [19]</td>
          <td>85.62</td>
      </tr>
      <tr>
          <td>MGFN [6]</td>
          <td>86.98</td>
      </tr>
      <tr>
          <td>SSRL [17]</td>
          <td>87.43</td>
      </tr>
      <tr>
          <td>CLIP-TSA [15]</td>
          <td>87.58</td>
      </tr>
      <tr>
          <td>Sultani et al. [32]</td>
          <td>77.92</td>
      </tr>
      <tr>
          <td>GCL [51]</td>
          <td>79.84</td>
      </tr>
      <tr>
          <td>GCN [57]</td>
          <td>82.12</td>
      </tr>
      <tr>
          <td>MIST [9]</td>
          <td>82.30</td>
      </tr>
      <tr>
          <td>CLAWS [50]</td>
          <td>83.03</td>
      </tr>
      <tr>
          <td>DYANNET [35]</td>
          <td>84.50</td>
      </tr>
      <tr>
          <td>Tur el al. [37]</td>
          <td>66.85</td>
      </tr>
      <tr>
          <td>GODS [38]</td>
          <td>70.46</td>
      </tr>
      <tr>
          <td>Explainable VAD Methods</td>
          <td>Explainable VAD Methods</td>
      </tr>
      <tr>
          <td>LAVAD [52]</td>
          <td>80.28</td>
      </tr>
      <tr>
          <td>Holmes-VAD [55]</td>
          <td>84.61</td>
      </tr>
      <tr>
          <td>VADor [26]</td>
          <td>85.90</td>
      </tr>
      <tr>
          <td>ZS CLIP [52]</td>
          <td>53.16</td>
      </tr>
      <tr>
          <td>ZS IMAGEBIND-I [52]</td>
          <td>53.65</td>
      </tr>
      <tr>
          <td>ZS IMAGEBIND-V [52]</td>
          <td>55.78</td>
      </tr>
      <tr>
          <td>LLAVA-1.5 [20]</td>
          <td>72.84</td>
      </tr>
      <tr>
          <td>VERA</td>
          <td>86.55</td>
      </tr>
  </tbody>
</table>
<p>methods, CLIP-TSA, on UCF-Crime, while offering the additional advantage of explainable predictions.</p>
<p>Similar advantages are also observed in Table 3 for XD-Violence. Considering multiple factors, including performance, training efficiency, system integration, and explainability, VERA stands out as a promising pipeline for VLMs in VAD.</p>

<h2 class="relative group">4.3. Ablation Studies
    <div id="43-ablation-studies" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#43-ablation-studies" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We perform necessary ablation studies on UCF-Crime to answer both Q2 and Q3</p>
<p>Table 3. AUC (%) on XD-Violence.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>AUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Non-Explainable VAD Methods</td>
          <td>Non-Explainable VAD Methods</td>
      </tr>
      <tr>
          <td>Hasan et al. [13]</td>
          <td>50.32</td>
      </tr>
      <tr>
          <td>Lu et al. [25]</td>
          <td>53.56</td>
      </tr>
      <tr>
          <td>BODS [38]</td>
          <td>57.32</td>
      </tr>
      <tr>
          <td>GODS [38]</td>
          <td>61.56</td>
      </tr>
      <tr>
          <td>RareAnom [34]</td>
          <td>68.33</td>
      </tr>
      <tr>
          <td>Explainable VAD Methods</td>
          <td>Explainable VAD Methods</td>
      </tr>
      <tr>
          <td>LAVAD [52]</td>
          <td>85.36</td>
      </tr>
      <tr>
          <td>ZS CLIP [52]</td>
          <td>38.21</td>
      </tr>
      <tr>
          <td>ZS IMAGEBIND-I [52]</td>
          <td>58.81</td>
      </tr>
      <tr>
          <td>ZS IMAGEBIND-V [52]</td>
          <td>55.06</td>
      </tr>
      <tr>
          <td>LLAVA-1.5 [20]</td>
          <td>79.62</td>
      </tr>
      <tr>
          <td>VERA</td>
          <td>88.26</td>
      </tr>
  </tbody>
</table>
<p>for a comprehensive evaluation on our design choices.</p>
<p>Frame Sampling Strategy in Training. We compare three frame sampling strategies for obtaining each V ˜ (j) in training: uniform sampling, random sampling, and TSN sampling (random sampling from equally divided segments).</p>
<p>Table 4 shows that uniform sampling performs the best (with batch size n = 2 and S = 8). This is because uniform sampling preserves the temporal structure and maintains consistent motion patterns throughout</p>
<p>Table 4. Sampling strategies explored in VERA training.</p>
<table>
  <thead>
      <tr>
          <th>Strategy</th>
          <th>AUC (%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Random [3]</td>
          <td>83.63</td>
      </tr>
      <tr>
          <td>TSN [39]</td>
          <td>82.63</td>
      </tr>
      <tr>
          <td>Uniform [54]</td>
          <td>86.55</td>
      </tr>
  </tbody>
</table>
<p>the long video, making it easier for VLMs to understand the video and update Q .</p>
<p>Table 5. The way we obtain guiding questions affects AUC substantially.</p>
<table>
  <thead>
      <tr>
          <th>Question Type</th>
          <th>AUC (%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>No questions</td>
          <td>78.81</td>
      </tr>
      <tr>
          <td>Manually written questions by human</td>
          <td>81.15</td>
      </tr>
      <tr>
          <td>Learned questions w/o iteratively inputting Vbatch in Eq. (2)</td>
          <td>78.06</td>
      </tr>
      <tr>
          <td>Iteratively learned questions (used in VERA)</td>
          <td>86.55</td>
      </tr>
  </tbody>
</table>
<p>How to Obtain Guiding Questions Q for VLM. As seen in Table 5, if the guiding questions are not incorporated into the VLM prompt, the AUC will drop largely to 78.81%, confirming the need to use simpler and more focused questions to provoke reasoning in the VLMs for VAD. Meanwhile, if we use manually written questions (Q0), the performance is suboptimal with an 81.15% AUC, which shows the need to use VL to find guiding questions. Lastly, if we only input batched predictions Y
b ˆ Y
batch and ground truths Yb Ybatch without inputting Vbatch in the optimizer, the Q updated in this way will dumb the VLMs and make it have a low AUC. Thus, inputting video frames as Eq. (2) does is necessary to learn good Q .</p>
<p>Number of Questions m . As shown in Fig. 4, when m is set to 1, the reasoning is limited to a single perspective, resulting in a lower AUC. As m increases up to 5, the model captures more comprehensive anomaly patterns, leading to improved AUC. However, increasing m</p>
<p>Figure 4. Effect of the number of guiding questions on AUC.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000004_15c773950227e7b8ea57bb945dde1a52fa1ffb85249802dab31169c7f2c4d042.png"
    ></figure>
<p>beyond 5 yields no significant gains. Therefore, we set m to 5 by default in VERA, if not otherwise specified.</p>
<p>Table 6. Ablation study of each step in VERA’s inference.</p>
<table>
  <thead>
      <tr>
          <th>Operation</th>
          <th>AUC (%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Initial (Step 1)</td>
          <td>76.10</td>
      </tr>
      <tr>
          <td>Initial + Retrieval (Step 2)</td>
          <td>84.53 (+8.43)</td>
      </tr>
      <tr>
          <td>Initial + Retrieval + Smoothing (Step 3)</td>
          <td>85.48 (+0.95)</td>
      </tr>
      <tr>
          <td>Initial + Retrieval + Smoothing + Weighting (Step 3)</td>
          <td>86.55 (+1.07)</td>
      </tr>
  </tbody>
</table>
<p>Coarse-to-Fine Anomaly Score Computation. We also validate the anomaly score computation by VERA. Table 6</p>
<p>shows the AUC is 76.10% when using the flattened initial score obtained in Step 1, and leveraging retrieved segments in Step 2 significantly boosts the AUC to 84.53%, highlighting the effectiveness of incorporating ensemble scores based on scene context. Meanwhile, smoothing and weighting in Step 3 further improves the AUC by around 1% each, verifying the benefit of integrating temporal context.</p>
<p>Generalizability Test. We further examine the generalizability of VERA across different model sizes, VLM architectures, and datasets to address Q3.</p>
<p>First, we apply VERA to InternVL2-40B, a larger model in the InternVL2 family compared to InternVL2-8B. As shown in Table 7, InternVL2-40B achieves effective AUC performance, slightly exceeding that of InternVL2-8B, indicating that VL in VERA enables models of various scales to identify a Q suitable</p>
<p>Table 9. AUC (%) across datasets.</p>
<table>
  <thead>
      <tr>
          <th>fVLM</th>
          <th>Source of Q</th>
          <th>Source of Q</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>fVLM</td>
          <td>InternVL2-8B</td>
          <td>8B InternVL2-40B</td>
      </tr>
      <tr>
          <td>InternVL2-8B</td>
          <td>86.55</td>
          <td>80.43</td>
      </tr>
      <tr>
          <td>InternVL2-40B</td>
          <td>85.24</td>
          <td>86.72</td>
      </tr>
      <tr>
          <td>Table 7. AUC (%) acro</td>
          <td>AUC (%) across model sizes .</td>
          <td>AUC (%) across model sizes .</td>
      </tr>
      <tr>
          <td>fVLM</td>
          <td>Source of Q InternVL28B Qwen2VL7B</td>
          <td>Source of Q InternVL28B Qwen2VL7B</td>
      </tr>
      <tr>
          <td>InternVL2-8B</td>
          <td>86.55</td>
          <td>81.37</td>
      </tr>
      <tr>
          <td>Qwen2-VL-7B</td>
          <td>79.60</td>
          <td>82.64</td>
      </tr>
      <tr>
          <td>Table 8. AUC (%) a</td>
          <td>UC (%) across architectures.</td>
          <td>UC (%) across architectures.</td>
      </tr>
      <tr>
          <td>Dataset</td>
          <td>Source of Q</td>
          <td>Source of Q</td>
      </tr>
      <tr>
          <td></td>
          <td>UCF-Crime X</td>
          <td>e XD-Violence</td>
      </tr>
      <tr>
          <td>UCF-Crime</td>
          <td>86.55</td>
          <td>88.26</td>
      </tr>
      <tr>
          <td>XD-Violence</td>
          <td>86.26</td>
          <td>88.26</td>
      </tr>
  </tbody>
</table>
<p>for their reasoning capabilities. Additionally, We also evaluate the transferability of Q across different scales and and observe an interesting phenomenon: the Q learned by InternVL2-8B remains effective for InternVL2-40B, but not vice versa. This is likely because the Q learned by the smaller model is readily interpretable by the larger model, whereas the Q derived from the larger model is more complex in syntactic structure and does not align well with the reasoning framework of the smaller model. Secondly, we select a different VLM, Qwen2-VL-7B [40], as the backbone for VERA. As shown in Table 8, while the AUC achieved with Qwen2-VL-7B is lower than that with InternVL2-8B, the VL in VERA remains effective, allowing it to outperform notable baselines such as LAVAD [52]. However, a notable gap exists when transferring Q across different model architectures in Table 8. Developing a universal Q that can effectively elicit reasoning capabilities across various VLM structures would be an promising direction for future research. Lastly, we observe that the transferability of Q depends on the training dataset. From Table 9, we observe that transferring Q learned from UCFCrime to XD-Violence results in a smaller performance drop compared to the reverse case. This suggests the source dataset is crucial to the transferability of Q across datasets.</p>

<h2 class="relative group">4.4. Qualitative Results and Case Studies
    <div id="44-qualitative-results-and-case-studies" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#44-qualitative-results-and-case-studies" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>W.l.o.g., we take one video on UCF-Crime to illustrate the explainability brought by the learned Q ⇤ qualitatively (on</p>
<p>1</p>
<p>Learned Guiding Questions !</p>
<p>∗</p>
<p>in VERA</p>
<p>.</p>
<p>Are there any people in the video who are not in their typical positions or engaging in</p>
<p>activities that are not consistent with their usual behavior?</p>
<p>2</p>
<p>.</p>
<p>Are there any vehicles in the video that are not in their typical positions or being used</p>
<p>in a way that is not consistent with their usual function?</p>
<p>3</p>
<p>.</p>
<p>Are there any objects in the video that are not in their typical positions or being used</p>
<p>in a way that is not consistent with their usual function?</p>
<p>4</p>
<p>.</p>
<p>Is there any visible damage or unusual movement</p>
<p>in the video that indicates an</p>
<p>anomaly?</p>
<p>5</p>
<p>.</p>
<p>Are there any unusual sounds or noises in the video that suggest an anomaly?</p>
<p>Figure 5. Given Q ⇤ by VERA, the frozen VLM (InternVL2-8B) will reason and explain the scene based on it. For illustration, we take as an example the video &ldquo;Arrest007 x264&rdquo; from UCF-Crime and include 6 scenes here. The complete anomaly scores are shown in Fig. 8 .</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000005_196c39891e3c6c3dba405f63b3bf35832ab2e070f930e5a7679e28a7a0fab2d2.png"
    ></figure>
<p>UCF-Crime Q ⇤ is &ldquo;1. Are there any people in the video who are not in their typical positions or engaging in activities that are not consistent with their usual behavior? 2. Are there any vehicles in the video that are not in their typical positions or being used in a way that is not consistent with their usual function? 3. Are there any objects in the video that are not in their typical positions or being used in a way that is not consistent with their usual function? 4. Is there any visible damage or unusual movement in the video that indicates an anomaly? 5. Are there any unusual sounds or noises in the video that suggest an anomaly?&rdquo;). As shown in Fig. 5, the main anomaly in this video is that a man tries to steal money from the washing machines in a laundromat and is arrested after being found by the police. In the selected 6 main video segments, the frozen VLM with VERA&rsquo;s learned questions is able to explain the scene by closely following the detailed anomaly characterization of the five learned guiding questions. W.l.o.g., we take the first 3 segments in Fig. 5 for instance to closely compare the caption quality with LAVAD, a representative baseline. As shown in Fig. 6, VERA&rsquo;s captions include both precise descriptions (bold text) and reasoning (text in purple) about anomalies, while LAVAD&rsquo;s captions contain only plain descriptions. This difference owes to VERA&rsquo;s learned guiding questions, which transform VLM&rsquo;s thinking and phrasing.</p>
<p>A more interesting advantage of VERA is that it allows humans to further interact with VLMs because it retains the general question-answering ability of pretrained VLMs. This is because VERA does not require finetuning of the VLM backbone weights. Although finetuning VLMs with parameter-efficient methods like [14 , 24 , 29] is easy and computationally tractable, instruction-tuned models still inevitably lose the flexibility to handle general questions (due to catastrophic forgetting), as they are trained to respond to certain queries with fixed answer styles. In contrast, as shown in Fig. 7, the learned Q ⇤ can steer reasoning in a frozen VLM while allowing it to flexibly answer openended (like follow-up or counterfactual) questions, which is</p>
<p>Figure 6. Qualitative comparison between VERA and LAVAD.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000006_b9fae0c3ec06aa49651e08233ad1d567115c3bc1d6722b1586e7d6233c7240f4.png"
    ></figure>
<p>Figure 7. VERA can take open-ended questions and interact with humans.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000007_584091d3c688abac2dc99208f70c9f92185aa1b37ce3ef0be3e2c84517e78bb9.png"
    ></figure>
<p>an important ability lost in IT-based models.</p>
<p>Moreover, as shown in Fig. 8, owing to the proposed coarse-to-fine anomaly scoring, the anomaly score dynamics from VERA well represent the actual real-time anomaly level in this video and gradually increases to nearly 1 when the man is being arrested. This result verifies that VERA allows VLMs to effec-</p>
<p>Figure 8. Anomaly scores generated by VERA (with InternVL2-8B) in &ldquo;Arrest007 x264&rdquo; from UCF-Crime.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000008_972ff40ca1667c6cba40f6af195d664c4c0f40c42ba8f73e554786d349bc4c9d.png"
    ></figure>
<p>tively identify anomalies with a holistic model, reducing the manpower and computational overhead for VAD.</p>

<h2 class="relative group">5. Concluding Remarks
    <div id="5-concluding-remarks" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#5-concluding-remarks" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We propose a novel pipeline, VERA, which can effectively elicit the reasoning ability from VLMs to perform explainable VAD without additional computation overhead. This is done through an effective and novel application of verbalized machine learning [45] to VLM. In training, VERA obtains the guiding questions detailing anomaly patterns through the verbal interaction between the learner and the optimizer agents. In inference, VERA uses them to enhance VLMs for identifying anomalies and compute frame-level anomaly scores in a coarse-to-fine process. Experimental results validate the effectiveness of the VERA framework in achieving state-of-the-art explainable VAD performance.</p>

<h2 class="relative group">References
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 1</li>
<li>[2] Daniel Bogdoll, Maximilian Nitsche, and J Marius Zollner. ¨ ¨ Anomaly detection in autonomous driving: A survey. In CVPR Workshops, 2022. 1</li>
<li>[3] Meinardus Boris, Batra Anil, Rohrbach Anna, and Rohrbach Marcus. The surprising effectiveness of multimodal large language models for video moment retrieval. arXiv preprint arXiv:2406.18113, 2024. 7</li>
<li>[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. 1</li>
<li>[5] Junxi Chen, Liang Li, Li Su, Zheng-jun Zha, and Qingming Huang. Prompt-enhanced multiple instance learning for weakly supervised video anomaly detection. In CVPR , 2024. 3</li>
<li>[6] Yingxian Chen, Zhengzhe Liu, Baoheng Zhang, Wilton Fok, Xiaojuan Qi, and Yik-Chung Wu. Mgfn: Magnitudecontrastive glance-and-focus network for weakly-supervised video anomaly detection. In AAAI, 2023. 6 , 14</li>
<li>[7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR , 2024. 1 , 2 , 6</li>
<li>[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1</li>
<li>[9] Jia-Chang Feng, Fa-Ting Hong, and Wei-Shi Zheng. Mist: Multiple instance self-training framework for video anomaly detection. In CVPR, 2021. 6</li>
<li>[10] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023. 5 , 15</li>
<li>[11] Rafael C Gonzalez. Digital image processing. Pearson education india, 2009. 5</li>
<li>[12] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In CVPR, 2024. 3</li>
<li>[13] Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K</li>
</ul>
<p>Roy-Chowdhury, and Larry S Davis. Learning temporal regularity in video sequences. In CVPR, 2016. 6 , 11</p>
<ul>
<li>
<p>[14] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2021. 8</p>
</li>
<li>
<p>[15] Hyekang Kevin Joo, Khoa Vo, Kashu Yamazaki, and Ngan Le. Clip-tsa: Clip-assisted temporal self-attention for weakly-supervised video anomaly detection. In ICIP, 2023. 6 , 14</p>
</li>
<li>
<p>[16] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 4</p>
</li>
<li>
<p>[17] Guoqiu Li, Guanxiong Cai, Xingyu Zeng, and Rui Zhao. Scale-aware spatio-temporal relation learning for video anomaly detection. In ECCV, 2022. 6</p>
</li>
<li>
<p>[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML , 2023. 1</p>
</li>
<li>
<p>[19] Shuo Li, Fang Liu, and Licheng Jiao. Self-training multisequence learning with transformer for weakly supervised video anomaly detection. In AAAI, 2022. 6 , 14</p>
</li>
<li>
<p>[20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR , 2024. 6 , 14</p>
</li>
<li>
<p>[21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2024. 1 , 3</p>
</li>
<li>
<p>[22] Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep hyperspherical learning. In NeurIPS, 2017. 5</p>
</li>
<li>
<p>[23] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Future frame prediction for anomaly detection–a new baseline. In CVPR, 2018. 3</p>
</li>
<li>
<p>[24] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, et al. Parameter-efficient orthogonal finetuning via butterfly factorization. In ICLR, 2024. 8</p>
</li>
<li>
<p>[25] Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event detection at 150 fps in matlab. In ICCV, 2013. 3 , 6</p>
</li>
<li>
<p>[26] Hui Lv and Qianru Sun. Video anomaly detection and explanation via large language models. arXiv preprint arXiv:2401.05702, 2024. 1 , 2 , 3 , 6</p>
</li>
<li>
<p>[27] Hui Lv, Zhongqi Yue, Qianru Sun, Bin Luo, Zhen Cui, and Hanwang Zhang. Unbiased multiple instance learning for weakly supervised video anomaly detection. In CVPR, 2023. 3</p>
</li>
<li>
<p>[28] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero-shot image classification. In ICCV, 2023. 3</p>
</li>
<li>
<p>[29] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Scholkopf. Controlling text-to-image diffusion by orthogo- ¨ ¨ nal finetuning. In NeurIPS, 2023. 8</p>
</li>
<li>
<p>[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 18</p>
</li>
<li>
<p>[31] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Scholkopf, Thomas Brox, and Peter Gehler. Towards total ¨ ¨ recall in industrial anomaly detection. In CVPR, 2022. 1</p>
</li>
<li>
<p>[32] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In CVPR, 2018. 2 , 3 , 6 , 14</p>
</li>
<li>
<p>[33] Jiaqi Tang, Hao Lu, Ruizheng Wu, Xiaogang Xu, Ke Ma, Cheng Fang, Bin Guo, Jiangbo Lu, Qifeng Chen, and Yingcong Chen. Hawk: Learning to understand open-world video anomalies. Advances in Neural Information Processing Systems, 37:139751–139785, 2024. 3</p>
</li>
<li>
<p>[34] Kamalakar Vijay Thakare, Debi Prosad Dogra, Heeseung Choi, Haksub Kim, and Ig-Jae Kim. Rareanom: A benchmark video dataset for rare type anomalies. Pattern Recognition, 140:109567, 2023. 6</p>
</li>
<li>
<p>[35] Kamalakar Vijay Thakare, Yash Raghuwanshi, Debi Prosad Dogra, Heeseung Choi, and Ig-Jae Kim. Dyannet: A scene dynamicity guided self-trained video anomaly detection network. In WACV, 2023. 6</p>
</li>
<li>
<p>[36] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W Verjans, and Gustavo Carneiro. Weakly-supervised video anomaly detection with robust temporal feature magnitude learning. In ICCV, 2021. 6 , 14</p>
</li>
<li>
<p>[37] Anil Osman Tur, Nicola Dall&rsquo;Asen, Cigdem Beyan, and Elisa Ricci. Unsupervised video anomaly detection with diffusion models conditioned on compact motion representations. In International Conference on Image Analysis and Processing, 2023. 3 , 6</p>
</li>
<li>
<p>[38] Jue Wang and Anoop Cherian. Gods: Generalized one-class discriminative subspaces for anomaly detection. In ICCV, V, 2019. 3 , 6</p>
</li>
<li>
<p>[39] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, 2016. 7</p>
</li>
<li>
<p>[40] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language model&rsquo;s perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 , 7</p>
</li>
<li>
<p>[41] Jhih-Ciang Wu, He-Yen Hsieh, Ding-Jie Chen, Chiou-Shann Fuh, and Tyng-Luh Liu. Self-supervised sparse representation for video anomaly detection. In ECCV, 2022. 6 , 14</p>
</li>
<li>
<p>[42] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. Not only look, but also listen: Learning multimodal violence detection under weak supervision. In ECCV, 2020. 2 , 3 , 6 , 14</p>
</li>
<li>
<p>[43] Peng Wu, Xuerong Zhou, Guansong Pang, Yujia Sun, Jing Liu, Peng Wang, and Yanning Zhang. Open-vocabulary video anomaly detection. In CVPR, pages 18297–18307, 2024. 6 , 11 , 14</p>
</li>
<li>
<p>[44] Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng Wang, and Yanning Zhang. Vadclip: Adapting vision-language models for weakly supervised video anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6074–6082, 2024. 3</p>
</li>
<li>
<p>[45] Tim Z Xiao, Robert Bamler, Bernhard Scholkopf, and ¨ ¨ Weiyang Liu. Verbalized machine learning: Revisiting machine learning with language models. arXiv preprint arXiv:2406.04344, 2024. 3 , 4 , 8</p>
</li>
<li>
<p>[46] Yuchen Yang, Kwonjoon Lee, Behzad Dariush, Yinzhi Cao, and Shao-Yuan Lo. Follow the rules: reasoning for video anomaly detection with large language models. arXiv preprint arXiv:2407.10299, 2024. 1 , 3 , 6</p>
</li>
<li>
<p>[47] Zhiwei Yang, Jing Liu, and Peng Wu. Text prompt with normality guidance for weakly supervised video anomaly detection. In CVPR, 2024. 3</p>
</li>
<li>
<p>[48] Muchao Ye, Xiaojiang Peng, Weihao Gan, Wei Wu, and Yu Qiao. Anopcn: Video anomaly detection via deep predictive coding network. In ACM international conference on multimedia, 2019. 3</p>
</li>
<li>
<p>[49] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic &ldquo;differentiation&rdquo; via text. arXiv preprint arXiv:2406.07496, 2024. 3</p>
</li>
<li>
<p>[50] Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, and Seung-Ik Lee. Claws: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection. In ECCV, 2020. 6</p>
</li>
<li>
<p>[51] M Zaigham Zaheer, Arif Mahmood, M Haris Khan, Mattia Segu, Fisher Yu, and Seung-Ik Lee. Generative cooperative learning for unsupervised video anomaly detection. In CVPR, 2022. 6</p>
</li>
<li>
<p>[52] Luca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, and Elisa Ricci. Harnessing large language models for training-free video anomaly detection. In CVPR , 2024. 1 , 3 , 5 , 6 , 7 , 14 , 15</p>
</li>
<li>
<p>[53] Chen Zhang, Guorong Li, Yuankai Qi, Shuhui Wang, Laiyun Qing, Qingming Huang, and Ming-Hsuan Yang. Exploiting completeness and uncertainty of pseudo labels for weakly supervised video anomaly detection. In CVPR, 2023. 3</p>
</li>
<li>
<p>[54] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In EMNLP, 2023. 3 , 7</p>
</li>
<li>
<p>[55] Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Chuchu Han, Xiaonan Huang, Changxin Gao, Yuehuan Wang, and Nong Sang. Holmes-vad: Towards unbiased and explainable video anomaly detection via multi-modal llm. arXiv preprint arXiv:2406.12235, 2024. 1 , 2 , 3 , 6 , 14</p>
</li>
<li>
<p>[56] Menghao Zhang, Jingyu Wang, Qi Qi, Haifeng Sun, Zirui Zhuang, Pengfei Ren, Ruilong Ma, and Jianxin Liao. Multiscale video anomaly detection by multi-grained spatiotemporal representation learning. In CVPR, 2024. 3</p>
</li>
<li>
<p>[57] Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H Li, and Ge Li. Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection. In CVPR, 2019. 6</p>
</li>
<li>
<p>[58] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In ICLR, 2024. 1</p>
</li>
</ul>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/Ye_VERA_Explainable_Video_Anomaly_Detection_via_Verbalized_Learning_of_Vision-Language_CVPR_2025_paper.md"
          data-oid-likes="likes_papers/Ye_VERA_Explainable_Video_Anomaly_Detection_via_Verbalized_Learning_of_Vision-Language_CVPR_2025_paper.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/zanella_harnessing_large_language_models_for_training-free_video_anomaly_detection_cvpr_2024_paper/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/wu_open-vocabulary_video_anomaly_detection_cvpr_2024_paper/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
