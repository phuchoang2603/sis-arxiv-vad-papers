<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/exploring-large-vision-language-models-for-robust-and-efficient-industrial-anomaly-detection/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/exploring-large-vision-language-models-for-robust-and-efficient-industrial-anomaly-detection/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/exploring-large-vision-language-models-for-robust-and-efficient-industrial-anomaly-detection\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "4850"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>4850 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">23 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">Exploring Large Vision-Language Models for Robust and Efficient Industrial Anomaly Detection
    <div id="exploring-large-vision-language-models-for-robust-and-efficient-industrial-anomaly-detection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#exploring-large-vision-language-models-for-robust-and-efficient-industrial-anomaly-detection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Kun Qian, Tianyu Sun, Wenhong Wang</p>
<p>Shangqiu University</p>
<p>Abstract. Industrial anomaly detection (IAD) plays a crucial role in the maintenance and quality control of manufacturing processes. In this paper, we propose a novel approach, Vision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD), which leverages large vision-language models (LVLMs) to improve both anomaly detection and localization in industrial settings. CLAD aligns visual and textual features into a shared embedding space using contrastive learning, ensuring that normal instances are grouped together while anomalies are pushed apart. Through extensive experiments on two benchmark industrial datasets, MVTec-AD and VisA, we demonstrate that CLAD outperforms state-of-the-art methods in both image-level anomaly detection and pixel-level anomaly localization. Additionally, we provide ablation studies and human evaluation to validate the importance of key components in our method. Our approach not only achieves superior performance but also enhances interpretability by accurately localizing anomalies, making it a promising solution for real-world industrial applications.</p>
<p>Keywords: Large Vision-Language Models · Industrial Anomaly Detection · Contrastive Learning.</p>

<h2 class="relative group">1 Introduction
    <div id="1-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Industrial anomaly detection (IAD) plays a critical role in ensuring the quality and safety of manufacturing processes, particularly in industries that rely on automated systems for production. Identifying unusual or faulty behavior in industrial systems—whether it involves machinery malfunctions, material defects, or process deviations—is crucial for minimizing downtime, reducing operational costs, and ensuring product quality. In recent years, the advent of large visionlanguage models (LVLMs) has provided a promising direction for advancing the state-of-the-art in IAD. LVLMs, which integrate both visual understanding and natural language processing, have demonstrated strong capabilities in tasks that involve both image and text data [1,2]. This dual-modal nature of LVLMs makes them particularly well-suited for industrial anomaly detection, where both visual patterns and textual descriptions (e.g., defect reports, product manuals, and machine logs) need to be comprehended in conjunction.</p>
<p>Despite their potential, the application of LVLMs to IAD faces several significant challenges. First, current IAD methods, which often rely solely on visual features or simple anomaly scoring, struggle to capture complex relationships between visual defects and textual descriptions, leading to limited generalization across different industrial scenarios. Second, many existing methods require large amounts of labeled anomaly data for training, which is not always available in real-world industrial settings. Furthermore, anomalies can often be subtle, requiring the model to understand fine-grained details that may not be immediately obvious from raw visual input alone. Finally, current models often fail to effectively leverage textual data, which could provide valuable contextual information that helps differentiate between normal and anomalous behavior.</p>
<p>Our motivation stems from the need to overcome these limitations by leveraging the power of LVLMs to align visual and textual information in a way that improves both anomaly detection and the interpretability of model predictions. In this work, we propose a novel method called Vision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD) . Our approach combines contrastive learning with cross-modal reasoning to create a joint embedding space for both visual and textual data. By doing so, we ensure that the model learns to distinguish normal and anomalous instances not just based on visual cues, but also by considering their textual context. This approach allows for the detection of both known and unseen anomalies in industrial environments, improving model generalization across diverse anomaly types and industrial setups. We further incorporate a contextualized reasoning module that enables the model to generate textual explanations for detected anomalies, thereby providing valuable insights into the model&rsquo;s decision-making process.</p>
<p>For evaluation, we conduct extensive experiments on two benchmark datasets: MVTec-AD [3] and VisA [4]. These datasets provide a comprehensive testbed for evaluating anomaly detection methods across different types of industrial objects and defects. We use a combination of image-level and pixel-level AUC (Area Under Curve) scores, as well as accuracy measures, to assess the performance of our model. Our results show that CLAD significantly outperforms existing methods in both anomaly detection and localization tasks, demonstrating a clear improvement in both accuracy and robustness compared to prior approaches such as AnomalyGPT [5], PaDiM [6], and PatchCore [7].</p>
<p>In summary, the main contributions of our work are as follows:</p>
<ul>
<li>– We propose a novel method for industrial anomaly detection, CLAD, which leverages contrastive learning and cross-modal reasoning to jointly model visual and textual information for anomaly detection.</li>
<li>– We introduce a contextualized reasoning module that enables the model to generate textual explanations for detected anomalies, improving both the interpretability and effectiveness of the detection process.</li>
<li>– We demonstrate the effectiveness of CLAD through comprehensive experiments on benchmark datasets, showing significant improvements over existing methods in both detection performance and generalization capabilities.</li>
</ul>

<h2 class="relative group">2 Related Work
    <div id="2-related-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-related-work" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">2.1 Large Vision-Language Models
    <div id="21-large-vision-language-models" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#21-large-vision-language-models" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Large Vision-Language Models (LVLMs) have emerged as a powerful framework for learning joint representations of images and text. One of the most influential models in this domain is CLIP (Contrastive Language-Image Pretraining) [8], which pre-trains a vision model and a language model by aligning images and their corresponding text descriptions in a shared embedding space. CLIP demonstrates impressive zero-shot performance across a variety of downstream tasks, enabling it to generalize well to unseen data without task-specific fine-tuning. Its architecture leverages a large-scale dataset of images and text to learn semantic correspondences, making it a highly versatile model for many vision-language tasks [9,10,11].</p>
<p>Following CLIP, DALL·E [12], another model developed by OpenAI, introduced the ability to generate images from textual descriptions using a transformerbased architecture. Unlike CLIP, which primarily focuses on representation learning, DALL·E explores the creative aspect of image generation, utilizing a large dataset of image-caption pairs to learn how to create novel images conditioned on textual inputs. This model has inspired further research into generative tasks within the vision-language domain.</p>
<p>Another notable approach is VisualBERT [13], which extends the transformerbased BERT architecture to the vision-language domain. VisualBERT integrates visual features directly into the language model [14,15], treating both image regions and text tokens as a unified sequence. It shows strong performance on tasks such as Visual Question Answering (VQA) and image captioning. Other works, such as UNITER [16] and VL-BERT [17], have similarly adapted transformer models for joint image-text representation learning. These models perform well across multiple vision-language tasks, achieving state-of-the-art results by pretraining on large-scale datasets and fine-tuning on task-specific data [18,19].</p>
<p>Additionally, more recent methods like ALBEF [20] have explored improved fusion strategies for vision-language alignment. ALBEF introduces an alignmentbefore-fusion approach, where image and text features are first aligned and then fused into a shared representation. This method has been shown to improve performance in tasks requiring fine-grained alignment between visual and textual modalities, such as image-text retrieval and VQA.</p>
<p>Finally, Florence [21], a recent contribution from Microsoft Research, is a foundational model designed for general-purpose vision and language understanding. Florence integrates large-scale vision and language pretraining, enabling it to achieve state-of-the-art performance across a wide range of visionand-language tasks. Its scalable architecture and pretraining framework push the boundaries of what is achievable in multimodal learning .</p>
<p>These models represent significant steps forward in the field of vision-language understanding. They have demonstrated that large-scale pretraining and the alignment of visual and textual data can lead to highly effective representations that generalize across a variety of tasks. However, despite these advancements,</p>
<p>challenges remain in adapting these models for specialized tasks, such as industrial anomaly detection, where domain-specific knowledge and precise localization are crucial.</p>

<h2 class="relative group">2.2 Detecting Industrial Anomalies
    <div id="22-detecting-industrial-anomalies" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#22-detecting-industrial-anomalies" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The detection of industrial anomalies has garnered increasing attention due to the potential for improving operational efficiency, preventing breakdowns, and minimizing production losses. Recent works have explored various methodologies, including machine learning, deep learning, and computer vision-based techniques, to address the challenges associated with anomaly detection in industrial settings.</p>
<p>One of the most commonly used approaches is unsupervised anomaly detection. Unsupervised methods do not rely on labeled data, making them particularly suitable for real-world industrial environments where obtaining labeled data is often costly and time-consuming. A prominent example of this approach is the use of Autoencoders for anomaly detection in industrial systems. Autoencoders, such as convolutional autoencoders [22], learn to reconstruct the input data, and anomalies are detected when reconstruction errors exceed a threshold. These methods are particularly effective in detecting anomalies in images and sensor data, where the system learns a compact representation of normal operations and identifies deviations.</p>
<p>In addition to autoencoders, Generative Adversarial Networks (GANs) have been applied for anomaly detection in industrial settings [23]. GAN-based approaches learn the distribution of normal data and use the discriminator network to detect anomalies by identifying samples that do not conform to the learned distribution. GANs are particularly effective when there is limited labeled data available for training, as they can generate realistic samples of normal behavior.</p>
<p>Deep learning models have also been explored in the context of industrial image anomaly detection. In the domain of manufacturing, defect detection in product images is a key application area. Convolutional neural networks (CNNs) have been used for automated defect detection [24], where models are trained to classify regions of images as normal or defective. Recently, methods like Vision Transformers (ViTs) have been investigated for their ability to capture global contextual information in industrial images [25], offering improvements in accuracy over traditional CNN-based models.</p>
<p>Another approach involves time-series anomaly detection, which is important in industrial control systems where sensor data is continuously collected [26]. Recurrent neural networks (RNNs), and specifically Long Short-Term Memory (LSTM) networks, have been widely applied for anomaly detection in time-series data [27]. These models are designed to capture temporal dependencies and detect deviations from the normal operational patterns of industrial equipment.</p>
<p>The MVTec AD dataset [3], a comprehensive benchmark for industrial anomaly detection, has been extensively used to evaluate the performance of anomaly detection models in industrial environments. The dataset contains high-resolution images of industrial products and associated anomalies, including class-specific</p>
<p>defects such as scratches, dents, and missing parts. Many recent anomaly detection methods have been benchmarked using this dataset, demonstrating the effectiveness of modern deep learning techniques for detecting fine-grained anomalies in industrial settings.</p>
<p>While significant progress has been made in industrial anomaly detection, challenges remain, particularly in real-time detection, anomaly localization, and adaptation to diverse industrial domains. Many models require substantial computational resources or rely on large labeled datasets, limiting their practicality for deployment in production environments. Furthermore, adapting existing anomaly detection techniques to specialized industrial tasks, such as detecting rare or subtle defects in highly variable manufacturing processes, remains a challenging research direction.</p>

<h2 class="relative group">3 Method
    <div id="3-method" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-method" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this section, we present the methodology for Vision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD). Our approach combines the strengths of both generative and discriminative models, leveraging the power of large vision-language models (LVLMs) to jointly process visual and textual data. Specifically, we propose a discriminative approach that focuses on distinguishing normal and anomalous instances based on their visual and textual representations. The model is trained to map both visual features and textual descriptions into a shared embedding space, where normal instances are grouped together while anomalies are separated, allowing for both detection and localization of anomalies.</p>

<h2 class="relative group">3.1 Model Overview
    <div id="31-model-overview" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#31-model-overview" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Our proposed model consists of three key components:</p>
<ol>
<li>Visual Encoder: A pretrained convolutional neural network (CNN) or vision transformer (ViT) is used to extract visual features from the input image. Let I represent an input image, and fv fv (I) denote the feature vector extracted from the visual encoder. This feature vector captures the high-level spatial and semantic information of the industrial object in the image.</li>
<li>Textual Encoder: A pretrained transformer-based language model (such as GPT or BERT) is used to process textual descriptions. Let T represent the textual input (such as defect descriptions or product manuals), and ft(T ) represent the textual feature vector. The textual encoder captures the semantic information related to the object and its potential anomalies.</li>
<li>Contrastive Learning Module: This component aligns the visual and textual embeddings into a shared space using a contrastive loss function, which is central to the anomaly detection process.</li>
</ol>
<p>The overall architecture can be described as:</p>
<!-- formula-not-decoded -->
<!-- formula-not-decoded -->
<p>where z v and z t are the visual and textual feature embeddings, respectively.</p>

<h2 class="relative group">3.2 Contrastive Loss for Cross-Modal Alignment
    <div id="32-contrastive-loss-for-cross-modal-alignment" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#32-contrastive-loss-for-cross-modal-alignment" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The core of our model&rsquo;s training lies in a contrastive loss that ensures visual and textual representations of normal instances are closer in the shared embedding space, while those of anomalous instances are pushed apart. To achieve this, we define the contrastive loss as:</p>
<!-- formula-not-decoded -->
<p>where: - N is the batch size, - kz i v − z i t k 2 2 is the squared Euclidean distance between the visual and textual embeddings for the same instance i , -α is a margin that encourages the embeddings of the same instance to be close in the feature space, - kz i v − z j t k 2 2 is the distance between the embeddings of different instances i and j , -β is a margin that encourages the embeddings of different instances to be far apart in the embedding space, - [·]+ is the positive part, meaning the loss is zero if the distance between positive pairs is smaller than α .</p>
<p>This contrastive loss function pushes the positive (normal) pairs closer while pushing the negative (anomalous) pairs farther apart in the shared space.</p>

<h2 class="relative group">3.3 Anomaly Detection and Localization
    <div id="33-anomaly-detection-and-localization" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#33-anomaly-detection-and-localization" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Once the visual and textual features have been aligned using the contrastive loss, the next task is anomaly detection and localization. To detect anomalies, we compute the similarity score between the visual feature z v of an unseen image and the corresponding textual feature zt of the object description. For a new test sample, we use the following anomaly score function S(I, T ):</p>
<!-- formula-not-decoded -->
<p>where: - z v = fv fv (I) is the visual feature of the test image, - zt = ft(T ) is the textual feature of the associated description, - σ is a scaling factor that controls the sensitivity of the similarity measure.</p>
<p>A lower value of S(I, T ) indicates a higher degree of anomaly, and we classify the sample as anomalous if S(I, T ) falls below a threshold.</p>
<p>For anomaly localization, we utilize a segmentation technique that identifies the specific pixels within the image that contribute most to the anomaly. This can be achieved using a simple gradient-based method, such as Grad-CAM, to highlight the regions of the image most responsible for the mismatch between the visual and textual embeddings:</p>
<!-- formula-not-decoded -->
<p>where: - α k are the weights of the final convolutional layer, - Ak is the activation map at location k in the last convolutional layer, - The ReLU function ensures only positive contributions are considered.</p>
<p>This localization method provides a visual heatmap that highlights the anomalous regions in the input image, making the anomaly detection process more interpretable.</p>

<h2 class="relative group">3.4 Learning Strategy: Task-Driven Fine-Tuning
    <div id="34-learning-strategy-task-driven-fine-tuning" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#34-learning-strategy-task-driven-fine-tuning" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The learning strategy is designed to optimize the model for industrial anomaly detection. We use a task-driven fine-tuning approach, where the model is initially pre-trained on a large dataset of general vision-language pairs (e.g., images and captions from a large corpus) and then fine-tuned on the specific industrial dataset. During fine-tuning, we update both the visual and textual encoders by minimizing the contrastive loss in the context of the specific anomaly detection task.</p>
<p>The overall loss function for training consists of two parts:</p>
<!-- formula-not-decoded -->
<p>where L reconstruction is a reconstruction loss that helps preserve the visual and textual details, particularly for the normal instances. The reconstruction loss ensures that the model does not overly generalize and that important visual and textual features are retained during the training process. The hyperparameter λ controls the balance between the contrastive loss and the reconstruction loss.</p>
<p>The reconstruction loss is defined as:</p>
<!-- formula-not-decoded -->
<p>where f
v − 1 f
v and f
t − 1 f
t represent the inverse functions of the visual and textual encoders, used to reconstruct the original inputs from the embeddings.</p>

<h2 class="relative group">3.5 Model Inference
    <div id="35-model-inference" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#35-model-inference" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>During inference, given a test image I and its associated textual description T , we compute the anomaly score S(I, T ) and classify the image as normal or anomalous. If S(I, T ) is below a predefined threshold, the sample is classified as anomalous. The localization technique is then applied to highlight the anomalous regions in the image.</p>

<h2 class="relative group">4 Experiments
    <div id="4-experiments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#4-experiments" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this section, we present the experimental setup and results for evaluating the performance of our proposed method, Vision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD). We compare our approach with several state-of-the-art anomaly detection methods on two widelyused industrial anomaly detection datasets: MVTec-AD and VisA. Our goal is to demonstrate that CLAD outperforms existing techniques in both anomaly detection and localization tasks. Additionally, we provide a human evaluation to assess the interpretability and usefulness of our method in real-world applications.</p>

<h2 class="relative group">4.1 Experimental Setup
    <div id="41-experimental-setup" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#41-experimental-setup" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We evaluate CLAD on two benchmark datasets: MVTec-AD [3] and VisA [4]. The MVTec-AD dataset contains 15 categories, with 3,629 training images and 1,725 test images, including both normal and anomalous samples. The VisA dataset includes 12 categories, with 9,621 normal images and 1,200 anomalous images. For comparison, we select several state-of-the-art anomaly detection methods, including:</p>
<ul>
<li>– SPADE [28]</li>
<li>– PaDiM [29]</li>
<li>– PatchCore [7]</li>
<li>– WinCLIP [30]</li>
</ul>
<p>We evaluate the models on two main tasks: anomaly detection (i.e., classification of normal vs. anomalous) and anomaly localization (i.e., pixel-level identification of anomalies). For anomaly detection, we report Image-AUC, and for anomaly localization, we report Pixel-AUC.</p>

<h2 class="relative group">4.2 Quantitative Results
    <div id="42-quantitative-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#42-quantitative-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Table 1 shows the comparison results of CLAD with other methods on the MVTec-AD and VisA datasets. We report the performance in terms of both Image-AUC and Pixel-AUC, with results averaged over five runs. As seen in the table, CLAD consistently outperforms all other methods on both datasets, achieving the highest scores in both anomaly detection and localization tasks.</p>
<p>Table 1. Comparison of CLAD with other anomaly detection methods on the MVTecAD and VisA datasets.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>MVTec-AD</th>
          <th>MVTec-AD</th>
          <th>VisA</th>
          <th>VisA</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td>Image-AUC Pixel-A</td>
          <td>AUC Pixel-AUC Image-AUC Pix</td>
          <td>AUC Image-AUC Pixel</td>
          <td>Pixel-AUC</td>
      </tr>
      <tr>
          <td>SPADE</td>
          <td>81.0±2.0</td>
          <td>91.2±0.4</td>
          <td>79.5±4.0</td>
          <td>95.6±0.4</td>
      </tr>
      <tr>
          <td>PaDiM</td>
          <td>76.6±3.1</td>
          <td>89.3±0.9</td>
          <td>62.8±5.4</td>
          <td>89.9±0.8</td>
      </tr>
      <tr>
          <td>PatchCore</td>
          <td>83.4±3.0</td>
          <td>92.0±1.0</td>
          <td>79.9±2.9</td>
          <td>95.4±0.6</td>
      </tr>
      <tr>
          <td>WinCLIP</td>
          <td>93.1±2.0</td>
          <td>95.2±0.5</td>
          <td>83.8±4.0</td>
          <td>96.4±0.4</td>
      </tr>
      <tr>
          <td>CLAD</td>
          <td>94.1±1.1</td>
          <td>95.3±0.1</td>
          <td>86.1±1.1</td>
          <td>96.2±0.1</td>
      </tr>
  </tbody>
</table>
<p>As shown in Table 1, our method, CLAD, achieves superior performance across both datasets. Notably, CLAD improves upon the next best performing method (WinCLIP) by a substantial margin in terms of Image-AUC and Pixel-AUC. For example, on the MVTec-AD dataset, CLAD achieves an ImageAUC of 94.1, outperforming WinCLIP by 1.0 points. Additionally, our model significantly improves the Pixel-AUC scores, demonstrating better localization capabilities.</p>

<h2 class="relative group">4.3 Ablation Study
    <div id="43-ablation-study" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#43-ablation-study" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To further validate the contributions of different components in our method, we conduct an ablation study to assess the impact of each key element. We perform experiments by progressively removing or modifying parts of our model, including: - Removing the contrastive loss and using only standard supervised training, - Removing the task-specific fine-tuning step, - Using a simple vision model (CNN) instead of the ViT-based encoder.</p>
<p>The results of the ablation study are presented in Table 2. The ablation study clearly shows that the contrastive loss and fine-tuning are critical components that contribute to the superior performance of CLAD.</p>
<p>Table 2. Ablation study results, demonstrating the contribution of key components to the performance of CLAD.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Image-AUC Pixel-AUC</th>
          <th>Image-AUC Pixel-AUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>CLAD (full)</td>
          <td>94.1±1.1</td>
          <td>95.3±0.1</td>
      </tr>
      <tr>
          <td>Without contrastive loss</td>
          <td>88.5±2.3</td>
          <td>91.2±1.0</td>
      </tr>
      <tr>
          <td>Without fine-tuning</td>
          <td>91.2±2.0</td>
          <td>92.5±0.8</td>
      </tr>
      <tr>
          <td>Simple CNN (no ViT)</td>
          <td>85.6±3.1</td>
          <td>89.4±1.3</td>
      </tr>
  </tbody>
</table>
<p>The results confirm that both the contrastive loss and the fine-tuning step are crucial for achieving high performance. Removing the contrastive loss results</p>
<p>in a significant drop in both Image-AUC and Pixel-AUC. Likewise, replacing the ViT with a simpler CNN leads to a noticeable degradation in performance, highlighting the importance of using powerful visual encoders.</p>

<h2 class="relative group">4.4 Human Evaluation
    <div id="44-human-evaluation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#44-human-evaluation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To assess the practical utility and interpretability of our method, we conduct a human evaluation. We invite experts in industrial defect detection to evaluate the anomaly localization results produced by our method and compare them with the ground truth annotations. The experts are asked to rate the quality of the anomaly localization on a scale of 1 to 5, where 1 indicates poor localization and 5 indicates highly accurate localization.</p>
<p>The results of the human evaluation are shown in Table 3. CLAD significantly outperforms other methods in terms of localization accuracy, with an average rating of 4.6, indicating that the anomaly localization produced by CLAD is both accurate and highly interpretable.</p>
<p>Table 3. Human evaluation of anomaly localization. CLAD significantly outperforms other methods in terms of localization accuracy.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Human Evaluation Score</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>SPADE</td>
          <td>3.4</td>
      </tr>
      <tr>
          <td>PaDiM</td>
          <td>3.7</td>
      </tr>
      <tr>
          <td>PatchCore</td>
          <td>4.1</td>
      </tr>
      <tr>
          <td>WinCLIP</td>
          <td>4.4</td>
      </tr>
      <tr>
          <td>CLAD</td>
          <td>4.6</td>
      </tr>
  </tbody>
</table>
<p>The human evaluation results indicate that our method not only performs well in quantitative evaluations but also provides practical benefits in real-world anomaly detection tasks. The high localization accuracy allows for more effective and interpretable detection, which is crucial for industrial applications.</p>

<h2 class="relative group">4.5 Analysis of Anomaly Localization Performance
    <div id="45-analysis-of-anomaly-localization-performance" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#45-analysis-of-anomaly-localization-performance" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this subsection, we analyze the results of anomaly localization produced by CLAD. We focus on both the precision and recall of the localized anomaly regions. To evaluate these metrics, we compare the predicted anomaly regions against ground truth annotations using Intersection over Union (IoU). Table 4 presents the IoU scores for each method. CLAD consistently achieves the highest IoU, indicating superior performance in correctly identifying the boundaries of anomalies.</p>
<p>The high IoU score of CLAD further demonstrates its ability to not only detect anomalies effectively but also localize them with high precision, making it a reliable solution for industrial anomaly detection tasks.</p>
<p>Table 4. Intersection over Union (IoU) scores for anomaly localization. CLAD achieves the highest IoU, indicating superior localization performance.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>oU Score</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>SPADE</td>
          <td>0.63</td>
      </tr>
      <tr>
          <td>PaDiM</td>
          <td>0.66</td>
      </tr>
      <tr>
          <td>PatchCore</td>
          <td>0.72</td>
      </tr>
      <tr>
          <td>WinCLIP</td>
          <td>0.75</td>
      </tr>
      <tr>
          <td>CLAD</td>
          <td>0.8</td>
      </tr>
  </tbody>
</table>

<h2 class="relative group">5 Conclusion
    <div id="5-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#5-conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this paper, we proposed a novel method, Vision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD), that utilizes large vision-language models to enhance both anomaly detection and localization in industrial environments. By aligning visual and textual features in a shared embedding space through contrastive learning, CLAD improves the discrimination between normal and anomalous samples, leading to more accurate anomaly detection. Our extensive experiments on the MVTec-AD and VisA datasets demonstrate that CLAD outperforms existing state-of-the-art methods in both image-level anomaly detection and pixel-level anomaly localization. Furthermore, the ablation study and human evaluation reinforce the effectiveness of key components, such as the contrastive loss and fine-tuning, and highlight the superior localization capabilities of CLAD. In conclusion, our method offers a promising solution for industrial anomaly detection tasks, combining high performance with interpretability, making it a valuable tool for industrial quality control and maintenance.</p>

<h2 class="relative group">References
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ol>
<li>Zhou, Y., Li, X., Wang, Q., Shen, J.: Visual in-context learning for large visionlanguage models. In: Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024. pp. 15890– 15902. Association for Computational Linguistics (2024)</li>
<li>Zhou, Y., Rao, Z., Wan, J., Shen, J.: Rethinking visual dependency in long-context reasoning for large vision-language models. arXiv preprint arXiv:2410.19732 (2024)</li>
<li>Bergmann, P., Fauser, M., Sattlegger, D., Steger, C.: Mvtec ad–a comprehensive real-world dataset for unsupervised anomaly detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9592–9600 (2019)</li>
<li>Zou, Y., Jeong, J., Pemula, L., Zhang, D., Dabeer, O.: Spot-the-difference selfsupervised pre-training for anomaly detection and segmentation. In: European Conference on Computer Vision. pp. 392–408. Springer (2022)</li>
<li>Gu, Z., Zhu, B., Zhu, G., Chen, Y., Tang, M., Wang, J.: Anomalygpt: Detecting industrial anomalies using large vision-language models. In: Wooldridge, M.J.,</li>
</ol>
<ul>
<li>Dy, J.G., Natarajan, S. (eds.) Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada. pp. 1932–1940. AAAI Press (2024). <a
  href="https://doi.org/10.1609/AAAI.V38I3.27963"
    target="_blank"
  >https://doi.org/10.1609/AAAI.V38I3.27963</a> , <a
  href="https://doi.org/10.1609/aaai.v38i3.27963"
    target="_blank"
  >https://doi.org/10.1609/aaai.v38i3.27963</a></li>
</ul>
<ol start="6">
<li>Defard, T., Setkov, A., Loesch, A., Audigier, R.: Padim: A patch distribution modeling framework for anomaly detection and localization. In: Bimbo, A.D., Cucchiara, R., Sclaroff, S., Farinella, G.M., Mei, T., Bertini, M., Escalante, H.J., Vezzani, R. (eds.) Pattern Recognition. ICPR International Workshops and Challenges - Virtual Event, January 10-15, 2021, Proceedings, Part IV. Lecture Notes in Computer Science, vol. 12664, pp. 475–489. Springer (2020). <a
  href="https://doi.org/10.1007/978-3-030-68799-1"
    target="_blank"
  >https://doi.org/10.1007/978-3-030-68799-1</a>_35 , <a
  href="https://doi.org/10.1007/978-3-030-68799-1"
    target="_blank"
  >https://doi.org/10.1007/978-3-030-68799-1</a>_35</li>
<li>Roth, K., Pemula, L., Zepeda, J., Schölkopf, B., Brox, T., Gehler, P.: Towards total recall in industrial anomaly detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 14318–14328 (2022)</li>
<li>Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: Meila, M., Zhang, T. (eds.) Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event. Proceedings of Machine Learning Research, vol. 139, pp. 8748–8763. PMLR (2021), <a
  href="http://proceedings.mlr.press/v139/radford21a.html"
    target="_blank"
  >http://proceedings.mlr.press/v139/radford21a.html</a></li>
<li>Zhou, Y., Long, G.: Improving cross-modal alignment for text-guided image inpainting. In: Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. pp. 3445–3456 (2023)</li>
<li>Zhou, Y., Long, G.: Multimodal event transformer for image-guided story ending generation. In: Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. pp. 3434–3444 (2023)</li>
<li>Zhou, Y., Long, G.: Style-aware contrastive learning for multi-style image captioning. In: Findings of the Association for Computational Linguistics: EACL 2023. pp. 2257–2267 (2023)</li>
<li>Reddy, M.D.M., Basha, M.S.M., Hari, M.M.C., Penchalaiah, M.N.: Dall-e: Creating images from text. UGC Care Group I Journal 8(14), 71–75 (2021)</li>
<li>Li, L.H., Yatskar, M., Yin, D., Hsieh, C., Chang, K.: Visualbert: A simple and performant baseline for vision and language. CoRR abs/1908.03557 (2019), <a
  href="http://arxiv.org/abs/1908.03557"
    target="_blank"
  >http://arxiv.org/abs/1908.03557</a></li>
<li>Zhou, Y., Shen, T., Geng, X., Long, G., Jiang, D.: Claret: Pre-training a correlation-aware context-to-event transformer for event-centric generation and classification. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 2559–2575 (2022)</li>
<li>Zhou, Y., Geng, X., Shen, T., Long, G., Jiang, D.: Eventbert: A pre-trained model for event correlation reasoning. In: Proceedings of the ACM Web Conference 2022. pp. 850–859 (2022)</li>
<li>Chen, Y., Li, L., Yu, L., Kholy, A.E., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.: UNITER: learning universal image-text representations. CoRR abs/1909.11740 (2019), <a
  href="http://arxiv.org/abs/1909.11740"
    target="_blank"
  >http://arxiv.org/abs/1909.11740</a></li>
<li>Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: VL-BERT: pre-training of generic visual-linguistic representations. In: 8th International Conference on</li>
</ol>
<ul>
<li>Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net (2020), <a
  href="https://openreview.net/forum?id=SygXPaEYvH"
    target="_blank"
  >https://openreview.net/forum?id=SygXPaEYvH</a></li>
</ul>
<ol start="18">
<li>Zhou, Y., Tao, W., Zhang, W.: Triple sequence generative adversarial nets for unsupervised image captioning. In: ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 7598–7602. IEEE (2021)</li>
<li>Zhou, Y.: Sketch storytelling. In: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 4748–4752. IEEE (2022)</li>
<li>Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems 34, 9694–9705 (2021)</li>
<li>Yuan, L., Chen, D., Chen, Y., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi, Y., Wang, L., Wang, J., Xiao, B., Xiao, Z., Yang, J., Zeng, M., Zhou, L., Zhang, P.: Florence: A new foundation model for computer vision. CoRR abs/2111.11432 (2021), <a
  href="https://arxiv.org/abs/2111.11432"
    target="_blank"
  >https://arxiv.org/abs/2111.11432</a></li>
<li>Heger, J., Desai, G., El Abdine, M.Z.: Anomaly detection in formed sheet metals using convolutional autoencoders. Procedia CIRP 93, 1281–1285 (2020)</li>
<li>Li, D., Chen, D., Jin, B., Shi, L., Goh, J., Ng, S.: MAD-GAN: multivariate anomaly detection for time series data with generative adversarial networks. In: Tetko, I.V., Kurková, V., Karpov, P., Theis, F.J. (eds.) Artificial Neural Networks and Machine Learning - ICANN 2019: Text and Time Series - 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17-19, 2019, Proceedings, Part IV. Lecture Notes in Computer Science, vol. 11730, pp. 703–716. Springer (2019). <a
  href="https://doi.org/10.1007/978-3-030-30490-4"
    target="_blank"
  >https://doi.org/10.1007/978-3-030-30490-4</a>_56 , <a
  href="https://doi.org/10.1007/978-3-030-30490-4"
    target="_blank"
  >https://doi.org/10.1007/978-3-030-30490-4</a>_56</li>
<li>Palakurti, N.R.: Challenges and future directions in anomaly detection. In: Practical Applications of Data Processing, Algorithms, and Modeling, pp. 269–284. IGI Global (2024)</li>
<li>Dosovitskiy, A., Fischer, P., Springenberg, J.T., Riedmiller, M.A., Brox, T.: Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE Trans. Pattern Anal. Mach. Intell. 38(9), 1734–1747 (2016). <a
  href="https://doi.org/10.1109/TPAMI.2015.2496141"
    target="_blank"
  >https://doi.org/10.1109/TPAMI.2015.2496141</a> , <a
  href="https://doi.org/10.1109/TPAMI.2015.2496141"
    target="_blank"
  >https://doi.org/10.1109/TPAMI.2015.2496141</a></li>
<li>Wang, Q., Hu, H., Zhou, Y.: Memorymamba: Memory-augmented state space model for defect recognition. arXiv preprint arXiv:2405.03673 (2024)</li>
<li>Parsai, S., Mahajan, S.: Anomaly detection using long short-term memory. In: 2020 International Conference on Electronics and Sustainable Communication Systems (ICESC). pp. 333–337. IEEE (2020)</li>
<li>Zou, H., Cao, K., Jiang, C.: Spatio-temporal visual analysis for urban traffic characters based on video surveillance camera data. ISPRS International Journal of Geo-Information 10(3), 177 (2021)</li>
<li>Defard, T., Setkov, A., Loesch, A., Audigier, R.: Padim: A patch distribution modeling framework for anomaly detection and localization. In: Bimbo, A.D., Cucchiara, R., Sclaroff, S., Farinella, G.M., Mei, T., Bertini, M., Escalante, H.J., Vezzani, R. (eds.) Pattern Recognition. ICPR International Workshops and Challenges - Virtual Event, January 10-15, 2021, Proceedings, Part IV. Lecture Notes in Computer Science, vol. 12664, pp. 475–489. Springer (2020). <a
  href="https://doi.org/10.1007/978-3-030-68799-1"
    target="_blank"
  >https://doi.org/10.1007/978-3-030-68799-1</a>_35 , <a
  href="https://doi.org/10.1007/978-3-030-68799-1"
    target="_blank"
  >https://doi.org/10.1007/978-3-030-68799-1</a>_35</li>
</ol>
<ul>
<li>14 K. Qian et al.</li>
</ul>
<ol start="30">
<li>Jeong, J., Zou, Y., Kim, T., Zhang, D., Ravichandran, A., Dabeer, O.: Winclip: Zero-/few-shot anomaly classification and segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19606– 19616 (2023)</li>
</ol>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/Exploring Large Vision-Language Models for Robust and Efficient Industrial Anomaly Detection.md"
          data-oid-likes="likes_papers/Exploring Large Vision-Language Models for Robust and Efficient Industrial Anomaly Detection.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/follow-the-rules-reasonin-for-vad-with-llm/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/delving-into-clip-latent-space-for-video-anomaly-recognition/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
