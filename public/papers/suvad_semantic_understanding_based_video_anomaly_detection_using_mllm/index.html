<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/suvad_semantic_understanding_based_video_anomaly_detection_using_mllm/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/suvad_semantic_understanding_based_video_anomaly_detection_using_mllm/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/suvad_semantic_understanding_based_video_anomaly_detection_using_mllm\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "4299"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>4299 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">21 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">SUVAD: Semantic Understanding Based Video Anomaly Detection Using MLLM
    <div id="suvad-semantic-understanding-based-video-anomaly-detection-using-mllm" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#suvad-semantic-understanding-based-video-anomaly-detection-using-mllm" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Shibo Gao∗†, Peipei Yang†‡(B), Linlin Huang ∗</p>
<p>∗ Beijing Jiaotong University</p>
<p>† State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences ‡ School of Artificial Intelligence, University of Chinese Academy of Sciences</p>
<p>Abstract—Video anomaly detection (VAD) aims at detecting anomalous events in videos. Most existing VAD methods distinguish anomalies by learning visual features of the video, which usually face several challenges in real-world applications. First, these methods are mostly scene-dependent, whose performances degrade obviously once the scene changes. Second, these methods are incapable of giving explanations to the detected anomalies. Third, these methods cannot adjust definitions of normal or abnormal events during test time without retraining the model. One important reason for the drawbacks is that these visualbased methods mainly detect anomalies by fitting visual patterns rather than semantically understanding the events in videos. In this paper, we propose a training-free method named Semantic Understanding based Video Anomaly Detection (SUVAD) using multi-modal large language model (MLLM). By exploiting MLLMs to generate detailed texture descriptions for videos, SUVAD achieves semantic video understanding, and then detects anomalies directly by large language models. We also designed several techniques to mitigate the hallucination problem of MLLMs. Compared to the methods based on visual features, SUVAD obtains obviously better scene generalization, anomaly interpretability, and the ability of flexible adjustment of anomaly definitions. We evaluate our method on five mainstream datasets. The results show that SUVAD achieves the best performance among all the training-free methods.</p>
<p>Index Terms—Video Anomaly Detection, Multi-modal Largelanguage Model, Training-free</p>

<h2 class="relative group">I. INTRODUCTION
    <div id="i-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#i-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video anomaly detection (VAD) aims to detect events in the videos that significantly deviate from normal patterns. Due to its widespread applications in areas such as intelligent surveillance systems and video censorship, VAD has garnered increasing attention in both academia and industry [1]–[11].</p>
<p>Most existing VAD methods identify anomalous events by fitting the normal or anomalous visual patterns learned from the training videos [12]–[22]. Although these methods demonstrate good performances in their experiments, they encounter multiple challenges when applied to the real world.</p>
<p>Firstly, the methods based on visual features are prone to be scene-specific. Models over-fitted to the specific scenes will experience significant performance degradation when switched to another scene. Secondly, these methods focus on detecting anomalies while neglecting to provide detailed explanations to the anomalous events. Thirdly, the definition of anomalous events can be learnt only from the training videos and thus are incapable of being adjusted without re-training the model using new data.</p>
<p>The aforementioned problems stem largely from the fact that most existing methods focus exclusively on visual features when detecting anomalies, rather than comprehending the video content. In reality, anomalies within videos are dictated by the events. Clearly, the recently successful multi-modal large language models [23]–[26] (MLLMs) and large language models [27], [28] (LLMs) are well-suited to address this task. Zanella et al. [29] were the first to attempt using MLLMs for VAD tasks. Unfortunately, they failed to fully exploit the potential of MLLMs. Firstly, their definition of anomalies relies entirely on LLMs, making it completely uncontrollable. Secondly, they are severely affected by hallucination problems.</p>
<p>Fig. 1. Compared to the methods based on visual features, our method can support flexible anomaly definition, adapt to different scenes, and provide explanations for anomalies under training-free.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_25e0ba91b361c6a6c3912b186f8899ee58980f52043dc241bf5938b4e0dc701f.png"
    ></figure>
<p>In this paper, we propose a training-free VAD method named Semantic Understanding Based VAD (SUVAD), to address the aforementioned problems. It first exploits MLLMs to achieve semantic understanding of the training videos and generate textual descriptions for normal or abnormal events according to the labels. Subsequently, SUVAD generates textual description for each frame of the test video and calculates its anomaly score by comparing it with the descriptions of normal and abnormal events using an LLM. The anomalous frames can be detected according to the scores with interpretative descriptions about the anomalies. Furthermore, we designed several techniques such as score smoothing and caption correction to mitigate hallucination problems. Fig. 1 illustrates the differences between SUVAD and other visual-based methods.</p>
<p>Using semantic information instead of visual features for</p>
<p>anomaly detection, SUVAD can well adapt to different scenes without obvious performance degradation. Benefiting from the multi-modal model, SUVAD can conveniently give explanation to the detected anomalies and adjust the definition of normal or abnormal events according to textual inputs without retraining the model. Finally, the strategy of course-to-fine VAD effectively suppresses hallucination problems, further enhancing model performance.</p>
<p>We evaluate our method on both semi-supervised and weakly-supervised tasks using five mainstream datasets. The results show that SUVAD achieves the best performance among all the training-free methods and achieves comparable performance to other supervised methods. We summarize our contributions as follows:</p>
<ul>
<li>We propose a training-free VAD method named SUVAD that detects anomalies based on semantic understanding of the video contents. Compared with the methods based on visual features, it benefits from better generalization to scenes, ability of anomaly explanation and the flexibility to adjust anomaly definitions.</li>
<li>We propose a series of techniques to mitigate the hallucination problems of MLLMs for VAD task.</li>
<li>Experimental results on five mainstream datasets demonstrate that our method achieves excellent versatility and competitive performance.</li>
</ul>

<h2 class="relative group">II. METHOD
    <div id="ii-method" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#ii-method" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">A. Overview
    <div id="a-overview" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-overview" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Fig. 2. Illustration of the SUVAD.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_a5699b4c3444270d3e614c66ac874a00c716d17b8f34356969de7d53d0b46b12.png"
    ></figure>
<p>Fig. 2 illustrates the overall architecture of our proposed method, SUVAD, designed for detecting anomalies in videos through three phases. Specifically, SUVAD accepts arbitrary labeled text, image, or video inputs and generates the lists of normal/anomalous events. Subsequently, SUVAD generates textual descriptions for video clips. By comparing the video descriptions with the event lists, SUVAD assigns the videolevel anomaly scores and locates high-probability anomalous clips. Lastly, SUVAD analyzes the descriptions of each frame within the high-probability anomalous clip to provide a framelevel anomaly score. The final anomaly score is derived from a weighted combination of the video-level and framelevel anomaly scores. To mitigate the hallucination problems inherent in MLLMs, we employ a Caption Correction module to reassign image captions across consecutive frames and apply score smoothing to ensure event continuity.</p>

<h2 class="relative group">B. Normal/Anomalous Patterns Learning in the Training-Free Manner
    <div id="b-normalanomalous-patterns-learning-in-the-training-free-manner" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-normalanomalous-patterns-learning-in-the-training-free-manner" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Most existing VAD methods [12]–[18], [30], [31] typically learn visual features from the videos in the training set to learn the definition of normal or anomalies. Once the model is trained, the boundaries between abnormal and normal events are also fixed. These visual-based methods are not only susceptible to data imbalance, which can lead to misjudgments, but also lack the flexibility to adjust the definition of anomalous events without re-annotating the datasets and re-training the model.</p>
<p>Unlike the visual-based methods, SUVAD can determine the anomalous or normal events in a simple and flexible way under training-free. Specifically, SUVAD can accept any input in the form of text Ti Tinput , images Iinput, or videos Vinput, and summarize it into the lists of abnormal/normal events L a, L n with brief descriptions. For textual input, SUVAD utilizes LLMs to summarize it. For image or video input, SUVAD first generates corresponding descriptions using MLLMs, and then summarizes these descriptions using LLMs. Through this method, SUVAD retains the ability to learn the definitions of normal or anomalous events from data in the training-free manner, thereby addressing the aforementioned issues.</p>

<h2 class="relative group">C. Coarse-grained Anomaly Detection
    <div id="c-coarse-grained-anomaly-detection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-coarse-grained-anomaly-detection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In the field of VAD, anomalous events often exhibit a form of continuity, necessitating an analysis that not only focuses on the intricate visual information within individual video frames but also profoundly comprehends the associative information between frames [17], [18], [29]. Furthermore, while MLLMs demonstrate substantial advantages in modeling capabilities compared to CNNs, they also entail the significant increase in computational costs. Based on these two points, the SUVAD initially performs a coarse-grained anomaly detection on the test video and identifies high-probability clips that are most likely to contain anomalies. This strategy, while fully considering the associative information between video frames, also reduces the subsequent computational overhead and alleviates the hallucination problems from MLLMs.</p>
<p>Specifically, for a test video contains a series of frames V test = {F1, F2, . . . , Fj}, the SUVAD first divides it into video clips C1 , C 2, . . . , C n according to a fixed interval d , where C i = {Fi ×d , Fi ×d+1 , . . . , F(i+1)×d − 1 }. Then SUVAD uses MLLMs to generate video captions Cap(Ci) for them.</p>
<p>Subsequently, SUVAD employs LLMs to compare and analyze the video captions with the event lists produced in the previous stage. Based on the degree of match between the captions and the lists, SUVAD assigns an anomaly score s(Ci) to each video clip, indicating potential anomalous events and summarizing these events into a list L ′ a :</p>
<!-- formula-not-decoded -->
<p>TABLE I COMPARISON WITH OTHER STATE -OF -THE -ART METHODS ON THE SINGLE DATASET .</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Supervised  Mode</th>
          <th>Explanation</th>
          <th>Semi-Supervised Datasets</th>
          <th>Semi-Supervised Datasets</th>
          <th>Semi-Supervised Datasets</th>
          <th>Weakly Supervised Datasets Vil(AP) UCFCi(AUC)</th>
          <th>Weakly Supervised Datasets Vil(AP) UCFCi(AUC)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Method</td>
          <td>Mode</td>
          <td>Explanation</td>
          <td>Ped2(AUC)</td>
          <td>Avenue(AUC)</td>
          <td>SH Tech(AUC)</td>
          <td>XD-Violence(AP)</td>
          <td>UCF-Crime(AUC)</td>
      </tr>
      <tr>
          <td>BA Framework [21]</td>
          <td>Semi</td>
          <td>No</td>
          <td>98.7%</td>
          <td>92.3%</td>
          <td>82.7%</td>
          <td>×</td>
          <td>×</td>
      </tr>
      <tr>
          <td>Ristea et al. [32]</td>
          <td>Semi</td>
          <td>No</td>
          <td>-</td>
          <td>91.6%</td>
          <td>83.8%</td>
          <td>×</td>
          <td>×</td>
      </tr>
      <tr>
          <td>Wang et al. [17</td>
          <td>Semi</td>
          <td>No</td>
          <td>99.0%</td>
          <td>92.2%</td>
          <td>84.3%</td>
          <td>×</td>
          <td>×</td>
      </tr>
      <tr>
          <td>CLIP-TSA [14]</td>
          <td>Weakly</td>
          <td>No</td>
          <td>×</td>
          <td>×</td>
          <td>×</td>
          <td>82.1%</td>
          <td>87.5%</td>
      </tr>
      <tr>
          <td>VadCLIP [15]</td>
          <td>Weakly</td>
          <td>No</td>
          <td>×</td>
          <td>×</td>
          <td>×</td>
          <td>84.5%</td>
          <td>88.0%</td>
      </tr>
      <tr>
          <td>UMIL [33]</td>
          <td>Weakly</td>
          <td>No</td>
          <td>×</td>
          <td>×</td>
          <td>×</td>
          <td>-</td>
          <td>87.5%</td>
      </tr>
      <tr>
          <td>ZS CLIP [34]</td>
          <td>Training-free</td>
          <td>No</td>
          <td>61.7%</td>
          <td>52.3%</td>
          <td>50.2%</td>
          <td>17.8%</td>
          <td>53.1%</td>
      </tr>
      <tr>
          <td>LLAVA-1.5 [25]</td>
          <td>Training-free</td>
          <td>Yes</td>
          <td>82.9%</td>
          <td>67.4%</td>
          <td>59.6%</td>
          <td>50.2%</td>
          <td>72.8%</td>
      </tr>
      <tr>
          <td>Video chatgpt [26]</td>
          <td>Training-free</td>
          <td>Yes</td>
          <td>85.1%</td>
          <td>76.9%</td>
          <td>69.1%</td>
          <td>53.8%</td>
          <td>75.3%</td>
      </tr>
      <tr>
          <td>LAVAD [29]</td>
          <td>Training-free</td>
          <td>Yes</td>
          <td>×</td>
          <td>×</td>
          <td>×</td>
          <td>62.0%</td>
          <td>80.2%</td>
      </tr>
      <tr>
          <td>SUVAD(Ours)</td>
          <td>Training-free</td>
          <td>Yes</td>
          <td>96.8%</td>
          <td>89.3%</td>
          <td>80.2%</td>
          <td>70.1%</td>
          <td>83.9%</td>
      </tr>
  </tbody>
</table>
<p>When only input data labeled as normal is provided, L a = ∅. In this scenario, SUVAD analyzes whether the captions Cap(Ci) contain any content that does not belong to the normal event list L n . Based on this, SUM assigns the anomaly score s(Ci) ranging from 0/10 to 10/10. The score of 10/10 signifies that the captions Cap(Ci) definitely include at least one event that is not part of L n , whereas a score of 0/10 indicates that the caption content entirely falls within the normal event category. Conversely, when only input data labeled as anomalous is provided, L n = ∅. SUVAD examines whether the captions Cap(Ci) contain descriptions of events listed in L a and assigns a score from 0/10 to 10/10 based on match degree, using the opposite scoring logic.</p>
<p>Lastly, SUVAD employs two methods to locate highprobability anomalous clips within the test video. On one hand, given a threshold τ , SUVAD flags clips with scores exceeding this threshold as high-probability anomalous clips:</p>
<!-- formula-not-decoded -->
<p>On the other hand, considering that dividing the video at a fixed interval may result in the disruption of continuous events, SUVAD also leverages the temporal localization capabilities of MLLMs to uncover clips that might be overlooked yet contain anomalous events using the aforementioned list H2 = MLLM(L ′ a
, V test ). Ultimately, SUVAD takes the intersection of them H = H 1 ∪ H 2 as the result and passes it on to the next processing stage.</p>

<h2 class="relative group">D. Fine-Grained Anomaly Detection
    <div id="d-fine-grained-anomaly-detection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-fine-grained-anomaly-detection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>As mentioned in II-C, intricate visual information within video frames is also crucial. After coarse-grained anomaly detection, SUVAD conducts a detailed analysis of each frame in the captured high-probability anomalous clips H and integrates video-level and frame-level anomaly scores to provide the overall anomaly scores.</p>
<p>Firstly, SUVAD generates an image caption Cap(Fj ) for each frame in the high-probability clips. To further mitigate the impact of hallucination problems, SUVAD employs the Caption Correction module to refine the generated image captions. Specifically, SUVAD utilizes an aligned vision- language model to encode several adjacent frames and their corresponding image captions, and then redistributes them:</p>
<!-- formula-not-decoded -->
<p>where &lt; · &gt; is the cosine similarity, and the ΦI and ΦT are the image encoder and the test encoder.</p>
<p>Subsequently, similar to the previous stage, SUVAD compares the image captions with the lists, assigning an abnormality score s(Fj ) to each frame. The final score is derived from the combination of two scores:</p>
<!-- formula-not-decoded -->
<p>where α and β are the constants.</p>
<p>Considering that events are continuous and to further avoid the influence of randomness in MLLMs on anomaly detection, we apply the Savitzky-Golay filter to smooth the scores:</p>
<!-- formula-not-decoded -->
<p>Where qp qp /Q is the smoothing coefficient, determined through polynomial fitting using the least squares method.</p>

<h2 class="relative group">III. EXPERIMENTS
    <div id="iii-experiments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#iii-experiments" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">A. Datasets
    <div id="a-datasets" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-datasets" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>SVAD Datasets: Commonly used SVAD datasets include UCSD Ped2 [35], CUHK dataset [36], and Shanghai Tech dataset [1]. These datasets only provide videos containing normal events during training and require the model to accurately locate anomalous segments in the test set during the testing phase. The anomalies in these datasets include running, cycling, and skateboarding, etc.</p>
<p>WVAD Datasets: Commonly used WVAD datasets include UCF-Crime [37] and XD-Violence [38]. These datasets provide videos containing both normal and anomalous events during training, along with specific anomalous categories, but the anomalous annotations are at the video level. These datasets also require the model to accurately locate anomalous segments in the test set. Compared to SVAD datasets, the anomalies included in these datasets are extreme events such as explosions, robberies, shootings, etc.</p>

<h2 class="relative group">B. Implementation Details
    <div id="b-implementation-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-implementation-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In the evaluation phase, SUVAD employs cogvlm2 [24] to generate image captions and its video version, cogvlm2 − video [24], to generate video captions. Additionally, SUVAD utilizes llama − 3 [27] for text analysis and score assignment. It is noteworthy that the selection of these models is not fixed, and currently mainstream MLLMs and LLMs with the same functionality can achieve similar performance. For the constant terms in Equ.4, we set α to 0.4 and β to 0.6. Regarding the Savitzky-Golay filter, we set the order of the polynomial to 3 and the size of the smoothing window to 53.</p>

<h2 class="relative group">C. Comparison with State-of-the-art Methods
    <div id="c-comparison-with-state-of-the-art-methods" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-comparison-with-state-of-the-art-methods" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Comparison of our method with various representative methods is shown in Table. I. The results demonstrate that SUVAD achieves the best performance among all trainingfree methods and achieves comparable performance to other supervised methods.</p>
<p>Due to variations in supervisory information and event types across several datasets, it is generally challenging for visualbased methods to transition from one type of task to another. In Table. I, &ldquo;×&rdquo; represents that the method cannot be evaluated on that particular dataset, while &ldquo;−&rdquo; indicates that the paper did not report evaluation results for that dataset.</p>
<p>Benefiting from the powerful analytical capabilities of MLLMs and LLMs, as well as the training-free framework of SUVAD, our proposed method can easily switch between different VAD tasks. Furthermore, our method can provide explanations for anomalies and flexibly adjust the definition of anomalous events.</p>

<h2 class="relative group">D. Experiment on Scene Generalization
    <div id="d-experiment-on-scene-generalization" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-experiment-on-scene-generalization" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>As mentioned previously, a significant advantage of SUVAD lies in its generalization to different scenes. Benefiting from its training-free framework and the ability to understand videos, SUVAD is not susceptible to the interference of visual features like other methods, thereby achieving strong generalization across different scenes.</p>
<p>TABLE II EXPERIMENTAL RESULTS OF SCENE GENERALIZATION , USING AUC AS THE METRIC .</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Avenue→SHT</th>
          <th>SHT→Avenue</th>
          <th>SHT→Ped2</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ZS CLIP [34]</td>
          <td>60.9%</td>
          <td>62.3%</td>
          <td>52.7%</td>
      </tr>
      <tr>
          <td>ZS CLIP IB [39]</td>
          <td>61.3%</td>
          <td>64.5%</td>
          <td>53.6%</td>
      </tr>
      <tr>
          <td>Astrid et al. [40]</td>
          <td>51.7%</td>
          <td>54.3%</td>
          <td>65.9%</td>
      </tr>
      <tr>
          <td>Wang et al. [17]</td>
          <td>59.3%</td>
          <td>62.9%</td>
          <td>75.6%</td>
      </tr>
      <tr>
          <td>SUVAD(Ours)</td>
          <td>77.3%</td>
          <td>84.9%</td>
          <td>96.1%</td>
      </tr>
  </tbody>
</table>
<p>We designed three different experimental schemes using three mainstream SVAD datasets to explore the generalization capability of SUVAD in various scenarios: Shanghai Tech(SHT)→Avenue, Avenue→SHT, and SHT→Ped2. We compared our method with four other high-performance methods under identical experimental settings, and the results are presented in Table. II. It is evident that when evaluating between different scenes, other methods failed to maintain their performance, whereas our method continued to achieve high detection accuracy.</p>

<h2 class="relative group">E. Ablation Study
    <div id="e-ablation-study" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#e-ablation-study" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To further illustrate the superiority of the SUVAD architecture, we conducted ablation experiments on various modules in SUVAD on Shanghai Tech, and the results are shown in Table.III. It can be seen that each module we designed further improves the model&rsquo;s performance.</p>
<p>TABLE III EXPERIMENTAL RESULTS OF ABLATION STUDY .</p>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Finer  Detection</th>
          <th>Coarse  Detection</th>
          <th>Caption  Correction</th>
          <th>Score  Smoothing</th>
          <th>Results (AUC)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>A</td>
          <td>✓</td>
          <td></td>
          <td></td>
          <td></td>
          <td>71.2%</td>
      </tr>
      <tr>
          <td>B</td>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td></td>
          <td>74.0%</td>
      </tr>
      <tr>
          <td>C</td>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td>74.9%</td>
      </tr>
      <tr>
          <td>D</td>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
          <td>80.2%</td>
      </tr>
  </tbody>
</table>
<p>Fig. 3. Visualization of ablation study.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_a2e62c2b436689fa45e38e1ed08a678dcc2ecf1346084bc66702c303195de5d8.png"
    ></figure>
<p>We conducted a visualization experiment on the 01 0131 video from Shanghai Tech dataset using four settings from Table. III. The results are shown in Fig. 3. Obviously, the coarse detection significantly reduces the hallucination problem of MLLMs when processing normal frames, while the Caption Correction module mitigates the hallucinations of MLLMs when dealing with anomalous ones. The Score Smoothing module takes into account the anomaly scores of the entire video, greatly improving detection accuracy while reducing the influence of hallucinations.</p>

<h2 class="relative group">IV. CONCLUTION
    <div id="iv-conclution" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#iv-conclution" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This paper introduces SUVAD, a training-free video anomaly detection method based on semantic understanding utilizing MLLMs. Most existing VAD methods detect anomalies by using the visual features of normal or abnormal patterns learnt from videos rather than understanding video content. This results in their poor performance in scene generalization, interpretative ability to anomalies, and flexible adjustment of anomaly definitions, which are required in real-world VAD. Leveraging the powerful video understanding capability MLLMs and LLMs, the SUVAD obtains significantly better performances in these aspects. The experimental results show that our method achieves the best performance among all training-free methods and is comparable to the state-of-theart methods of other supervision settings.</p>

<h2 class="relative group">ACKNOWLEDGMENT
    <div id="acknowledgment" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#acknowledgment" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This work has been supported by &ldquo;Scientific and Technological Innovation 2030&rdquo; Program of China Ministry of Science and Technology (2021ZD0113803) and the National Natural Science Foundation of China (NSFC) grant 62276258.</p>

<h2 class="relative group">REFERENCES
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>[1] W. Luo, W. Liu, and S. Gao, &ldquo;A revisit of sparse coding based anomaly detection in stacked rnn framework,&rdquo; in Proceedings of the IEEE international conference on computer vision, 2017, pp. 341–349.</li>
<li>[2] H. Park, J. Noh, and B. Ham, &ldquo;Learning memory-guided normality for anomaly detection,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 14 372–14 381.</li>
<li>[3] F. Dong, Y. Zhang, and X. Nie, &ldquo;Dual discriminator generative adversarial network for video anomaly detection,&rdquo; IEEE Access, vol. 8, pp. 88 170–88 176, 2020.</li>
<li>[4] Y. Lu, F. Yu, M. K. K. Reddy, and Y. Wang, &ldquo;Few-shot scene-adaptive anomaly detection,&rdquo; in European Conference on Computer Vision, 2020, pp. 125–141.</li>
<li>[5] C. Park, M. Cho, M. Lee, and S. Lee, &ldquo;Fastano: Fast anomaly detection via spatio-temporal patch transformation,&rdquo; in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , 2022, pp. 2249–2259.</li>
<li>[6] Z. Xu, X. Zeng, G. Ji, and B. Sheng, &ldquo;Improved anomaly detection in surveillance videos with multiple probabilistic models inference,&rdquo; Intelligent Automation &amp; Soft Computing, vol. 31, pp. 1703–1717, 2022.</li>
<li>[7] K. Cheng, X. Zeng, Y. Liu, M. Zhao, C. Pang, and X. Hu, &ldquo;Spatialtemporal graph convolutional network boosted flow-frame prediction for video anomaly detection,&rdquo; in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2023, pp. 1–5.</li>
<li>[8] S. Gao, J. Gong, P. Yang, C. Liang, and L. Huang, &ldquo;A stable long-term tracking method for group-housed pigs,&rdquo; in International Conference on Image and Graphics. Springer, 2023, pp. 238–249.</li>
<li>[9] S. Gao, P. Yang, and L. Huang, &ldquo;Scene-adaptive svad based on multimodal action-based feature extraction,&rdquo; in Proceedings of the Asian Conference on Computer Vision, 2024, pp. 2471–2488.</li>
<li>[10] T. Feng, Q. Qi, L. Guo, and J. Wang, &ldquo;Meta-uad: A meta-learning scheme for user-level network traffic anomaly detection,&rdquo; arXiv preprint arXiv:2408.17031, 2024.</li>
<li>[11] T. Feng, X. Wang, F. Han, L. Zhang, and W. Zhu, &ldquo;U2udata: A large-scale cooperative perception dataset for swarm uavs autonomous flight,&rdquo; in Proceedings of the 32nd ACM International Conference on Multimedia, 2024, pp. 7600–7608.</li>
<li>[12] H. Zhou, J. Yu, and W. Yang, &ldquo;Dual memory units with uncertainty regulation for weakly supervised video anomaly detection,&rdquo; arXiv preprint arXiv:2302.05160, 2023.</li>
<li>[13] M. Z. Zaheer, A. Mahmood, M. H. Khan, M. Segu, F. Yu, and S.-I. Lee, &ldquo;Generative cooperative learning for unsupervised video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 14 744–14 754.</li>
<li>[14] H. K. Joo, K. Vo, K. Yamazaki, and N. Le, &ldquo;Clip-tsa: Clip-assisted temporal self-attention for weakly-supervised video anomaly detection,&rdquo; in 2023 IEEE International Conference on Image Processing (ICIP) , 2023, pp. 3230–3234.</li>
<li>[15] P. Wu, X. Zhou, G. Pang, L. Zhou, Q. Yan, P. Wang, and Y. Zhang, &ldquo;Vadclip: Adapting vision-language models for weakly supervised video anomaly detection,&rdquo; arXiv preprint arXiv:2308.11681, 2023.</li>
<li>[16] A. Acsintoae, A. Florescu, M.-I. Georgescu, T. Mare, P. Sumedrea, R. T. Ionescu, F. S. Khan, and M. Shah, &ldquo;Ubnormal: New benchmark for supervised open-set video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 20 143–20 153.</li>
<li>[17] G. Wang, Y. Wang, J. Qin, D. Zhang, X. Bao, and D. Huang, &ldquo;Video anomaly detection by solving decoupled spatio-temporal jigsaw puzzles,&rdquo; in European Conference on Computer Vision, 2022, pp. 494–511.</li>
<li>[18] W. Liu, W. Luo, D. Lian, and S. Gao, &ldquo;Future frame prediction for anomaly detection–a new baseline,&rdquo; in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 6536–6545.</li>
<li>[19] C. Park, M. Cho, M. Lee, and S. Lee, &ldquo;Fastano: Fast anomaly detection via spatio-temporal patch transformation,&rdquo; in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , 2022, pp. 2249–2259.</li>
<li>[20] G. Yu, S. Wang, Z. Cai, E. Zhu, C. Xu, J. Yin, and M. Kloft, &ldquo;Cloze test helps: Effective video anomaly detection via learning to complete video events,&rdquo; in Proceedings of the 28th ACM international conference on multimedia, 2020, pp. 583–591.</li>
<li>[21] M. I. Georgescu, R. T. Ionescu, F. S. Khan, M. Popescu, and M. Shah, &ldquo;A background-agnostic framework with adversarial training for abnormal event detection in video,&rdquo; IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 9, pp. 4505–4523, 2021.</li>
<li>[22] Y. Liu, D. Li, W. Zhu, D. Yang, J. Liu, and L. Song, &ldquo;Msn-net: Multiscale normality network for video anomaly detection,&rdquo; in ICASSP 20232023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1–5.</li>
<li>[23] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Rojas, G. Feng, H. Zhao, H. Lai et al., &ldquo;Chatglm: A family of large language models from glm-130b to glm-4 all tools,&rdquo; arXiv preprint arXiv:2406.12793 , 2024.</li>
<li>[24] W. Hong, W. Wang, M. Ding, W. Yu, Q. Lv, Y. Wang, Y. Cheng, S. Huang, J. Ji, Z. Xue et al., &ldquo;Cogvlm2: Visual language models for image and video understanding,&rdquo; arXiv preprint arXiv:2408.16500 , 2024.</li>
<li>[25] H. Liu, C. Li, Y. Li, and Y. J. Lee, &ldquo;Improved baselines with visual instruction tuning,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 26 296–26 306.</li>
<li>[26] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan, &ldquo;Video-chatgpt: Towards detailed video understanding via large vision and language models,&rdquo; arXiv preprint arXiv:2306.05424, 2023.</li>
<li>[27] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., &ldquo;The llama 3 herd of models,&rdquo; arXiv preprint arXiv:2407.21783, 2024.</li>
<li>[28] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., &ldquo;Llama 2: Open foundation and fine-tuned chat models,&rdquo; arXiv preprint arXiv:2307.09288, 2023.</li>
<li>[29] L. Zanella, W. Menapace, M. Mancini, Y. Wang, and E. Ricci, &ldquo;Harnessing large language models for training-free video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18 527–18 536.</li>
<li>[30] Z. Liu, X.-M. Wu, D. Zheng, K.-Y. Lin, and W.-S. Zheng, &ldquo;Generating anomalies for video anomaly detection with prompt-based feature mapping,&rdquo; in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 24 500–24 510.</li>
<li>[31] N. Madan, N.-C. Ristea, R. T. Ionescu, K. Nasrollahi, F. S. Khan, T. B. Moeslund, and M. Shah, &ldquo;Self-supervised masked convolutional transformer block for anomaly detection,&rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.</li>
<li>[32] N.-C. Ristea, F.-A. Croitoru, R. T. Ionescu, M. Popescu, F. S. Khan, and M. Shah, &ldquo;Self-distilled masked auto-encoders are efficient video anomaly detectors,&rdquo; arXiv preprint arXiv:2306.12041, 2023.</li>
<li>[33] H. Lv, Z. Yue, Q. Sun, B. Luo, Z. Cui, and H. Zhang, &ldquo;Unbiased multiple instance learning for weakly supervised video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 8022–8031.</li>
<li>[34] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., &ldquo;Learning transferable visual models from natural language supervision,&rdquo; in International conference on machine learning, 2021, pp. 8748–8763.</li>
<li>[35] V. Mahadevan, W. Li, V. Bhalodia, and N. Vasconcelos, &ldquo;Anomaly detection in crowded scenes,&rdquo; in IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2010, pp. 1975–1981.</li>
<li>[36] C. Lu, J. Shi, and J. Jia, &ldquo;Abnormal event detection at 150 fps in matlab,&rdquo; in Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 2720–2727.</li>
<li>[37] W. Sultani, C. Chen, and M. Shah, &ldquo;Real-world anomaly detection in surveillance videos,&rdquo; in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 6479–6488.</li>
<li>[38] P. Wu, J. Liu, Y. Shi, Y. Sun, F. Shao, Z. Wu, and Z. Yang, &ldquo;Not only look, but also listen: Learning multimodal violence detection under weak supervision,&rdquo; in Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16. Springer, 2020, pp. 322–339.</li>
<li>[39] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra, &ldquo;Imagebind: One embedding space to bind them all,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 15 180–15 190.</li>
<li>[40] M. Astrid, M. Z. Zaheer, and S.-I. Lee, &ldquo;Synthetic temporal anomaly guided end-to-end video anomaly detection,&rdquo; in Proceedings of the IEEE International Conference on Computer Vision, 2021, pp. 207–214.</li>
</ul>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/SUVAD_Semantic_Understanding_Based_Video_Anomaly_Detection_Using_MLLM.md"
          data-oid-likes="likes_papers/SUVAD_Semantic_Understanding_Based_Video_Anomaly_Detection_Using_MLLM.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/text-driven-traffic-anomaly-detection-with-temporal-high-frequency/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/survey-paper/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
