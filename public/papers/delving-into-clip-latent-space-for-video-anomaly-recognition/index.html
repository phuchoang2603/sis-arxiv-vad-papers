<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/delving-into-clip-latent-space-for-video-anomaly-recognition/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/delving-into-clip-latent-space-for-video-anomaly-recognition/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/delving-into-clip-latent-space-for-video-anomaly-recognition\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "11424"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>11424 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">54 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">Delving into CLIP latent space for Video Anomaly Recognition
    <div id="delving-into-clip-latent-space-for-video-anomaly-recognition" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#delving-into-clip-latent-space-for-video-anomaly-recognition" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Luca Zanella ⋄a,∗∗ , Benedetta Liberatori ⋄a , Willi Menapace a , Fabio Poiesi b , Yiming Wang b , Elisa Riccia,b a University of Trento, Trento, Italy b</p>
<p>Fondazione Bruno Kessler, Trento, Italy</p>

<h2 class="relative group">ABSTRACT
    <div id="abstract" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#abstract" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We tackle the complex problem of detecting and recognising anomalies in surveillance videos at the frame level, utilising only video-level supervision. We introduce the novel method AnomalyCLIP , the first to combine Large Language and Vision (LLV) models, such as CLIP, with multiple instance learning for joint video anomaly detection and classification. Our approach specifically involves manipulating the latent CLIP feature space to identify the normal event subspace, which in turn allows us to effectively learn text-driven directions for abnormal events. When anomalous frames are projected onto these directions, they exhibit a large feature magnitude if they belong to a particular class. We also introduce a computationally efficient Transformer architecture to model short- and long-term temporal dependencies between frames, ultimately producing the final anomaly score and class prediction probabilities. We compare AnomalyCLIP against state-of-the-art methods considering three major anomaly detection benchmarks, i.e. ShanghaiTech, UCF-Crime, and XD-Violence, and empirically show that it outperforms baselines in recognising video anomalies. Project website and code are available at <a
  href="https://luca-zanella-dvl.github.io/AnomalyCLIP/"
    target="_blank"
  >https://luca-zanella-dvl.github.io/AnomalyCLIP/</a> .</p>
<p>© 2023 This manuscript version is made available under the CC-BY-NC-ND 4.0 license .</p>

<h2 class="relative group">1. Introduction
    <div id="1-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video anomaly detection (VAD) is the task of automatically identifying activities that deviate from normal patterns in videos (Suarez and Naval Jr , 2020). VAD has been widely studied by the computer vision and multimedia communities (Bao et al. , 2022; Feng et al. , 2021b; Mei and Zhang , 2017; Nayak et al. , 2021; Sun et al. , 2022; Wang et al. , 2020; Xu et al. , 2019) for several important applications, such as surveillance (Sultani et al. , 2018) and industrial monitoring (Roth et al. , 2022).</p>
<p>VAD is challenging because data is typically highly imbalanced, i.e. normal events are many, whilst abnormal events are rare and sporadic. VAD can be addressed as an outof-distribution detection problem, i.e. one-class classification (OOC) (Liu et al. , 2021; Lv et al. , 2021; Park et al. , 2020; Xu et al. , 2019): only visual data corresponding to the normal state is used as training data, and an input test video is classified as normal or abnormal based on its deviation from the learnt</p>
<p>∗∗ Corresponding author:</p>
<p>e-mail: <a
  href="mailto:luca.zanella-3@unitn.it">luca.zanella-3@unitn.it</a> (Luca Zanella ⋄ )</p>
<p>⋄ Equal contribution.</p>
<p>normal state. However, OOC methods can be particularly ineffective in complex real-world applications where normal activities are diverse. An uncommon normal activity may cause a false alarm because it differs from the learnt normal activities. Alternatively, VAD can be addressed with fully-supervised approaches based on frame-level annotations (Bai et al. , 2019; Wang et al. , 2019). Despite their good performance, they are considered impractical because annotations are costly to produce. Unsupervised approaches can also be used, but their performance in complex settings is not yet satisfactory (Zaheer et al. , 2022). For these reasons, the most recent approaches are designed for weakly-supervised learning scenarios (Li et al. , 2022a; Sultani et al. , 2018; Tian et al. , 2021; Wu and Liu , 2021): they exploit video-level supervision.</p>
<p>Whilst existing weakly-supervised VAD methods have shown to be effective in anomaly detection (Li et al. , 2022a), they are not designed for recognising anomaly types (e.g. shooting vs. explosion). Performing Video Anomaly Recognition (VAR) in addition to VAD, that is not only detecting anomalous events but also recognising the underlying activities, is desirable as it provides more informative and actionable insights. However, addressing VAR in a weakly-supervised setting is highly challenging due to the extreme data imbalance</p>
<p>Fig. 1: Comparison of various anomaly recognition methods on the ShanghaiTech, UCF-Crime, and XD-Violence datasets in terms of the mean area under the curve (mAUC) of the receiver operating characteristic (ROC) and the mean average precision (mAP) of the precision-recall curve (PRC), which calculate the mean of binary AUC ROC and AP PRC values for all anomalous classes, respectively. A higher mAUC and mAP are crucial for video anomaly recognition as they reflect the model&rsquo;s ability in correctly recognising the correct abnormal class. Notably, our proposed method, AnomalyCLIP, achieves the highest performance on all datasets, surpassing both the state-of-the-art methods on video anomaly detection that are re-purposed for anomaly recognition and CLIP-based video action recognition methods.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_58e9e0699c2666368c73396b560dc0d46021509277cccd44f5fac58b9cba1d24.png"
    ></figure>
<p>and the limited samples representing each anomaly (Sultani et al. , 2018).</p>
<p>We have recently experienced the emergence of powerful deep learning models that are trained on massive web-scale datasets (Schuhmann et al. , 2021). These models, commonly referred to as Large Language and Vision (LLV) models or foundation models (Radford et al. , 2021; Singh et al. , 2022), have shown strong generalisation capabilities in several downstream tasks and have become a key ingredient of modern computer vision and multimedia systems. These pre-trained models are publicly available and can be seamlessly integrated into any recognition system. LLV models can also be effectively applied to videos and to supervised action recognition tasks (Wang et al. , 2021; Xu et al. , 2021).</p>
<p>In this paper, we introduce the first method that jointly addresses VAD and VAR with LLV models. We argue that by leveraging representations derived from LLV models, we can obtain more discriminative features for recognising and classifying abnormal behaviours. However, as supported by our experiments (Fig. 1), a naive application of existing LLV models to VAR-VAD does not suffice due to the imbalance of the training data and the subtle differences between frames of the same video containing and non containing anomalous contents.</p>
<p>Therefore, we propose AnomalyCLIP, a novel solution for VAR based on the CLIP model (Radford et al. , 2021), achieving state-of-the-art anomaly recognition performance as shown in Fig. 1 .</p>
<p>AnomalyCLIP produces video representations that can be mapped to the textual description of the anomalous event. Rather than directly operating on the CLIP feature space, we re-centre it around a normality prototype, as shown in Fig. 2 (a). In this way, the space assumes important semantics: the magnitude of the features indicates the degree of anomaly, while the direction from the origin indicates the anomaly type. To learn the directions that represent the desired anomaly classes, we propose a Selector model that employs prompt learning and a projection operator tailored to our new space to identify the parts in a video that better match the textual description of the anomaly. This ability is instrumental to address the data imbalance problem. We use the predictions of the Selector model to implement a semantically-guided Multiple Instance Learning (MIL) strategy that aims to widen the gap between the most anomalous segments of anomalous videos and normal ones. Differently from the features typically employed in VAD that are extracted using temporal-aware backbones (Carreira and Zisserman , 2017; Liu et al. , 2022), CLIP visual features do not bear any temporal semantics as it operates at the image level. We thus propose a Temporal model, implemented as an Axial Transformer (Ho et al. , 2019), which models both short-term relationships between successive frames and long-term dependencies between parts of the video.</p>
<p>As illustrated in Fig.1, we evaluate the proposed approach on three benchmark datasets, ShanghaiTech (Liu et al. , 2018), UCF-Crime (Sultani et al. , 2018) and XD-Violence (Wu et al. , 2020), and empirically show that our method achieves state-ofthe-art performance in VAR.</p>
<p>The contributions of our paper are summarised as follows:</p>
<ul>
<li>we propose the first method for VAR that is based on LLV models to detect and classify the type of anomalous events;</li>
<li>we introduce a transformation of the LLV model feature space driven by a normality prototype to effectively learn the prompt directions for anomaly types;</li>
<li>we propose a novel Selector model that uses semantic information imbued in the transformed LLV feature space as a robust way to perform MIL segment selection and anomaly recognition;</li>
<li>we design a Temporal model to better aggregate temporal information by modelling both the short-term relationships between neighbouring frames and the long-term dependencies among segments.</li>
</ul>

<h2 class="relative group">2. Related Works
    <div id="2-related-works" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-related-works" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video Anomaly Detection. Recognising anomalous behaviours in video surveillance streams is a traditional task in computer vision and multimedia analysis. Existing methods</p>
<p>for VAD can be grouped into four main categories based on the level of supervision available during training. The first group includes fully-supervised methods that assume available framelevel annotations in the training set (Bai et al. , 2019; Wang et al. , 2019). The second group includes weakly-supervised approaches that only require video-level normal/abnormal annotations (Li et al. , 2022a , b; Sultani et al. , 2018; Tian et al. , 2021; Wu and Liu , 2021). The third group includes one-class classification methods that assume the availability of only normal training data (Liu et al. , 2021; Lv et al. , 2021; Park et al. , 2020). The fourth group includes unsupervised models that do not use training data annotations (Narasimhan , 2018; Zaheer et al. , 2022).</p>
<p>Amongst these types of methods, weakly-supervised approaches have gained higher popularity, as they typically yield good results while limiting the annotation effort. Sultani et al. (2018) were the first to formulate weakly-supervised VAD as a multiple-instance learning (MIL) task, dividing each video into short segments that form a set, known as bag. Bags generated from abnormal videos are called positive bags, and those generated from normal videos negative bags. Since this pioneering work, MIL has become a paradigm for VAD and several subsequent works have proposed to refine the associated ranking model to more robustly predict anomaly scores. For example, Tian et al. (2021) proposed a Robust Temporal Feature Magnitude (RTFM) loss that is applied to a deep network consisting of a pyramid of dilated convolutions and a self-attention mechanism to model both short-term and long-term relationships between video snippets close in time and events in the whole video. Wu et al. (2022) introduced Self-Supervised Sparse Representation Learning, an approach that combines dictionary-based representation with self-supervised learning techniques to identify abnormal events. Chen et al. (2022) introduced Magnitude-Contrastive Glance-and-Focus Network, a neural network that uses a feature amplification mechanism and a magnitude contrastive loss to enhance the importance of feature discriminative for anomalies. Motivated by the fact that anomalies can occur at any location and at any scale of the video, Li et al. (2022a) proposed Scale-Aware Spatio-Temporal Relation Learning (SSRL), an approach that extends RTFM by not only learning short-term and long-term temporal relationships but also learning multi-scale region-aware features. While SSRL achieves state-of-the-art results in common VAD benchmarks, its high computational complexity limits its applicability. To the best of our knowledge no previous works have explored foundation models (Radford et al. , 2021) for VAD, as we propose in this work.</p>
<p>Large Language and Vision models. The emergence of novel large multimodal neural networks (Radford et al. , 2021; Schuhmann et al. , 2021 , 2022; Singh et al. , 2022), which can learn joint visual-text embedding spaces, has enabled unprecedented results in several image and video understanding tasks. Current LLV models adopt modality-specific encoders and are trained via contrastive techniques to align the data representations from different modalities (Jia et al. , 2021; Radford et al. , 2021). Despite their simplicity, these methods have been shown to achieve impressive zero-shot generalisation capabilities. While earlier approaches such as CLIP (Radford et al. , 2021) operate on images, LLV models have recently and successfully been extended to the video domains. VideoCLIP (Xu et al. , 2021) is an example of this and it is designed to align video and textual representations by contrasting temporally overlapping video-text pairs with mined hard negatives. VideoCLIP can achieve strong zeroshot performance in several video understanding tasks. ActionCLIP (Wang et al. , 2021) models action recognition as a video-text matching problem rather than a classical 1-out-of-N majority vote task. Similarly to ours, their method uses the feature space of CLIP to learn semantically-aware representations of videos. However, a direct exploitation of the CLIP feature space fails in capturing information on anomalous events for which a specific adaptation, proposed in this work, is necessary. In addition, action recognition methods often fall short in weakly-supervised VAD tasks due to data imbalance between normal and abnormal events, coupled with the need for framelevel evaluation at test time, despite only having video-level supervision. To the best of our knowledge, no prior work has specifically utilised LLV models to tackle the VAD problem.</p>

<h2 class="relative group">3. Proposed approach
    <div id="3-proposed-approach" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-proposed-approach" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Weakly-supervised VAD is the task of learning to classify each frame in a video as either normal or anomalous using a dataset of tuples in the form (V , y), where V is a video and y a binary label indicating whether the video contains an anomaly in any of its frames. With respect to VAD, in VAR we introduce the additional task of recognising the type of the detected anomaly in each frame. Therefore, VAR considers a dataset of tuples (V , c ), where c indicates the type of anomaly in the video (c = ∅ means no anomaly is present, thus being Normal). In the following, we omit the subscripts for the purpose of readability.</p>
<p>To address the video-level supervision and the imbalance between normal videos and abnormal ones in VAD, the Multiple Instance Learning (MIL) framework (Sultani et al. , 2018) is widely used. MIL models each video as a bag of segments V = [S1 , &hellip;, S S ] ∈ R S×F×D , where S is the number of segments, F is the number of frames in each segment, and D is the number of features associated to each frame. Each segment can be seen as S = [x1 , &hellip;, xF] ∈ R F×D where x ∈ R D is the feature corresponding to each frame. MIL computes a likelihood of each frame being anomalous, selects the most anomalous ones based on it, and maximises the difference in the predicted likelihood between the normal frames and the ones selected as the most anomalous.</p>
<p>In this paper, we propose to leverage the CLIP model (Radford et al. , 2021) to address VAR and show that:</p>
<p>i) the alignment between the visual and textual modalities in the CLIP feature space can be used as an effective likelihood estimator for anomalies; ii) such estimator, not only can detect anomalous occurrences, but also their types; iii) such estimator is effective only when adopting our proposed CLIP space re-centring transformation (see Fig. 2 (a)). Our method is composed of two models as shown in Fig. 2 (b): a Selector model and a Temporal model. The Selector model S produces the likelihood that each frame belongs to an anomalous</p>
<p>Fig. 2: (a) Illustration of the CLIP space and the effects of the re-centring transformation with features of normal. When the space is not re-centred around the normality prototype m, directions d ′ are similar, making it difficult to discern anomaly types, and feature magnitude is not linked to the degree of anomaly, making it difficult to identify anomalous events. When re-centred, the distribution of the magnitudes of features projected on each d identifies the degree of detected anomaly of the corresponding type. (b) Illustration of our proposed framework. The Selector model learns directions d using CoOp (Zhou et al. , 2022), and uses them to identify the likelihood of each feature x to represent an occurrence of the corresponding anomalous class. MIL selection of the top-K and bottom-K abnormal segments is performed by considering the distribution of likelihoods along the corresponding direction. A Temporal model performs temporal aggregation of the features to produce the final prediction.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_bdd83483fb3e1b0dc6436facabf91221bc36660d15f063ce8b5d9a74a2807cdf.png"
    ></figure>
<p>class S(x) ∈ R C , where C is the number of anomalous classes. We exploit the vision-text alignment in the CLIP feature space and the CoOp prompt learning approach (Zhou et al. , 2022) to estimate this likelihood. The Temporal model T assigns a binary likelihood to each frame of a video indicating whether the frame is anomalous or normal. Unlike S , T exploits temporal information to improve predictions and we implement it with a Transformer network (Ho et al. , 2019). The predictions from S and T are then aggregated to produce a distribution indicating the probability of a frame being normal or abnormal, and which abnormal class it belongs to. We train our model using a combination of MIL and regularisation losses. Importantly, as T is randomly initialised, the likelihood scores are less reliable, thus we always use the likelihoods produced by S to perform segment selection in MIL.</p>
<p>We describe the proposed Selector model and Temporal model in detail in Sec. 3.1 and Sec. 3.2, respectively. In Sec. 3.3, we show how we aggregate the predictions of both models for estimating the final probability distribution. Finally, we describe the training and inference in Sec. 3.4 .</p>

<h2 class="relative group">3.1. Selector model
    <div id="31-selector-model" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#31-selector-model" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>It is crucial for VAD and VAR to reliably distinguish anomalous and normal frames in anomalous videos given only videolevel weak supervision. Motivated by the recent findings in applying LLV models to video action recognition tasks (Wang et al. , 2021; Xu et al. , 2021), we propose a novel likelihood estimator, encapsulated by our Selector model, that combines the CLIP (Radford et al. , 2021) feature space and the CoOp (Zhou et al. , 2022) prompt learning approach to learn a set of directions in this space that identify each type of anomaly and their likelihood.</p>
<p>Our main intuition (see Fig. 2 (a)) is that the CLIP feature space presents an underlying structure where the set of CLIP features extracted for each frame in the dataset forms a space that is clustered around a central point which we call the normality prototype. Consequently, the difference between a fea- ture and the normal prototype determines important characteristics: the magnitude of the distance reflects the likelihood of it being abnormal, while its direction indicates the type of anomaly. Such important characteristics would not be exploited by a naive application of the CLIP feature space to VAR (see Table 9). Unleashing the potential of this space in detecting anomalies thus requires a re-centring transformation, a main contribution of this work.</p>
<p>Following this intuition, we define the normal prototype m as the average feature extracted by the CLIP image encoder EI on all N frames I contained in videos labelled as normal in the dataset:</p>
<!-- formula-not-decoded -->
<p>For each frame I in the dataset, we produce frame features x by subtracting the normality prototype from the CLIP encoded feature, i.e., x = EI(I) − m .</p>
<p>We then exploit the visual-text aligned CLIP feature space and learn the textual prompt embeddings whose directions are used to indicate the anomalous classes. In particular, we employ the prompt learning CoOp method (Zhou et al. , 2022) which we find ideal to find such directions as empirically demonstrated by our experiments (see Sec. 4.3).</p>
<p>Given a class c and the textual description of the corresponding label t c expressed as a sequence of token embeddings, we consider a sequence of learnable context vectors t ctx and derive the corresponding direction for the class d c ∈ R D as:</p>
<!-- formula-not-decoded -->
<p>where E T indicates the CLIP text encoder. The use of the textual description acts as a prior for the learned direction to match the corresponding type of anomaly, while the context vectors are jointly optimised during training as part of the parameters of S in order to enable the refinement of the direction. A different direction is learned for each class.</p>
<p>The learned directions serve as the base for our Selector</p>
<p>model S. As shown in Fig. 2(b), the magnitude of the projection of frame feature x on direction d c indicates the likelihood of the anomalous class c:</p>
<!-- formula-not-decoded -->
<p>where P indicates our projection operation. However, simply projecting the feature vector on the direction would make the magnitude of the projection susceptible to scale, where anomalous features of one class can potentially have a different magnitude from features of another anomalous class. To mitigate this issue, we perform a batch normalisation (Ioffe and Szegedy , 2015) after the projection which produces a distribution of projected features with zero mean and unitary variance:</p>
<!-- formula-not-decoded -->
<p>where BN indicates batch normalisation without affine transformation. As such, we expect within a batch the dominant normal features to be close to the origin and the abnormal features to be at the right side tail of the distribution.</p>
<p>The definition of likelihood can be extended to segments by summing the likelihoods of each frame:</p>
<!-- formula-not-decoded -->

<h2 class="relative group">3.2. Temporal Model
    <div id="32-temporal-model" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#32-temporal-model" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The Selector model only learns an initial time-independent separation between anomalous and normal frames as the CLIP model operates at the image frame level. However, the temporal information is an important piece of information for VAR that we can exploit. We thus propose the Temporal model T to model the relationships among frames in both short-term and long-term, to enrich the visual features and to produce the predictions that indicate the likelihood of whether a frame is anomalous:</p>
<!-- formula-not-decoded -->
<p>We use a Transformer architecture to capture the short-term temporal dependencies between frames in a segment and the long-term temporal dependencies between all segments in a video, motivated by their success in relevant sequence modelling tasks (Vaswani et al. , 2017). As all the video segments of V are received as the input, the large number of segments S and frames F increases the computational requirements for self attention. To reduce this cost, we implement T as an Axial Transformer (Ho et al. , 2019) that computes attention separately for the two axes corresponding to the segments and the features in each segment. As suggested by experiments in Sec. 4.3, Axial Transformer is also less prone to over-fitting, a likely case in VAR, as compared to standard Transformer. We terminate the model with a sigmoid activation so that the output likelihood can also be interpreted as a probability.</p>

<h2 class="relative group">3.3. Predictions Aggregation
    <div id="33-predictions-aggregation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#33-predictions-aggregation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We combine the predictions from S and T to obtain the final output: the probabilities indicating whether a frame is normal or anomalous (pN(x) and pA(x)) and the probability that a frame presents an anomaly of a certain class (pA , c (x)).</p>
<p>Given an input frame feature x, we define its probability of being anomalous pA(x) as its corresponding output from the Temporal model T. The probability of the frame being normal is pN(x) = 1 − pA(x). To obtain the probability distribution of the frame to present an anomaly of a specific class pA , c (x), we employ the predictions of the Selector model that can be seen as the conditional distribution over the anomalous classes pc|A(x) = softmax(S(x)). From the definition of conditional probability it follows that pA , c (x) = pA(x) ∗ p c |A (x).</p>

<h2 class="relative group">3.4. Training
    <div id="34-training" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#34-training" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We train the model following the MIL framework. Specifically, MIL considers a batch with an equal number of normal and anomalous videos, uses the predicted likelihoods to identify the top-K most abnormal segments in anomalous videos, and imposes separation from the other, normal ones (Sultani et al. , 2018). Due to the higher capacity of T with respect to S and its initial random initialisation, T can not directly perform this selection since the predicted likelihoods would be excessively noisy. Instead, we use the likelihood predictions from S to perform MIL segment selection.</p>
<p>Our framework is trained end-to-end using losses on anomalous videos, losses on normal videos, and regularization losses, which we describe in the following.</p>
<p>Given an anomalous video V of class c, we define the set of top-K most anomalous segments V + = {S + 1
, &hellip;, S + K } and, symmetrically, of bottom-K least anomalous segments V − = {S − 1
, &hellip;, S − K } according to the likelihood assigned by the framelevel model S on the direction corresponding to class c. We consider all frames in V + and maximise the likelihood of the corresponding class being predicted by S by minimising the loss L
A DIR L
A :</p>
<!-- formula-not-decoded -->
<p>where the likelihood tensor is indexed using the class c. To provide gradients to the temporal model, we also maximise pA , c (x) for each frame contained in the segments using cross entropy:</p>
<!-- formula-not-decoded -->
<p>Distinguishing normal and anomalous frames in anomalous videos is a challenging problem in VAR due to the appearance similarity between frames of the same video. To foster a better separation between these frames, we additionally consider V − and maximise pN(x) for each frame in the segments using cross entropy:</p>
<!-- formula-not-decoded -->
<p>To leverage the information in normal videos, for each segment S i in normal video V, we minimise the likelihood predicted by the Selector model:</p>
<!-- formula-not-decoded -->
<p>Following the VAD literature (Feng et al. , 2021a; Sultani et al. , 2018; Tian et al. , 2021) we also require the model to maximise the probability of each frame in its top-K most abnormal segments V + = {S + 1
, &hellip;, S + K } to be normal :</p>
<!-- formula-not-decoded -->
<p>We regularise training with two additional losses (Sultani et al. , 2018) on all frames of anomalous videos only. One is a sparsity loss on the predicted scores and encourages the minimal amount of frames to be predicted as abnormal:</p>
<!-- formula-not-decoded -->
<p>The other is a smoothness term that regularises the predictions along the temporal dimension:</p>
<!-- formula-not-decoded -->
<p>where indexing is performed on the flattened sequence of frames in the video.</p>
<p>We jointly train the Selector and Temporal models using as final training objective:</p>
<!-- formula-not-decoded -->

<h2 class="relative group">4. Experiments
    <div id="4-experiments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#4-experiments" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this section, we validate our method against a range of baselines taken from state-of-the-art VAD and action recognition methods which we adapt to the VAR task. After introducing the metrics for the novel VAR task, we perform evaluation on three datasets and perform comparison in both the VAD and VAR tasks. An extensive ablation study is performed to justify our main design choices. Sec 4.1 describes our experiment setup in terms of datasets and evaluation protocols. We then present and discuss the results in comparison against state-ofthe-art methods in Sec 4.2 and the ablation study in Sec 4.3 .</p>

<h2 class="relative group">4.1. Experiment Setup
    <div id="41-experiment-setup" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#41-experiment-setup" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Datasets. We perform our study using three widely-used VAD datasets, i.e., ShanghaiTech (Liu et al. , 2018), UCF-Crime (Sultani et al. , 2018), and XD-Violence (Wu et al. , 2020). ShanghaiTech consists of 437 videos, recorded from multiple surveillance cameras in a university campus. A total of 130 abnormal events of 17 anomaly classes are captured in 13 different scenes. We adopt the dataset in the configuration of Zhong et al. (2019) which adapts it to the weakly-supervised setting by organising it into 238 training videos and 199 testing videos. UCF-Crime is a large-scale dataset of real-world surveillance videos, containing 1900 long untrimmed videos that cover 13 real-world anomalies with significant impacts on public safety. The training set consists of 800 normal and 810 anomalous videos and the testing set includes the remaining 150 normal and 140 anomalous videos. XD-Violence is a large-scale violence detection dataset comprising 4754 untrimmed videos with audio signals and weak labels, divided into a training set of 3954 videos and a test set of 800 videos. With a total duration of 217 hours, the dataset covers various scenarios and captures 6 categories of anomalies. Notably, each violent video may have multiple labels, ranging from 1 to 3. To accommodate our training setup, where only one anomaly type per video is considered, we select the subset of 4463 videos containing at most one anomaly.</p>
<p>Performance Metrics. We perform evaluation in terms of both VAD and VAR. Following previous works, we measure the performance regarding VAD using the area under the curve (AUC) of the frame-level receiver operating characteristics (ROC) as it is agnostic to thresholding for the detection task. A larger frame-level AUC means a better performance in classifying between normal and anomalous events. To measure the VAR performance, we extend the AUC metric to the multi-classification scenario. For each anomalous class, we measure the AUC by considering the anomalous frames of the class as positive and all other frames as negatives. Successively, the mean AUC (mAUC) is computed over all the anomalous classes. Similarly, for the XD-Violence dataset, we follow the established evaluation protocol (Wu et al. , 2020) and present VAD results using the average precision (AP) of the precision-recall curve (PRC), while for VAR results we report the mean AP (mAP), which is calculated by averaging the binary AP values across all anomalous classes.</p>
<p>Implementation details. At training time, each video is divided into S non-overlapping blocks. From each block, a random start-index is sampled from which segments of F consecutive frames are considered. If the raw video has length smaller than S × F, we adopt loop padding and repeat the video from the start until the minimum length of S × F is reached. Each mini-batch of size B used for training is composed of B/2 normal clips and B/2 anomalous clips. This is a simple but effective way to balance the mini-batch formation, which otherwise will contain mainly normal clips. At inference, to handle videos covering arbitrary temporal windows, we first divide each video V into S non-overlapping blocks, where each block contains frames whose number is a multiple of F, i.e., J × F, where J depends on the length of V ⋄ . We process V with J inferences to classify all frames in the video. At each j th inference, we extract the j th consecutive F frames from each block, forming segments with a total of S × F that span the whole video. We then feed the segments into our approach so that our Temporal model can reason the long-term temporal relationships among segments.</p>
<p>For a fair comparison with previous works in VAD (Tian et al. , 2021; Wu et al. , 2022; Li et al. , 2022a), we use K = 3 for the MIL selection of the top-K and bottom-K abnormal segments, S = 32 number of segments, F = 16 frames per segment and B = 64 batch size. Please refer to Appendix A for more implementation details and Appendix B for more details on hyper-parameters.</p>
<p>⋄</p>
<p>We perform loop padding to ensure that each video is of length J × S × F</p>
<p>Table 1: Results of the state-of-the-art methods and our AnomalyCLIP in terms of VAD and VAR on ShanghaiTech.</p>
<table>
  <thead>
      <tr>
          <th>Supervision</th>
          <th>Method</th>
          <th>Features</th>
          <th>VAD</th>
          <th>VAR</th>
          <th>AUC(%) m</th>
          <th>mAUC(%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>One-class</td>
          <td>MNAD (Park et al., 2020)  MPN (Lv et al., 2021)  HF2VAD (Liu et al., 2021)  Zaheer et al. (2022)</td>
          <td></td>
          <td>✓  ✓  ✓</td>
          <td></td>
          <td>70.50 73.80</td>
          <td></td>
      </tr>
      <tr>
          <td>One-class</td>
          <td>MNAD (Park et al., 2020)  MPN (Lv et al., 2021)  HF2VAD (Liu et al., 2021)  Zaheer et al. (2022)</td>
          <td></td>
          <td>✓</td>
          <td></td>
          <td>73.80</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>HF2VAD (Liu et al., 2021</td>
          <td></td>
          <td>✓</td>
          <td></td>
          <td>76.20</td>
          <td></td>
      </tr>
      <tr>
          <td>Ui</td>
          <td>Zaheer et al(2022)</td>
          <td>ResNext</td>
          <td>✓</td>
          <td></td>
          <td>7893</td>
          <td></td>
      </tr>
      <tr>
          <td>Unsupervised</td>
          <td>()  CLIP (Rdfd t l</td>
          <td>ResNext  i/6</td>
          <td>✓</td>
          <td>✓</td>
          <td>49.17</td>
          <td>51.02</td>
      </tr>
      <tr>
          <td></td>
          <td>Sultani et al. (2018)</td>
          <td>C3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>86.30</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>IBL (Zhang et al., 2019)</td>
          <td>C3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>82.50</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>Zaheer et al. (2022)</td>
          <td>ResNext</td>
          <td>✓</td>
          <td></td>
          <td>86.21</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>GCN (Zhong et al., 2019)</td>
          <td>TSN-RGB</td>
          <td>✓</td>
          <td></td>
          <td>84.44</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>MIST (Feng et al., 2021a</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>94.83</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>Wu et al. (2020)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>Weakly-</td>
          <td>CLAWS (Zaheer et al., 2020)</td>
          <td>C3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>89.67</td>
          <td></td>
      </tr>
      <tr>
          <td>Weakly-</td>
          <td>RTFM (Tian et al., 2021)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>97.21</td>
          <td>81.60</td>
      </tr>
      <tr>
          <td></td>
          <td>Wu and Liu (2021)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>97.48</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>MSL (Li et al., 2022b)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>96.08</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>MSL (Li et al., 2022b)</td>
          <td>VideoSwin-RGB</td>
          <td>✓</td>
          <td></td>
          <td>97.32</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>S3R (Wu et al., 2022)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>97.48</td>
          <td>87.88</td>
      </tr>
      <tr>
          <td></td>
          <td>MGFN (Chen et al., 2022)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>MGFN (Chen et al., 2022)</td>
          <td>VideoSwin-RGB</td>
          <td>✓</td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>ActionCLIP (Wang et al., 2021)</td>
          <td>ViT-B/16</td>
          <td></td>
          <td>✓</td>
          <td>96.36</td>
          <td>75.63</td>
      </tr>
      <tr>
          <td></td>
          <td>ActionCLIP (Wang et al., 202</td>
          <td>ViT-B/16</td>
          <td></td>
          <td>✓</td>
          <td>96.36</td>
          <td>75.63</td>
      </tr>
  </tbody>
</table>
<p>Table 2: Results of the state-of-the-art methods and our AnomalyCLIP in terms of VAD and VAR on UCF-Crime.</p>
<table>
  <thead>
      <tr>
          <th>Supervision</th>
          <th>Method</th>
          <th>Features</th>
          <th>VAD V</th>
          <th>AR AUC</th>
          <th>mAUC(%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>One-class</td>
          <td>SVM Baseline (Sultani et al., 2018)  SSV (Sohrab et al., 2018)  BODS (Wang and Cherian, 2019)  GODS (Wang and Cherian, 2019)  Zaheer et al. (2022)</td>
          <td>I3D-RGB  I3D-RGB</td>
          <td>✓  ✓  ✓  ✓  ✓</td>
          <td>50.00 58.50 68.26 70.46 74.20</td>
          <td>()</td>
      </tr>
      <tr>
          <td>Un-supervised</td>
          <td>CLIP (Radford et al2021</td>
          <td>ResNext</td>
          <td>✓  ✓</td>
          <td>5863</td>
          <td>74.28</td>
      </tr>
      <tr>
          <td></td>
          <td>BODS (Wang and Cherian, 2019)  GODS (Wang and Cherian, 2019)  Zaheer et al. (2022)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td>686 70.46 74.20</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>Zaheer et al. (2022)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td>74.20</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>Zaheer et al. (2022)</td>
          <td>ResNext</td>
          <td>✓</td>
          <td>74.20</td>
          <td></td>
      </tr>
      <tr>
          <td>Un-supervised</td>
          <td>Zaheer et al. (2022)</td>
          <td>ViT-B/16</td>
          <td></td>
          <td>58.63</td>
          <td>74.28</td>
      </tr>
      <tr>
          <td></td>
          <td>Sultani et al. (2018)</td>
          <td>C3D-RGB</td>
          <td>✓</td>
          <td>75.41</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>Sultani et al. (2018)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td>77.92</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>IBL (Zhang et al., 2</td>
          <td>C3D-RGB</td>
          <td>✓</td>
          <td>79.84</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>GCN (Zhong et al., 2019</td>
          <td>TSN-RGB</td>
          <td>✓</td>
          <td>82.12</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>MIST (Feng et al., 2021a</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td>82.30</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>Wu et al. (2020)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td>82.44</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>CLAWS (Zaheer et al., 202</td>
          <td>C3D-RGB</td>
          <td>✓</td>
          <td>82.44 8303</td>
          <td></td>
      </tr>
      <tr>
          <td>Weakly-  supervised</td>
          <td>RTFM (Tian et al., 2021)</td>
          <td>VideoSwin-RGB</td>
          <td>✓</td>
          <td>83.31</td>
          <td></td>
      </tr>
      <tr>
          <td>Weakly-  supervised</td>
          <td>RTFM (Tian et al., 2021)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td>84.03</td>
          <td>84.86</td>
      </tr>
      <tr>
          <td></td>
          <td>Wu and Liu (2021)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td>84.89</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>MSL (Li et al., 2022b</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td>85.30</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>MSL (Li et al., 2022b)</td>
          <td>VideoSwin-RGB</td>
          <td>✓</td>
          <td>85.62</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>S3R (Wu et al., 2022)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td>85.99</td>
          <td>86.55</td>
      </tr>
      <tr>
          <td></td>
          <td>MGFN (Chen et al., 2022)</td>
          <td>VideoSwin-RGB</td>
          <td></td>
          <td>86.67</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>MGFN (Chen et al., 2022)</td>
          <td>I3D-RGB</td>
          <td></td>
          <td>86.98</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>SSRL (Li et al., 2022a)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td>87.43</td>
          <td>88.88</td>
      </tr>
      <tr>
          <td></td>
          <td>ActionCLIP (Wang et al., 2021</td>
          <td>ViT-B/16</td>
          <td></td>
          <td>82.30</td>
          <td>88.88 8772</td>
      </tr>
      <tr>
          <td></td>
          <td>AnomalyCLIP (ours)</td>
          <td>ViT-B/16</td>
          <td>✓</td>
          <td>86.36</td>
          <td>87.72</td>
      </tr>
  </tbody>
</table>
<p>Table 3: Results of the state-of-the-art methods and our AnomalyCLIP in terms of VAD and VAR on XD-Violence.</p>
<table>
  <thead>
      <tr>
          <th>Supervision</th>
          <th>Method</th>
          <th>Features</th>
          <th>VAD</th>
          <th>VAR</th>
          <th>AP(%)</th>
          <th>mAP(%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Zero-shot</td>
          <td>CLIP (Radford et al., 2021)</td>
          <td>ViT-B/16</td>
          <td></td>
          <td>✓</td>
          <td>27.21</td>
          <td>21.32</td>
      </tr>
      <tr>
          <td></td>
          <td>Wu et al. (2020)</td>
          <td>C3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>67.19</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>Wu et al. (2020)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>73.2</td>
          <td></td>
      </tr>
      <tr>
          <td>Weakly-  supervised</td>
          <td>MSL (Li et al., 2022b)</td>
          <td>C3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>75.53</td>
          <td></td>
      </tr>
      <tr>
          <td>Weakly-  supervised</td>
          <td>Wu and Liu (2021)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>75.9</td>
          <td></td>
      </tr>
      <tr>
          <td>Weakly-  supervised</td>
          <td>RTFM (Tian et al., 2021)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>77.81</td>
          <td>43.04</td>
      </tr>
      <tr>
          <td>Weakly-  supervised</td>
          <td>MSL (Li et al., 2022b)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>78.28</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>MSL (Li et al., 2022b)</td>
          <td>VideoSwin-RGB</td>
          <td>✓</td>
          <td></td>
          <td>78.58</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>S3R (Wu et al., 2022)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>80.26</td>
          <td>36.06</td>
      </tr>
      <tr>
          <td></td>
          <td>MGFN (Chen et al., 2022)</td>
          <td>I3D-RGB</td>
          <td>✓</td>
          <td></td>
          <td>79.19</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>MGFN (Chen et al., 2022)</td>
          <td>VideoSwin-RGB</td>
          <td>✓</td>
          <td></td>
          <td>80.11</td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td>ActionCLIP (Wang et al., 2021)</td>
          <td>ViT-B/16</td>
          <td></td>
          <td>✓</td>
          <td>61.01</td>
          <td>40.24</td>
      </tr>
      <tr>
          <td></td>
          <td>AnomalyCLIP (ours)</td>
          <td>ViT-B/16</td>
          <td>✓</td>
          <td>✓</td>
          <td>78.51</td>
          <td>49.41</td>
      </tr>
  </tbody>
</table>

<h2 class="relative group">4.2. Evaluation Against Baselines
    <div id="42-evaluation-against-baselines" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#42-evaluation-against-baselines" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Regarding VAD, we compare AnomalyCLIP against stateof-the-art methods with different supervision setups, including one-class (Park et al. , 2020; Liu et al. , 2021; Lv et al. , 2021), unsupervised (Zaheer et al. , 2022) and weakly-supervised (Li et al. , 2022a; Tian et al. , 2021; Wu et al. , 2022). As none of the above-mentioned methods address the VAR task, we produce baselines by re-purposing some best-performing VAD methods including RTFM (Tian et al. , 2021), S3R (Wu et al. , 2022) and SSRL (Li et al. , 2022a) ⋄ , and CLIP-based baselines (Radford et al. , 2021; Wang et al. , 2021):</p>
<ul>
<li>Multi-classification with RTFM (Tian et al. , 2021), S3R (Wu et al. , 2022) and SSRL (Li et al. , 2022a) (weaklysupervised) .</li>
</ul>
<p>We keep the original pretrained model frozen and add a multi-class classification head that we train to predict the class using a cross entropy objective on the top-K most anomalous segments selected as in the original method. These baselines are weakly-supervised.</p>
<ul>
<li>CLIP (Radford et al. , 2021) (zero-shot). We achieve the classification by soft-maxing of the cosine similarities of the input frame feature x with vectors corresponding to the embedding of the textual prompt &ldquo;a video from a CCTV camera of a {class} &quot; using the pre-trained CLIP model.</li>
<li>ActionCLIP (Wang et al. , 2021) (weakly-supervised). We retrain ActionCLIP (Wang et al. , 2021) on our datasets by propagating the video-level anomaly labels to each frame of the corresponding video.</li>
</ul>
<p>Table 1 presents the results on ShanghaiTech (Liu et al. , 2018). Although ShanghaiTech is a rather saturated dataset for VAD due to its simplicity in scenarios, AnomalyCLIP scores the state-of-the-art results on both VAD and VAR, with +0 . 09% and +2 . 85% in terms of AUC ROC and mAUC ROC, respectively. ActionCLIP (Wang et al. , 2021) performs poorly in terms of mAUC, which we attribute to the low proportion of abnormal events in ShanghaiTech that makes the MIL selection strategy of particular importance to avoid incorrect supervisory signals on normal frames of abnormal videos. In contrast, our proposal has a better recognition of the positive instances of abnormal videos, thus achieving better performance even when anomalies are rare. AnomalyCLIP achieves a large improvement of +45 . 44% in terms of mAUC against zero-shot CLIP, demonstrating that a naive application of a VAR pipeline in the CLIP space does not yield satisfactory results. A revision of this space, implemented as our proposed transformation, is necessary to use it effectively.</p>
<p>Table 2 reports the results on UCF-Crime (Sultani et al. , 2018). Our method exhibits the best discrimination of the anomalous classes, achieving the highest mAUC ROC among baselines. Similar to ShanghaiTech, it also achieves an improvement in terms of mAUC against zero-shot CLIP, verifying the importance of our proposed adaptation of the CLIP space. Compared to ActionCLIP (Wang et al. , 2021), our AnomalyCLIP obtains +2 . 94% in terms of mAUC, highlighting the need for a MIL framework to mitigate mis-assignment of anomalous class labels to normal frames of anomalous videos. It is also worth noting that the higher mAUC obtained by ActionCLIP does not result in a competitive AUC ROC on VAD, which implicates a worse separation between normal and abnormal frames. When compared to the best performing method</p>
<p>⋄</p>
<p>We thank authors for making their code and models publicly available</p>
<p>Table 4: Results of the state-of-the-art methods and our AnomalyCLIP in terms of VAR on UCF-Crime. The table highlights the top performers, with cells highlighted in red representing first place, cells in orange representing second place, and cells in yellow representing third place.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td>Abuse</td>
          <td>Arrest</td>
          <td>Arson</td>
          <td>Assaul</td>
          <td>Burglary</td>
          <td>Explosion</td>
          <td>Fighting</td>
          <td>RoadAcc</td>
          <td>Robbery</td>
          <td>Shooting</td>
          <td>Shoplifting</td>
          <td>Stealing</td>
          <td>Vandalism</td>
          <td>mAUC</td>
      </tr>
      <tr>
          <td>RTFM Tian et al. (2021)</td>
          <td>79.99</td>
          <td>62.57</td>
          <td>90.53</td>
          <td>82.27</td>
          <td>85.53</td>
          <td>92.76</td>
          <td>85.21</td>
          <td>90.31</td>
          <td>81.17</td>
          <td>82.82</td>
          <td>92.56</td>
          <td>90.23</td>
          <td>87.20</td>
          <td>84.86</td>
      </tr>
      <tr>
          <td>S3R Wu et al. (2022)</td>
          <td>86.38</td>
          <td>68.45</td>
          <td>92.19</td>
          <td>93.55</td>
          <td>86.91</td>
          <td>93.55</td>
          <td>81.69</td>
          <td>85.03</td>
          <td>82.07</td>
          <td>85.32</td>
          <td>91.64</td>
          <td>94.59</td>
          <td>83.82</td>
          <td>86.55</td>
      </tr>
      <tr>
          <td>SSRL Li et al. (2022a)</td>
          <td>95.33</td>
          <td>79.26</td>
          <td>93.27</td>
          <td>91.74</td>
          <td>89.06</td>
          <td>92.25</td>
          <td>87.36</td>
          <td>80.24</td>
          <td>87.75</td>
          <td>84.50</td>
          <td>92.31</td>
          <td>94.22</td>
          <td>88.17</td>
          <td>88.88</td>
      </tr>
      <tr>
          <td>CLIP zero-shot Radford et al. (2021)</td>
          <td>57.37</td>
          <td>80.65</td>
          <td>93.72</td>
          <td>80.83</td>
          <td>74.34</td>
          <td>90.31</td>
          <td>83.54</td>
          <td>87.46</td>
          <td>70.22</td>
          <td>63.99</td>
          <td>71.21</td>
          <td>45.49</td>
          <td>66.45</td>
          <td>74.28</td>
      </tr>
      <tr>
          <td>ActionCLIP Wang et al. (2021)</td>
          <td>91.88</td>
          <td>90.47</td>
          <td>89.21</td>
          <td>86.87</td>
          <td>81.31</td>
          <td>94.08</td>
          <td>83.23</td>
          <td>94.34</td>
          <td>82.82</td>
          <td>70.53</td>
          <td>91.60</td>
          <td>94.06</td>
          <td>89.89</td>
          <td>87.72</td>
      </tr>
      <tr>
          <td>AnomalyCLIP</td>
          <td>75.03</td>
          <td>94.56</td>
          <td>96.66</td>
          <td>94.80</td>
          <td>90.08</td>
          <td>94.79</td>
          <td>88.76</td>
          <td>93.30</td>
          <td>86.85</td>
          <td>87.45</td>
          <td>89.47</td>
          <td>97.00</td>
          <td>89.78</td>
          <td>90.66</td>
      </tr>
  </tbody>
</table>
<p>Table 5: Results of the state-of-the-art methods and our AnomalyCLIP in terms of VAR on ShanghaiTech. The table highlights the top performers, with cells highlighted in red representing first place, cells in orange representing second place, and cells in yellow representing third place.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td>Car</td>
          <td>Chasin</td>
          <td>Circui</td>
          <td>Fall</td>
          <td>Fightin</td>
          <td>Jumping</td>
          <td>Monocyc</td>
          <td>Push</td>
          <td>Robbery</td>
          <td>Runnin</td>
          <td>Skateboard</td>
          <td>Stoop</td>
          <td>ThrowingObj</td>
          <td>Vaudevill</td>
          <td>Vehicle</td>
          <td></td>
      </tr>
      <tr>
          <td>RTFM Tian et al. (2021)</td>
          <td>99.70</td>
          <td>95.41</td>
          <td>99.83</td>
          <td>70.19</td>
          <td>97.36</td>
          <td>89.14</td>
          <td>37.9</td>
          <td>35.28</td>
          <td>67.01</td>
          <td>90.59</td>
          <td>96.81</td>
          <td>64.1</td>
          <td>97.93</td>
          <td>91.75</td>
          <td>90.85</td>
          <td>81.60</td>
      </tr>
      <tr>
          <td>S3R Wu et al. (2022)</td>
          <td>98.71</td>
          <td>96.80</td>
          <td>99.97</td>
          <td>85.63</td>
          <td>95.93</td>
          <td>69.33</td>
          <td>96.82</td>
          <td>54.76</td>
          <td>61.19</td>
          <td>94.43</td>
          <td>96.92</td>
          <td>75.46</td>
          <td>97.63</td>
          <td>97.78</td>
          <td>96.84</td>
          <td>87.8</td>
      </tr>
      <tr>
          <td>SSRL Li et al. (2022a</td>
          <td>99.35</td>
          <td>97.31</td>
          <td>99.95</td>
          <td>91.24</td>
          <td>96.88</td>
          <td>93.07</td>
          <td>89.7</td>
          <td>90.62</td>
          <td>91.81</td>
          <td>94.47</td>
          <td>97.73</td>
          <td>71.8</td>
          <td>98.44</td>
          <td>96.32</td>
          <td>95.49</td>
          <td>93.6</td>
      </tr>
      <tr>
          <td>CLIP zero-shot Radford et al. (2021)</td>
          <td>61.65</td>
          <td>77.88</td>
          <td>5.95</td>
          <td>61.73</td>
          <td>79.37</td>
          <td>23.68</td>
          <td>77.7</td>
          <td>63.36</td>
          <td>37.71</td>
          <td>54.39</td>
          <td>76.15</td>
          <td>8.47</td>
          <td>44.10</td>
          <td>65.97</td>
          <td>27.08</td>
          <td>51.02</td>
      </tr>
      <tr>
          <td>ActionCLIP Wang et al. (2021)</td>
          <td>98.50</td>
          <td>93.86</td>
          <td>98.59</td>
          <td>16.38</td>
          <td>97.45</td>
          <td>89.63</td>
          <td>98.0</td>
          <td>8.14</td>
          <td>67.36</td>
          <td>78.25</td>
          <td>97.10</td>
          <td>0.76</td>
          <td>97.70</td>
          <td>98.65</td>
          <td>93.97</td>
          <td>75.63</td>
      </tr>
      <tr>
          <td>AnomalyCLIP</td>
          <td>98.08</td>
          <td>96.66</td>
          <td>97.97</td>
          <td>96.69</td>
          <td>98.03</td>
          <td>95.48</td>
          <td>86.8</td>
          <td>97.99</td>
          <td>95.00</td>
          <td>97.95</td>
          <td>97.29</td>
          <td>98.62</td>
          <td>96.50</td>
          <td>96.97</td>
          <td>96.79</td>
          <td>96.46</td>
      </tr>
  </tbody>
</table>
<p>Table 6: Results of the state-of-the-art methods and our AnomalyCLIP in terms of VAR on XD-Violence. The table highlights the top performers, with cells highlighted in red representing first place, cells in orange representing second place, and cells in yellow representing third place.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>Class</th>
          <th>mAP</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td>Abuse</td>
          <td>CarAccident</td>
          <td>Explosion</td>
          <td>Fighting</td>
          <td>Riot</td>
          <td>Shooting</td>
          <td></td>
      </tr>
      <tr>
          <td>RTFM Tian et al. (2021)</td>
          <td>9.25</td>
          <td>25.36</td>
          <td>53.53</td>
          <td>61.73</td>
          <td>90.38</td>
          <td>18.01</td>
          <td>43.04</td>
      </tr>
      <tr>
          <td>S3R Wu et al. (2022)</td>
          <td>2.63</td>
          <td>23.82</td>
          <td>45.29</td>
          <td>49.88</td>
          <td>90.41</td>
          <td>4.34</td>
          <td>36.06</td>
      </tr>
      <tr>
          <td>CLIP zero-shot Radford et al. (2021)</td>
          <td>0.32</td>
          <td>12.21</td>
          <td>22.26</td>
          <td>25.25</td>
          <td>66.60</td>
          <td>1.26</td>
          <td>21.32</td>
      </tr>
      <tr>
          <td>ActionCLIP Wang et al. (2021)</td>
          <td>2.73</td>
          <td>25.15</td>
          <td>55.28</td>
          <td>58.09</td>
          <td>87.31</td>
          <td>12.87</td>
          <td>40.24</td>
      </tr>
      <tr>
          <td>AnomalyCLIP</td>
          <td>6.10</td>
          <td>31.31</td>
          <td>68.75</td>
          <td>71.44</td>
          <td>92.74</td>
          <td>26.13</td>
          <td>49.41</td>
      </tr>
  </tbody>
</table>
<p>SSRL (Li et al. , 2022a) on VAD, our method obtains an improvement of +1 . 78% in terms of mAUC on VAR, while being slightly worse with −1 . 07% in terms of AUC ROC on VAD.</p>
<p>Table 3 shows the results on XD-Violence (Wu et al. , 2020). AnomalyCLIP outperforms other state-of-the-art methods on VAR achieving the highest mAP. Compared to the VAD baselines&rsquo; models, AnomalyCLIP outperforms RTFM (Tian et al. , 2021) and demonstrates performance close to S3R (Wu et al. , 2022). Please refer to Appendix C for further details on how we obtain results on XD-Violence.</p>
<p>Tables 4 , 5, and 6 display the multi-class AUC and AP for each individual abnormal class. The proposed method has a clear advantage when applied to the UCF-Crime and XDViolence datasets, which are generally considered to be complex benchmarks in anomaly detection. Our method achieves the best mAUC and mAP on average, while it is less advantageous when dealing with anomalies that exhibit slight deviations from normal patterns, such as Shoplifting in UCF-Crime. The advantage of our proposed method is less noticeable when applied to the ShanghaiTech dataset, which captures simple scenes where most methods have achieved a saturated performance.</p>
<p>Fig. 3 presents the qualitative results of our proposed AnomalyCLIP in detecting and recognising anomalies within a set of UCF-Crime, ShanghaiTech, XD-Violence test videos. The model is capable of predicting both the presence of anomalies in test videos and the category of the anomalous event. In video Normal Video 246 from UCF-Crime (Row 2, Column 2), it can be seen how some frames have a higher-than-expected proba- bility of being abnormal. It is interesting to note how in the video RoadAccidents133 from UCF-Crime (Row 1, Column 2) the anomaly score remains high even in the aftermath of the accident. It is also interesting to note that for Normal videos, AnomalyCLIP is able to obtain a relatively low anomaly probability all over the frames, meaning our model has learnt a robust normal representation among Normal videos. Please refer to Appendix E for more results on the test videos. Furthermore, for a more intuitive understanding of the results presented in the paper, we invite readers to access the website <a
  href="https://lucazanella-dvl.github.io/AnomalyCLIP"
    target="_blank"
  >https://lucazanella-dvl.github.io/AnomalyCLIP</a>, where easily accessible qualitative results are available.</p>

<h2 class="relative group">4.3. Ablation
    <div id="43-ablation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#43-ablation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this section, we perform ablations of our method to validate our main design choices with UCF-Crime: the way in which we represent and learn directions, the transformations applied to the CLIP space and the employed way for estimating the likelihood of anomaly, the choice of architecture for the Temporal model, training objectives, and the impact of using features extracted from different backbones.</p>
<p>Representation and Learning of the Directions. In the ablation shown in Table 7, we evaluate the choice of the CoOp (Zhou et al. , 2022) framework to learn directions in the CLIP space. When CoOp is removed, we directly learn the directions from randomly initialized points in the CLIP space (Row 1) or make use of fixed engineered prompts of the form &ldquo;a video from a CCTV camera of a {class} &quot; (Row 2). Both choices result in degradation of the results, indicating that text-</p>
<p>Fig. 3: Qualitative results for VAR on four test videos from UCF-Crime (the top two rows), two test videos from ShanghaiTech (the third row), and two test videos from XD-Violence (the bottom row). For each video, we show at the bottom the predicted probability of each frame being anomalous by our model over the number of frames. We showcase some key frames to reflect the relevance between the predicted anomaly probability and the visual content. The red shaded areas denote the temporal ground-truth of anomalies. We also indicate the predicted anomalous class for detected abnormal frames in the red boxes, while videos without detected anomalies are indicated with blue boxes as Normal.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_e04912c9ffc68ac48237f40c61a5ee01280f885c2ce9e67985dbd0a9ae6104e9.png"
    ></figure>
<p>Table 7: Ablation on representation and learning of the directions of abnormality. &lsquo;Finetuning&rsquo; indicates that the last projection layer is fine-tuned. The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>Text encoder</th>
          <th>Directions</th>
          <th>AUC</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>No</td>
          <td>Direct Optimisation</td>
          <td>84.98</td>
          <td>69.86</td>
      </tr>
      <tr>
          <td>Frozen</td>
          <td>Engineered Prompts</td>
          <td>84.66</td>
          <td>81.35</td>
      </tr>
      <tr>
          <td>Frozen</td>
          <td>CoOp</td>
          <td>85.88</td>
          <td>87.39</td>
      </tr>
      <tr>
          <td>Finetuning</td>
          <td>CoOp</td>
          <td>86.36</td>
          <td>90.66</td>
      </tr>
  </tbody>
</table>
<p>Table 8: Comparisons of different architectural choices for the CoOp module. &lsquo;Shared&rsquo; means that all the classes share a unified context, otherwise each class has a specific context. The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>Context vectors</th>
          <th>Shared</th>
          <th>AUC</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>4</td>
          <td></td>
          <td>86.16</td>
          <td>91.05</td>
      </tr>
      <tr>
          <td>8</td>
          <td></td>
          <td>86.36</td>
          <td>90.66</td>
      </tr>
      <tr>
          <td>16</td>
          <td></td>
          <td>85.82</td>
          <td>90.65</td>
      </tr>
      <tr>
          <td>8</td>
          <td>✓</td>
          <td>85.97</td>
          <td>90.01</td>
      </tr>
  </tbody>
</table>
<p>guided initialization of the directions and directions finetuning are both necessary. Furthermore, we show that unfreezing the last projection of the text encoder (Row 4) enables a greater freedom in finetuning the discovered directions, yielding the best results.</p>
<p>In the ablation shown in Table 8, we evaluate the architectural choices on the CoOp module to learn directions in the CLIP space. Specifically, we experimented by varying the number of context vectors t ctx used from 4 to 8 to 16, and using shared or class-specific context vectors. Although using 4 context vectors results in a slightly higher mAUC score, we eventually opted to use 8 context vectors because they produce a higher AUC score. Results (Row 2 and 4) show that learning a specific set of context vectors for each class, is more tailored to fine-grained categories, rather than relying on more generic shared context vectors for all classes.</p>
<p>Likelihood Estimation and CLIP Latent Space Transformation. The way in which the extracted CLIP features are transformed and the chosen likelihood estimation method play a crucial role in the quality of segment selection. We evaluate several choices in this procedure in Table 9. Directly using the CLIP space and cosine similarities with the learned directions as likelihood estimators (Row 1) produces the worst VAR results, indicating that the use of the normality prototype m is of high importance in the context of anomaly detection. Second, Row 2 shows that MIL segment selection as a function of the feature magnitude without accounting for the direction is not as effective, given that the large magnitude could be attributed to irrelevant factors.</p>
<p>Temporal Model Architecture. Capturing temporal information is an essential aspect of VAR since it provides insights into the behaviour of objects and scenes over time. Table 10 shows results for different architectures of T i.e. a 3-layer MLP, two Transformer Encoders (Vaswani et al. , 2017), the multi-scale temporal network (MTN), designed in RTFM and used in S3R and SSRL, and the employed Axial Transformer. In particu-</p>
<p>Table 9: Ablation of different likelihood estimation methods, feature space transformations and MIL selection. &lsquo;Features&rsquo; indicates the transformation applied to CLIP features. The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>Likelihood</th>
          <th>Features</th>
          <th>MIL Selection</th>
          <th>AUC</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>cosine sim.</td>
          <td>CLIP</td>
          <td>cosine sim.</td>
          <td>85.59</td>
          <td>83.69</td>
      </tr>
      <tr>
          <td>S</td>
          <td>CLIP - m</td>
          <td>feature magnitude</td>
          <td>84.92</td>
          <td>89.82</td>
      </tr>
      <tr>
          <td>S</td>
          <td>CLIP - m</td>
          <td>S</td>
          <td>86.36</td>
          <td>90.66</td>
      </tr>
  </tbody>
</table>
<p>Table 10: Comparisons of different architectural choices for the Temporal model. The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>Temporal Model</th>
          <th>Short-term</th>
          <th>Long-term</th>
          <th>AUC</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>MLP</td>
          <td></td>
          <td></td>
          <td>74.86</td>
          <td>84.46</td>
      </tr>
      <tr>
          <td>Transformer</td>
          <td>✓</td>
          <td></td>
          <td>84.69</td>
          <td>88.38</td>
      </tr>
      <tr>
          <td>Transformer</td>
          <td></td>
          <td>✓</td>
          <td>85.1</td>
          <td>89.29</td>
      </tr>
      <tr>
          <td>MTN</td>
          <td></td>
          <td>✓</td>
          <td>82.71</td>
          <td>87.65</td>
      </tr>
      <tr>
          <td>Axial Transformer</td>
          <td>✓</td>
          <td>✓</td>
          <td>86.36</td>
          <td>90.66</td>
      </tr>
  </tbody>
</table>
<p>lar, one transformer encoder (Row 2) performs self-attention on each independent 16-frame segment, solely modelling shortterm dependencies. The other (Row 3) applies self-attention on segment embeddings, which are obtained by averaging 16frame feature embeddings within each segment, thereby only modelling long-term dependencies. To ensure a fair comparison, both transformers are designed to have a capacity similar to that of the Axial Transformer. The reduced performance of the MLP baseline (Row 1) indicates the necessity of considering temporal information which is not readily available in the extracted CLIP features. The Axial transformer can capture temporal dependencies and outperform the compared architectures.</p>
<p>Table 11 shows the results for different values of the embedding size and the number of layers. In the final architecture we use 1 layer and an embedding size of 256, for a total of 10.4 M trainable parameters.</p>
<p>Losses. Table 12 illustrates the contribution of the losses on the Selector model&rsquo;s outputs, where we progressively remove the losses from the full training objective. The loss on abnormal videos contributes to improved VAD and VAR results on UCFCrime. Similarly, using the loss on normal videos improves the results on Shanghaitech and XD-Violence, as can be seen in Tables D.16 and D.17 of Appendix D .</p>
<p>Table 13 similarly shows the contribution of the losses on the aggregated model&rsquo;s output, where we remove each from the complete training objective. We validate that each of the proposed losses promotes performance on both the VAD and VAR tasks.</p>
<p>The bottom-K least anomalous segments V − = {S − 1
, &hellip;, S − K } of anomalous videos proved to be beneficial for learning the Temporal Model. Inspired by this, we analyse the impact of incorporating this set of frames into the Selector Model loss by minimising the loss:</p>
<!-- formula-not-decoded -->
<p>Table 11: Comparisons of different architectural choices for the Axial Transformer. The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>Embedding size</th>
          <th>Number of layers</th>
          <th>AUC</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>64</td>
          <td>1</td>
          <td>82.83</td>
          <td>90.1</td>
      </tr>
      <tr>
          <td>128</td>
          <td>1</td>
          <td>84.97</td>
          <td>90.53</td>
      </tr>
      <tr>
          <td>256</td>
          <td>1</td>
          <td>86.36</td>
          <td>90.66</td>
      </tr>
      <tr>
          <td>512</td>
          <td>1</td>
          <td>85.51</td>
          <td>89.28</td>
      </tr>
      <tr>
          <td>256</td>
          <td>2</td>
          <td>85.89</td>
          <td>89.67</td>
      </tr>
      <tr>
          <td>256</td>
          <td>3</td>
          <td>85.15</td>
          <td>88.14</td>
      </tr>
  </tbody>
</table>
<p>Table 12: Ablation of the losses on the Selector model. The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>LDIR A</th>
          <th>LDIR N</th>
          <th>AUC</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td>✓</td>
          <td>85.89</td>
          <td>89.34</td>
      </tr>
      <tr>
          <td></td>
          <td>✓</td>
          <td>85.91</td>
          <td>87.26</td>
      </tr>
      <tr>
          <td>✓</td>
          <td></td>
          <td>86.46</td>
          <td>90.75</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>86.36</td>
          <td>90.66</td>
      </tr>
  </tbody>
</table>
<p>Moreover, instead of using all segments of normal videos in the Selector Model loss, we evaluate the impact of using only the top-K most abnormal segments V + = {S + 1
, &hellip;, S + K } by minimising the likelihood predicted by the Selector Model:</p>
<!-- formula-not-decoded -->
<p>In Table 14, we present our findings, which indicate that modifying the loss function in either of two ways cause a degradation of performance. Specifically, our experiments (Row 1) demonstrate that using the bottom-K least abnormal segments is only effective when learning the Temporal Model. This is because if there is no clear separation between the bottom-K and top-K abnormal features, the Selector Model can lead to incorrectly selected bottom-K features that prevent it from learning good directions in the feature space. However, incorporating the bottom-K least abnormal segments becomes beneficial in the Temporal Model, which has a greater capacity. Furthermore, our experiments indicate that using all normal segments (Row 3) provides a more robust estimation of the direction from normal to anomalous compared to using only the top-K most abnormal segments (Row 2).</p>
<p>Feature Representation. The purpose of this ablation study is to determine the most suitable feature space for the proposed method AnomalyCLIP. To achieve this, we first investigate whether the space learned by the Selector Model can be applied to the Temporal Model. This C-dimensional space is formed by projecting each frame feature x onto every d c direction, where C represents the number of anomalous classes. Our results, presented in Table 15, indicate that using only this space leads to sub-optimal model performance (Row 3). This finding highlights the necessity of incorporating the information contained in the original feature space as well. We also experiment with using I3D features for both the Selector Model and the Temporal Model (Row 1), but the results demonstrate that the model using these features performs worse. We attribute this to the</p>
<p>Table 13: Ablation of losses on the aggregated outputs. The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>LA+</th>
          <th>LA−</th>
          <th>LN+</th>
          <th>AUC</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td>✓</td>
          <td>✓</td>
          <td>45.23</td>
          <td>69.57</td>
      </tr>
      <tr>
          <td>✓</td>
          <td></td>
          <td>✓</td>
          <td>84.5</td>
          <td>90.88</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td>80.96</td>
          <td>86.1</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
          <td>86.36</td>
          <td>90.66</td>
      </tr>
  </tbody>
</table>
<p>Table 14: Ablation on the variation of Selector model losses. The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>LDIR A</th>
          <th>LDIR N</th>
          <th>LDIR N+</th>
          <th>LDIR A−</th>
          <th>AUC</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td>✓</td>
          <td>86.41</td>
          <td>88.29</td>
      </tr>
      <tr>
          <td>✓</td>
          <td></td>
          <td>✓</td>
          <td></td>
          <td>86.17</td>
          <td>90.53</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td></td>
          <td>86.36</td>
          <td>90.66</td>
      </tr>
  </tbody>
</table>
<p>fact that I3D features are mapped to a region of space that is not aligned with the text features, unlike the features generated by CLIP&rsquo;s image encoder. For this reason, we also experimented using I3D features for the Temporal Model and features from CLIP&rsquo;s image encoder for the Selector Model (Row 2). The result of this experiment further emphasises that the latent space of CLIP is a more semantic space in which anomalous events of different classes are more separated, which in turn leads to superior discriminative ability in detecting and recognising anomalous events.</p>

<h2 class="relative group">5. Conclusions
    <div id="5-conclusions" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#5-conclusions" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this work, we addressed the challenging task of Video Anomaly Recognition that extends the scope of Video Anomaly Detection by further requiring the classification of the anomalous activities. We proposed AnomalyCLIP, the first method that leverages LLV models in the context of VAR. Our work shed light on the fact that a naive application of existing LLV models (Radford et al. , 2021; Wang et al. , 2021) to VAR leads to unsatisfactory performance and we demonstrated that several technical design choices are required to build a multimodal deep network for detecting and classifying abnormal behaviours. We also performed an extensive experimental evaluation showing that AnomalyCLIP achieves state-or-the-art VAR results on the benchmark ShanghaiTech (Liu et al. , 2018), UCFCrime (Sultani et al. , 2018), and XD-Violence (Wu et al. , 2020) datasets. As future work, we plan to extend our method in open-set scenarios to reflect the real-world applications where anomalies are often not pre-defined. We will also investigate the applicability of our method in other multi-modal tasks, e.g., fine-grained classification.</p>

<h2 class="relative group">References
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>Bai, S., He, Z., Lei, Y., Wu, W., Zhu, C., Sun, M., Yan, J., 2019. Traffic anomaly detection via perspective map based on spatial-temporal information matrix, in: CVPR Workshops.</li>
<li>Bao, Q., Liu, F., Liu, Y., Jiao, L., Liu, X., Li, L., 2022. Hierarchical scene normality-binding modeling for anomaly detection in surveillance videos, in: ACM Multimedia.</li>
<li>Carreira, J., Zisserman, A., 2017. Quo vadis, action recognition? a new model and the kinetics dataset, in: CVPR.</li>
</ul>
<p>Table 15: Comparisons of different features. The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>Selector Model</th>
          <th>Temporal Model</th>
          <th>AUC</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>I3D-RGB</td>
          <td>I3D-RGB</td>
          <td>65.05</td>
          <td>84.24</td>
      </tr>
      <tr>
          <td>ViT-B/16</td>
          <td>I3D-RGB</td>
          <td>78.11</td>
          <td>88.26</td>
      </tr>
      <tr>
          <td>ViT-B/16</td>
          <td>S(x)</td>
          <td>84.44</td>
          <td>86.78</td>
      </tr>
      <tr>
          <td>ViT-B/16</td>
          <td>ViT-B/16</td>
          <td>86.36</td>
          <td>90.66</td>
      </tr>
  </tbody>
</table>
<ul>
<li>Chen, Y., Liu, Z., Zhang, B., Fok, W., Qi, X., Wu, Y.C., 2022. Mgfn: Magnitude-contrastive glance-and-focus network for weakly-supervised video anomaly detection. arXiv .</li>
<li>Feng, J.C., Hong, F.T., Zheng, W.S., 2021a. Mist: Multiple instance selftraining framework for video anomaly detection, in: CVPR.</li>
<li>Feng, X., Song, D., Chen, Y., Chen, Z., Ni, J., Chen, H., 2021b. Convolutional transformer based dual discriminator generative adversarial networks for video anomaly detection, in: ACM Multimedia.</li>
<li>Ho, J., Kalchbrenner, N., Weissenborn, D., Salimans, T., 2019. Axial attention in multidimensional transformers. arXiv .</li>
<li>Ioffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift, in: ICML.</li>
<li>Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T., 2021. Scaling up visual and vision-language representation learning with noisy text supervision, in: ICML.</li>
<li>Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al., 2017. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 .</li>
<li>Li, G., Cai, G., Zeng, X., Zhao, R., 2022a. Scale-aware spatio-temporal relation learning for video anomaly detection, in: ECCV, Springer.</li>
<li>Li, S., Liu, F., Jiao, L., 2022b. Self-training multi-sequence learning with transformer for weakly supervised video anomaly detection, in: AAAI.</li>
<li>Liu, W., Luo, W., Lian, D., Gao, S., 2018. Future frame prediction for anomaly detection–a new baseline, in: CVPR.</li>
<li>Liu, Z., Nie, Y., Long, C., Zhang, Q., Li, G., 2021. A hybrid video anomaly detection framework via memory-augmented flow reconstruction and flowguided frame prediction, in: ICCV.</li>
<li>Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H., 2022. Video swin transformer, in: CVPR.</li>
<li>Loshchilov, I., Hutter, F., 2019. Decoupled weight decay regularization, in: ICLR.</li>
<li>Lv, H., Chen, C., Cui, Z., Xu, C., Li, Y., Yang, J., 2021. Learning normal dynamics in videos with meta prototype network, in: CVPR.</li>
<li>Mei, T., Zhang, C., 2017. Deep learning for intelligent video analysis, in: ACM Multimedia.</li>
<li>Narasimhan, M.G., 2018. Dynamic video anomaly detection and localization using sparse denoising autoencoders. Multimedia Tools and Applications .</li>
<li>Nayak, R., Pati, U.C., Das, S.K., 2021. A comprehensive review on deep learning-based methods for video anomaly detection. Image and Vision Computing 106.</li>
<li>Park, H., Noh, J., Ham, B., 2020. Learning memory-guided normality for anomaly detection, in: CVPR.</li>
<li>Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al., 2021. Learning transferable visual models from natural language supervision, in: ICML.</li>
<li>Roth, K., Pemula, L., Zepeda, J., Scholkopf, B., Brox, T., Gehler, P., 2022. ¨ ¨ Towards total recall in industrial anomaly detection, in: CVPR.</li>
<li>Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al., 2022. Laion5b: An open large-scale dataset for training next generation image-text models. arXiv .</li>
<li>Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., Komatsuzaki, A., 2021. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv .</li>
<li>Singh, A., Hu, R., Goswami, V., Couairon, G., Galuba, W., Rohrbach, M., Kiela, D., 2022. Flava: A foundational language and vision alignment model, in: CVPR.</li>
<li>Sohrab, F., Raitoharju, J., Gabbouj, M., Iosifidis, A., 2018. Subspace support vector data description, in: ICPR.</li>
<li>Suarez, J.J.P., Naval Jr, P.C., 2020. A survey on deep learning techniques for video anomaly detection. arXiv .</li>
<li>Sultani, W., Chen, C., Shah, M., 2018. Real-world anomaly detection in surveillance videos, in: CVPR.</li>
<li>Sun, C., Jia, Y., Wu, Y., 2022. Evidential reasoning for video anomaly detection, in: ACM Multimedia.</li>
<li>Tian, Y., Pang, G., Chen, Y., Singh, R., Verjans, J.W., Carneiro, G., 2021. Weakly-supervised video anomaly detection with robust temporal feature magnitude learning, in: ICCV.</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need. NeurIPS .</li>
<li>Wang, G., Yuan, X., Zheng, A., Hsu, H.M., Hwang, J.N., 2019. Anomaly candidate identification and starting time estimation of vehicles from traffic videos, in: CVPR workshops.</li>
<li>Wang, J., Cherian, A., 2019. Gods: Generalized one-class discriminative subspaces for anomaly detection, in: ICCV.</li>
<li>Wang, M., Xing, J., Liu, Y., 2021. Actionclip: A new paradigm for video action recognition. arXiv .</li>
<li>Wang, Z., Zou, Y., Zhang, Z., 2020. Cluster attention contrast for video anomaly detection, in: ACM Multimedia.</li>
<li>Wu, J.C., Hsieh, H.Y., Chen, D.J., Fuh, C.S., Liu, T.L., 2022. Self-supervised sparse representation for video anomaly detection, in: ECCV.</li>
<li>Wu, P., Liu, J., 2021. Learning causal temporal relation and feature discrimination for anomaly detection. IEEE Transactions on Image Processing .</li>
<li>Wu, P., Liu, J., Shi, Y., Sun, Y., Shao, F., Wu, Z., Yang, Z., 2020. Not only look, but also listen: Learning multimodal violence detection under weak supervision, in: ECCV.</li>
<li>Xu, H., Ghosh, G., Huang, P.Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., Feichtenhofer, C., 2021. Videoclip: Contrastive pretraining for zero-shot video-text understanding, in: EMNLP.</li>
<li>Xu, K., Sun, T., Jiang, X., 2019. Video anomaly detection and localization based on an adaptive intra-frame classification network. IEEE Transactions on Multimedia .</li>
<li>Zaheer, M.Z., Mahmood, A., Astrid, M., Lee, S.I., 2020. Claws: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection, in: ECCV.</li>
<li>Zaheer, M.Z., Mahmood, A., Khan, M.H., Segu, M., Yu, F., Lee, S.I., 2022. Generative cooperative learning for unsupervised video anomaly detection, in: CVPR.</li>
<li>Zhang, J., Qing, L., Miao, J., 2019. Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection, in: ICIP.</li>
<li>Zhong, J.X., Li, N., Kong, W., Liu, S., Li, T.H., Li, G., 2019. Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection, in: CVPR.</li>
<li>Zhou, K., Yang, J., Loy, C.C., Liu, Z., 2022. Learning to prompt for visionlanguage models. International Journal of Computer Vision .</li>
</ul>
<p>In this appendix, we provide further details on the implementation and training of the proposed AnomalyCLIP. We also provide more details on how we obtain XD-Violence results. Furthermore, we report supplementary results of the ablation performed on the loss of the Selector model to support our design choices. Lastly, we offer additional qualitative results.</p>

<h2 class="relative group">Appendix A. Implementation Details
    <div id="appendix-a-implementation-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#appendix-a-implementation-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Similarly to CoOp (Zhou et al. , 2022), context vectors t ctx are randomly initialised by drawing from a zero-mean Gaussian distribution with standard deviation equal to 0 . 02. We use the CLIP image encoder (Radford et al. , 2021), specifically the ViT-B/16 implementation, without fine-tuning, and apply standard CLIP image augmentations to each frame. As supported by the ablation in Table 11, we employ a one-layer axial transformer (Ho et al. , 2019) for the Temporal Model with an embedding size of 256 for UCF-Crime (Sultani et al. , 2018) and 128 for XD-Violence (Wu et al. , 2020), and a two-layer axial transformer with an embedding size of 256 for ShanghaiTech (Liu et al. , 2018). In the case of UCF-Crime and XD-Violence, we use the image features of the CLIP space as input to the Temporal Model. However, for ShanghaiTech, we observe an improvement in performance by incorporating the output of the Selector Model as an additional input to the Temporal Model. This is likely because ShanghaiTech is less challenging than the other two and, as a result, the Selector Model already provides sufficient discriminative features.</p>
<p>Consistent with previous work on VAD Tian et al. (2021); Wu et al. (2022); Chen et al. (2022); Li et al. (2022a), we incorporate a random masking strategy during the selection process operated by S. Specifically, we randomly mask 70% of the segments to prevent the model from repeatedly selecting the same segments. This approach ensures a more diverse and representative selection of segments, thus improving the overall performance.</p>

<h2 class="relative group">Appendix B. Training Details
    <div id="appendix-b-training-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#appendix-b-training-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Training was performed using the AdamW optimiser (Loshchilov and Hutter , 2019) with parameters β1 = 0 . 9, β2 = 0 . 98, ϵ = 10 − 8 and weight decay w = 0 . 2. We tuned the learning rate and the number of epochs based on the behaviour of the training loss. Specifically, the learning rate is set to 5 × 10 − 4 , 10 − 5 and 5 × 10 − 6 for ShanghaiTech, UCF-Crime, and XD-Violence, respectively, warmed up for 10% of the total training epochs and decayed to zero following a cosine annealing schedule. The number of epochs is set to 50 for UCF-Crime and XD-Violence, while it is set to 100 for ShanghaiTech, due to its smaller size. We set the weight for each loss term to 1 without tuning. Following previous work (Tian et al. , 2021; Wu et al. , 2022; Li et al. , 2022a), we use λ1 = 8 × 10 − 3 ad λ2 = 8 × 10 − 4 for sparsity and smoothness regularisation terms, respectively.</p>
<p>Table D.16: Ablation of the losses on the Selector model on ShanghaiTech (Liu et al. , 2018) The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>LDIR A</th>
          <th>LDIR N</th>
          <th>AUC</th>
          <th>mAUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td>97.86</td>
          <td>95.92</td>
      </tr>
      <tr>
          <td></td>
          <td>✓</td>
          <td>97.35</td>
          <td>96</td>
      </tr>
      <tr>
          <td>✓</td>
          <td></td>
          <td>97.95</td>
          <td>96.35</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>98.07</td>
          <td>96.46</td>
      </tr>
  </tbody>
</table>
<p>Table D.17: Ablation of the losses on the Selector model on XD-Violence (Wu et al. , 2020) The final configuration of our model is represented by the row highlighted in grey in the table.</p>
<table>
  <thead>
      <tr>
          <th>LDIR A</th>
          <th>LDIR N</th>
          <th>AP</th>
          <th>mAP</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td>77.45</td>
          <td>47.74</td>
      </tr>
      <tr>
          <td></td>
          <td>✓</td>
          <td>78.69</td>
          <td>48.03</td>
      </tr>
      <tr>
          <td>✓</td>
          <td></td>
          <td>78.16</td>
          <td>49.02</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>78.51</td>
          <td>49.41</td>
      </tr>
  </tbody>
</table>

<h2 class="relative group">Appendix C. Reproducibility XD-Violence
    <div id="appendix-c-reproducibility-xd-violence" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#appendix-c-reproducibility-xd-violence" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>As the original implementations of RTFM (Tian et al. , 2021) and S3R (Wu et al. , 2022) do not provide neither the code nor the trained models for XD-Violence (Wu et al. , 2020), we made the necessary adaptations to support XD-Violence based on the information available in the original papers and on the opensource platform Github, and used the 2048-D features extracted after the final average pooling layer of the I3D ResNet50 model, pre-trained on Kinetics400 (Kay et al. , 2017). First, we pretrain their models on the entire XD-Violence dataset and save the checkpoint at the training iteration that obtains the highest average precision (AP) on the test set, following their training protocol. Subsequently, we maintain the original pre-trained model frozen and introduce a multiclass classification head. This newly introduced head undergoes training following the methodology outlined in Sec. 4.2 .</p>

<h2 class="relative group">Appendix D. Ablation
    <div id="appendix-d-ablation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#appendix-d-ablation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Losses. Tables D.16 and D.17 illustrate the contribution of the losses on the Selector model&rsquo;s outputs, where we progressively remove the losses from the full training objective, on ShanghaiTech (Liu et al. , 2018) and XD-Violence (Wu et al. , 2020), respectively. Both the losses on anomalous and normal videos contribute to better VAD and VAR results.</p>

<h2 class="relative group">Appendix E. Qualitative Results
    <div id="appendix-e-qualitative-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#appendix-e-qualitative-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Fig. E.4 presents additional qualitative results of our proposed AnomalyCLIP in detecting and recognising anomalies within a set of UCF-Crime and ShanghaiTech test videos. The model is capable of predicting both the presence of anomalies in test videos and the category of the anomalous event. Videos Arson016 (Row 1, Column 1), Arrest001 (Row 1, Column 2) and Burglary033 (Row 2, Column 2) serve as good examples of the effectiveness of the proposed method. The anomalies are temporally located, and the ground-truth labels (as indicated in</p>
<p>Fig. E.4: Qualitative results for VAR on twelve test videos from UCF-Crime (the top three rows), ShanghaiTech (the fourth row) and XD-Violence (the bottom row). For each video, we show at the bottom the predicted probability of each frame being anomalous by our model over the number of frames. We showcase some key frames to reflect the relevance between the predicted anomaly probability and the visual content. The red shaded areas denote the temporal ground-truth of anomalies. We also indicate the predicted anomalous class for detected abnormal frames in the red boxes, while videos without detected anomalies are indicated with blue boxes as Normal.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_2881f022908bbea5380ed3c93caa8c3ce13d858662be3594d711aee71e2f3221.png"
    ></figure>
<p>the video name) are correctly identified. However, it is worth noting that in Arson016 some frames are misjudged as Explosion, which is nevertheless a similar type of anomaly.</p>
<p>One failure case is observed in the sample Shoplifting039 (Row 2, Column 2), where the proposed method fails to detect the anomaly. The reason for this failure could be attributed to the fact that the annotated anomaly is visually very similar to a normal situation, making it difficult even for humans to understand that a shoplifting is taking place and not an authorised person moving an object. This result underscores the challenge of accurately detecting anomalies in complex and visually similar scenarios. In video Robbery102 (Row 2, Column 1), the anomaly is correctly located but wrongly classified as Assault, indicating the challenges of VAR.</p>
<p>Videos Shooting032 (Row 4, Column 2) and Fighting033 (Row 4, Column 1) are interesting examples that highlight the ability of the proposed method to detect anomalous situations even in the aftermath of the anomaly. In these videos, the anomaly probability remains high even after the anomalous situation annotated in the ground truth has ended, correctly indicating that there is still something anomalous happening.</p>
<p>The videos from ShanghaiTech (Row 5, Columns 1-2) also provide insights into the performance of the proposed method. In the video on the left, the anomaly is correctly classified as a vehicle. However, there is also a false alarm, which represents a failure case. On the right side of the last row, the video shows a monocycle anomaly that is wrongly classified as Running. It is reasonable to assume that the fast movement of the person riding the monocycle could have contributed to this misclassification.</p>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/Delving into CLIP latent space for Video Anomaly Recognition.md"
          data-oid-likes="likes_papers/Delving into CLIP latent space for Video Anomaly Recognition.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/exploring-large-vision-language-models-for-robust-and-efficient-industrial-anomaly-detection/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/cross-domain-learning-for-vad-with-limited-supervision/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
