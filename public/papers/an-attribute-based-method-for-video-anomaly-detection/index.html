<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/an-attribute-based-method-for-video-anomaly-detection/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/an-attribute-based-method-for-video-anomaly-detection/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/an-attribute-based-method-for-video-anomaly-detection\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "9506"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>9506 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">45 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">An Attribute-based Method for Video Anomaly Detection
    <div id="an-attribute-based-method-for-video-anomaly-detection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#an-attribute-based-method-for-video-anomaly-detection" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">Tal Reiss
    <div id="tal-reiss" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#tal-reiss" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel</p>

<h2 class="relative group">Yedid Hoshen
    <div id="yedid-hoshen" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#yedid-hoshen" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel</p>
<p>Reviewed on OpenReview: https: // openreview. net/ forum? id= XL1N6iLr0G</p>

<h2 class="relative group">Abstract
    <div id="abstract" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#abstract" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video anomaly detection (VAD) identifies suspicious events in videos, which is critical for crime prevention and homeland security. In this paper, we propose a simple but highly effective VAD method that relies on attribute-based representations. The base version of our method represents every object by its velocity and pose, and computes anomaly scores by density estimation. Surprisingly, this simple representation is sufficient to achieve state-of-the-art performance in ShanghaiTech, the most commonly used VAD dataset. Combining our attribute-based representations with an off-the-shelf, pretrained deep representation yields state-of-the-art performance with a 99 . 1% , 93 . 7%, and 85 . 9% AUROC on Ped2, Avenue, and ShanghaiTech, respectively. Our code is available at <a
  href="https://github.com/talreiss/Accurate-Interpretable-VAD"
    target="_blank"
  >https://github.com/talreiss/Accurate-Interpretable-VAD</a> .</p>

<h2 class="relative group">1 Introduction
    <div id="1-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video anomaly detection (VAD) aims to discover interesting but rare events in video. The task has attracted much interest as it is critical for crime prevention and homeland security. One-class classification (OCC) is one of the most popular VAD settings, where the training set consists of normal videos only, while at test time the trained model needs to distinguish between normal and anomalous events. The key challenge for learning-based VAD is that classifying an event as normal or anomalous depends on the human operator&rsquo;s particular definition of normality. Differently from supervised learning, there are no training examples of anomalous events, this essentially requires the learning algorithms to have strong priors.</p>
<p>Many previous VAD methods use a combination of deep networks with self-supervised objectives. A popular line of work consists of training a neural network to predict the next frame and classifying it as anomalous if the predicted and observed frames differ significantly. While such methods can achieve good performance, they do not make their priors explicit i.e., it is unclear why they consider some frames more anomalous than others, making them difficult to debug and improve. More recent methods involve a preliminary object extraction stage from video frames, implying that objects are significant for VAD. However, they typically use object-level self-supervised approaches that do not make their priors explicit. Our hypothesis is that making priors more explicit will improve VAD performance.</p>
<p>In this paper, we propose a new approach that directly represents each video frame by simple attributes that are semantically meaningful to humans. Our method extracts objects from every frame, and represents each object by two attributes: its velocity and body pose (in the case the object is human). These attributes are well known to be important for VAD (Markovitz et al., 2020; Georgescu et al., 2021a). We detect anomalous values of these representations by using density estimation. Concretely, our method classifies a frame as anomalous if it contains one or more objects that have unusual values of velocity and/or pose (see Fig. 1). Our simple velocity and pose representations achieve state-of-the-art performance (85.9% AUROC) on the most popular VAD dataset, ShanghaiTech.</p>
<p><a
  href="mailto:tal.reiss@mail.huji.ac.il">tal.reiss@mail.huji.ac.il</a> <a
  href="mailto:yedid.hoshen@mail.huji.ac.il">yedid.hoshen@mail.huji.ac.il</a></p>
<p>Figure 1: The Avenue and ShanghaiTech datasets. We present the most normal and anomalous frames for each feature. For anomalous frames, we visualize the bounding box of the object with the highest anomaly score. Best viewed in color.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_b9348d4b962ee40351bb74d89ad83666f029dde9a73ca5a9e06afa23e171d919.png"
    ></figure>
<p>While the velocity and pose representations are highly effective, they ignore other attributes, most importantly object category. As an example, if we never see a lion in normal videos, a test video containing a lion is anomalous. To represent these residual attributes, we model them with an off-the-shelf, deep representation (here, we use CLIP features). Our final method combines velocity, pose and the deep representations. It achieves state-of-the-art performance on the three most commonly reported datasets.</p>

<h2 class="relative group">2 Related Work
    <div id="2-related-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-related-work" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Classical video anomaly detection methods were typically composed of two steps: handcrafted feature extraction and anomaly scoring. Some of the manual features that were extracted were: optical flow histograms (Chaudhry et al., 2009; Colque et al., 2016; Perš et al., 2010) and SIFT (Lowe, 2004). Commonly used scoring methods include: density estimation (Eskin et al., 2002; Glodek et al., 2013; Latecki et al., 2007), reconstruction (Jolliffe, 2011), and one-class classification (Scholkopf et al., 2000).</p>
<p>In recent years, deep learning has gained in popularity as an alternative to these early works. The majority of video anomaly detection methods utilize at least one of three paradigms: reconstruction-based, predictionbased, skeletal-based, or auxiliary classification-based methods.</p>
<p>Reconstruction &amp; prediction based methods. In the reconstruction paradigm, the normal training data is typically characterized by an autoencoder, which is then used to reconstruct input video clips. The assumption is that a model trained solely on normal training clips will not be able to reconstruct anomalous frames. This assumption does not always hold true, as neural networks can often generalize to some extent out-of-distribution. Notable works are (Nguyen &amp; Meunier, 2019; Chang et al., 2020; Hasan et al., 2016b; Luo et al., 2017b; Yu et al., 2020; Park et al., 2020).</p>
<p>Prediction-based methods learn to predict frames or flow maps in video clips, including inpainting intermediate frames, predicting future frames, and predicting human trajectories (Liu et al., 2018a; Feng et al., 2021b; Chen et al., 2020; Lee et al., 2019; Lu et al., 2019; Park et al., 2020; Wang et al., 2021; Feng et al., 2021a; Yu et al., 2020). Additionally, some works take a hybrid approach combining the two paradigms (Liu et al., 2021b; Zhao et al., 2017; Ye et al., 2019; Tang et al., 2020; Morais et al., 2019). As these methods are</p>
<p>trained to optimize both objectives, input frames with large reconstruction or prediction errors are considered anomalous.</p>
<p>Self-supervised auxiliary tasks. There has been a great deal of research on learning from unlabeled data. A common approach is to train neural networks on suitably designed auxiliary tasks with automatically generated labels. Tasks include: video frame prediction (Mathieu et al., 2016), image colorization (Zhang et al., 2016; Larsson et al., 2016), puzzle solving (Noroozi &amp; Favaro, 2016), rotation prediction (Gidaris et al., 2018), arrow of time (Wei et al., 2018), predicting playback velocity (Doersch et al., 2015), and verifying frame order (Misra et al., 2016). Many video anomaly detection methods use self-supervised learning. In fact, self-supervised learning is a key component in the majority of reconstruction-based and prediction-based methods. SSMTL (Georgescu et al., 2021a) trains a CNN jointly on three auxiliary tasks: arrow of time, motion irregularity, and middle-box prediction, in addition to knowledge distillation. Jigsaw-Puzzle (Wang et al., 2022) trains neural networks to solve spatio-temporal jigsaw puzzles. The networks are then used for VAD.</p>
<p>Skeletal methods. Such methods rely on a pose tracker to extract the skeleton trajectories of each person in the video. Anomalies are then detected using the skeleton trajectory data. Our attribute-based method outperforms previous skeletal methods (e.g., (Markovitz et al., 2020; Rodrigues et al., 2020; Yu et al., 2021; Sun &amp; Gong, 2023)) by a large margin. In concurrent work, (Hirschorn &amp; Avidan, 2023) proposed a skeletalbased method that achieved comparable results to ours on the ShanghaiTech dataset but was outperformed by large margins on the rest of the evaluated benchmarks. Different from skeletal approaches, our method does not require pose tracking, which is extremely challenging in crowded scenes. Our pose features only use a single frame, while our velocity features only require a pair of frames. In contrast, skeletal approaches require pose tracking across many frames, which is expensive and error-prone. It is also important to note that skeletal features by themselves are ineffective in detecting non-human anomalies, therefore, being insufficient for providing a complete VAD solution.</p>
<p>Object-level video anomaly detection. Early methods, both classical and deep learning, operated on entire video frames. This proved difficult for VAD as frames contain many variations, as well as a large number of objects. More recent methods (Georgescu et al., 2021a; Liu et al., 2021b; Wang et al., 2022) operate at the object level by first extracting object bounding boxes using off-the-shelf object detectors. Then, they detect if each object is anomalous. This is an easier task, as objects contain much less variation than whole frames. Object-based methods yield significantly better results than frame-level methods.</p>
<p>It is often believed that due to the complexity of realistic scenes and the variety of behaviors, it is difficult to craft features that will discriminate between them. As object detection was inaccurate prior to deep learning, classical methods were previously applied at the frame level rather than at the object level, and therefore underperformed on standard benchmarks. We break this misconception and demonstrates that it is possible to craft semantic features that are accurate.</p>

<h2 class="relative group">3 Method
    <div id="3-method" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-method" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">3.1 Preliminaries
    <div id="31-preliminaries" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#31-preliminaries" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Our method assumes a training set consisting of Nc Nc video clips {c1, c2&hellip;cN c } ∈ Xtrain that are all normal (i.e., do not contain any anomalies). Each clip ciis comprised of Ni frames, ci = [fi,1, fi,2, &hellip;fi,N i ]. The goal is to classify each frame f ∈ c in an inference clip c as normal or anomalous. Our method represents each frame f as ϕ(f) ∈ R d , where d ∈ N is the feature dimension. We compute the anomaly score of frame f using an anomaly scoring function s(ϕ(f)), and classify it as anomalous if s(ϕ(f)) exceeds a threshold.</p>

<h2 class="relative group">3.2 Overview
    <div id="32-overview" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#32-overview" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We propose a method that represents each video frame as a set of objects, with each object characterized by its attributes. This contrasts with most previous methods that do not explicitly represent attributes. Specifically, we focus on two key attributes: velocity and body pose. Object velocity is probably the most important attribute as it can detect if an object is moving unusually fast e.g., running away, or in a strange</p>
<p>Figure 2: An overview of our method. We first extract optical flow maps and bounding boxes for all of the objects in the frame. We then crop each object from the original image and its corresponding flow map. Our representation consists of velocity, pose, and deep (CLIP) features.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_49b4548b5bff11872203e16f16941115a6d5a73f66b4db2a709509fd9e6e3f75.png"
    ></figure>
<p>direction e.g., against the direction of traffic. Given the challenges of capturing true 3D velocity without depth information, we use optical flow as an effective proxy to represent apparent motion between frames. Similarly, human body poses can reveal anomalous activities, such as throwing an item. Instead of using the full 3D human pose, we represent it with 2D landmarks, which are easier to detect. Both attributes have been used in previous VAD studies, such as Georgescu et al. (2021a) and Markovitz et al. (2020), as part of more complex approaches.</p>
<p>We compute the anomaly score based on density estimation of object-level feature descriptors. This is done in three stages: pre-processing, feature extraction, and density estimation. In the pre-processing stage, our method (i) uses an off-the-shelf motion estimator to estimate the optical flow for each frame; (ii) localizes and classifies the bounding boxes of all objects within a frame using an off-the-shelf object detector. The outputs of these models are used to extract object-level velocity, pose, and deep representations (see Sec. 3.4). Finally, our method uses density estimation to calculate the anomaly score of each test frame. See Fig. 2 for an illustration.</p>

<h2 class="relative group">3.3 Pre-processing
    <div id="33-pre-processing" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#33-pre-processing" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Our method extracts velocity and pose from each object in each video frame. To do so, we compute optical flow and body landmarks for each object in the video.</p>
<p>Optical flow. Our method uses optical flow as a proxy for its velocity. We extract the optical flow map o for each frame f ∈ c in every video clip c using an off-the-shelf optical flow model.</p>
<p>Object detection. Our method models frames by representing every object individually. This follows many recent papers, e.g., (Georgescu et al., 2021a; Liu et al., 2021b; Wang et al., 2022) that found that object-based representations are more effective than global, frame-level representations. We first detect all objects in each frame using an off-the-shelf object detector. Formally, our object detection generates a set of m bounding boxes bb1, bb2&hellip;bb m for each frame, with corresponding category labels y1, y2, &hellip;, y m.</p>
<p>Figure 3: An illustration of our velocity feature vector. Left: We quantize the orientations into B = 8 equi-spaced bins, and assign each optical flow vector in the object&rsquo;s bounding box is to a single bin. Right: The value of each bin is the average magnitude of the optical flow vectors assigned to this bin. Best viewed in color.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_7d31c9eb9706efc72d29eaedc5af07c1b91cbb5afc8c97944a48fe289be3d66a.png"
    ></figure>

<h2 class="relative group">3.4 Feature extraction
    <div id="34-feature-extraction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#34-feature-extraction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Our method represents each object by two semantic attributes, velocity and pose, and by an implicit, deep representation.</p>
<p>Velocity features. Our working hypothesis is that an unusual speed or direction of motion can often identify anomalies in video. As objects can move in both x and y axes and both the magnitude (speed) and orientation of the velocity may be anomalous, we compute velocity features for each object in each frame. We begin by cropping the frame-level optical flow map o by the bounding box bb of each detected object. Following this step, we obtain a set of cropped object flow maps (see Fig. 2). We rescale these flow maps to a fixed size of Hf low × Wf low. Next, we represent each flow map with the average motion for each orientation, where orientations are quantized into B ∈ N equi-spaced bins, following classical optical flow representations e.g., (Chaudhry et al., 2009). The final representation is a B-dimensional vector consisting of the average flow magnitude of the flow vectors in each bin (see Fig. 3). This representation is capable of describing motion in both radial and tangential directions. We denote our velocity feature extractor as: ϕvelocity : Hf low × Wf low → R B .</p>
<p>Pose features. Most anomalous events in video involve humans, so we include activity features in our representation. While a full understanding of activity requires temporal features, we find that human body pose from a single frame can detect many unusual activities. Even though human pose is essentially a 3D feature, 2D body landmark positions already provide much of the signal. We compute pose features for each human object bb using an off-the-shelf 2D keypoint extractor that outputs the pixel coordinates of each landmark position, denoted by ϕ ˆ pose (bb) ∈ R 2×d , where d ∈ N is the number of keypoints. To ensure invariance to the human&rsquo;s position and size, we normalize the keypoints. First, we subtract the coordinates of the top-left corner of the object bounding box from each landmark. We then scale the x and y axes so that the object bounding box has a fixed size of Hp Hpose × Wp Wpose , where Hp Hpose and Wp Wpose are constants. Formally, let l ∈ R 2 be the top-left corner of the human bounding box. The pose descriptor becomes:</p>
<!-- formula-not-decoded -->
<p>Here, height(bb) and width(bb) are the height and width of the object bounding box bb, respectively and l ∈ R 2 be the top-left corner of bb. Finally, we flatten ϕ pose to obtain the final pose feature vector.</p>
<p>Deep features. While velocity is the most discriminative attribute, other attributes beyond pose may also matter. Deep features implicitly bundle together many different attributes. Hence, we use them to model the residual attributes which are not described by velocity and pose. We follow previous anomaly detection works (Reiss et al., 2021; Reiss &amp; Hoshen, 2023), that used generic, pretrained encoders to implicitly represent image attributes. Concretely, we use a pretrained CLIP encoder (Radford et al., 2021), ϕdeep( . ), to represent the bounding box of each object in each frame. Note that CLIP representations do not achieve competitiveness</p>
<p>on their own; in fact, they perform much worse than the velocity representations (see Tab. 3). However, together, velocity, pose, and CLIP features represent video sufficiently well to outperform the state-of-the-art.</p>

<h2 class="relative group">3.5 Density Estimation
    <div id="35-density-estimation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#35-density-estimation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We use density estimation for scoring samples as normal or anomalous, where a low estimated density indicates anomaly. To estimate density, we fit a separate estimator for each feature. As velocity features are low-dimensional, we use a Gaussian mixture (GMM) estimator. As our pose and deep features are high-dimensional, we estimate their density using kNN. Specifically, we compute the L2 distance between the feature x of a target object and the nearest k exemplars in the corresponding training feature set. We compare different exemplar selection methods is in Sec. 4.5. We denote our density estimators by svel ( . ), s pose ( . ), sdeep( . ) .</p>
<p>Score calibration. Combining the three density estimators requires calibration. To do so, we estimate the distribution of anomaly scores on the normal training set. We then scale the scores using min-max normalization. The kNN used for scoring pose and deep features present a subtle point. When computing kNN on the training set, the exemplars must not be taken from the same clip as the target object. The reason is that the same object appears in nearby frames with virtually no variation, distorting kNN estimates. Instead, we compute the kNN between each training set object and all objects in the other video clips provided in the training set. We can now define ∀f ∈ {velocity, pose, deep}: µf = max x {sf (ϕf (x))} and ν f = min x {sf (ϕf (x))} .</p>

<h2 class="relative group">3.6 Inference
    <div id="36-inference" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#36-inference" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We extract a representation for each object bb in each frame f in each inference clip, as describe above. We then compute an anomaly score for each attribute feature of each object bb. The score for every frame is simply the maximum score across all objects. The final anomaly score is the sum of the individual feature scores normalized by our calibration parameters:</p>
<!-- formula-not-decoded -->
<p>As anomalous events span multiple frames, we smooth the frame scores using a temporal smoothing filter.</p>

<h2 class="relative group">4 Experiments
    <div id="4-experiments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#4-experiments" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">4.1 Datasets
    <div id="41-datasets" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#41-datasets" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We evaluated our method on three publicly available VAD datasets, using their training and test splits. Only test videos included anomalous events. We report the statistics of the datasets in Tab. 1.</p>
<p>UCSD Ped2. This dataset (Mahadevan et al., 2010) contains 16 normal training videos and 12 test videos at a 240 × 360 pixel resolution. Videos show a fixed scene with a camera above the scene and pointed downward. The training video clips contain only normal behavior of pedestrians walking, while examples of abnormal events are bikers, skateboarding, and cars.</p>
<p>CUHK Avenue. This dataset (Lu et al., 2013) contains 16 normal training videos and 21 test videos at 360 × 640 pixel resolution. Videos show a fixed scene using a ground-level camera. Training video clips contain only normal behavior. Examples of abnormal events are strange activities (e.g. throwing objects, loitering, and running), movement in the wrong direction, and abnormal objects.</p>
<p>ShanghaiTech Campus. This dataset (Liu et al., 2018a) is the largest publicly available dataset for VAD. There are 330 training videos and 107 test videos from 13 different scenes at 480 × 856 pixel resolution. ShanghaiTech contains video clips with complex light conditions and camera angles, making this dataset</p>
<p>Table 1: Statistics of the evaluation datasets.</p>
<table>
  <thead>
      <tr>
          <th>Dataset</th>
          <th>Number of Frames  t Tt t Nl Al</th>
          <th>Number of Frames  t Tt t Nl Al</th>
          <th>Number of Frames  t Tt t Nl Al</th>
          <th>Number of Frames  t Tt t Nl Al</th>
          <th>Number of Frames  t Tt t Nl Al</th>
          <th>Scenes</th>
          <th>Anomaly Types</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Dataset</td>
          <td>Total</td>
          <td>Train set</td>
          <td>Test set</td>
          <td>Normal</td>
          <td>Anomalous</td>
          <td>Scenes</td>
          <td>Anomaly Types</td>
      </tr>
      <tr>
          <td>UCSD Ped2</td>
          <td>4,560</td>
          <td>2,550</td>
          <td>2,010</td>
          <td>2,924</td>
          <td>1,636</td>
          <td>1</td>
          <td>5</td>
      </tr>
      <tr>
          <td>CUHK Avenue</td>
          <td>30,652</td>
          <td>15,328</td>
          <td>15,324</td>
          <td>26,832</td>
          <td>3,820</td>
          <td>1</td>
          <td>5</td>
      </tr>
      <tr>
          <td>ShanghaiTech</td>
          <td>317,398</td>
          <td>274,515</td>
          <td>42,883</td>
          <td>300,308</td>
          <td>17,090</td>
          <td>13</td>
          <td>11</td>
      </tr>
  </tbody>
</table>
<p>more challenging than the other two. Anomalies include robberies, jumping, fights, car invasions, and bike riding in pedestrian areas.</p>

<h2 class="relative group">4.2 Implementation Details
    <div id="42-implementation-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#42-implementation-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We use ResNet50 Mask-RCNN (He et al., 2017) pretrained on MS-COCO (Lin et al., 2014) to extract object bounding boxes. To filter out low confidence objects, we follow the same configurations as in (Georgescu et al., 2021a). Specifically for Ped2, Avenue, and ShanghaiTech, we set confidence thresholds of 0.5, 0.8, and 0.8. In order to generate optical flow maps, we use FlowNet2 (Ilg et al., 2017). For our landmark detection, we use AlphaPose (Fang et al., 2017) pretrained on MS-COCO with d = 17 keypoints. We use a pretrained ViT B-16 CLIP (Dosovitskiy et al., 2020; Radford et al., 2021) image encoder as our deep feature extractor. Our method is built around the extracted objects and flow maps. We use Hvelocity × Wvelocity = 224 × 224 to rescale flow maps. As for Hp Hpose × Wp Wpose rescaling, we calculate the average height and width from the bounding boxes of the train set and use those values. The lower resolution of Ped2 prevents objects from filling a histogram, and to extract pose representations, therefore we use B = 1 orientations and rely solely on velocity and deep representations. We use B = 8 orientations for Avenue and ShanghaiTech. When testing, for anomaly scoring we use kNN for the pose and deep representations with k = 1 nearest neighbors. For velocity, we use GMM with n = 5 Gaussians. Finally, the anomaly score of a frame represents the maximum score among all the objects within that frame.</p>

<h2 class="relative group">4.3 Evaluation Metrics
    <div id="43-evaluation-metrics" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#43-evaluation-metrics" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Our study uses standard VAD evaluation metrics. We vary the threshold over the anomaly scores to measure the frame-level Area Under the Receiver Operation Characteristic (AUROC) with respect to the ground-truth annotations. We report two types of AUROC: (i) micro-averaged AUROC, which computes the score by on all frames from all videos; (ii) macro-averaged AUROC, which computes the AUROC score individually for each video and then averages the scores of all videos. Most existing studies report micro-averaged AUROC, while only a few report macro-averaged AUROC.</p>

<h2 class="relative group">4.4 Quantitative Results
    <div id="44-quantitative-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#44-quantitative-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We compare our method and the state-of-the-art from recent years in Tab. 2. We took the performance numbers of the baseline methods directly from the original papers.</p>
<p>Ped2 results. Most methods obtained over 94% on Ped2, indicating that of the three public datasets, it is the simplest. While our method is comparable to the current state-of-the-art method (HF 2 (Liu et al., 2021b)) in terms of performance, the near-perfect results on Ped2 indicate it is practically solved.</p>
<p>Avenue results. Our method obtained a new state-of-the-art micro-averaged AUROC of 93.7%. Our method also outperformed the current state-of-the-art in terms of macro-averaged AUROC by a considerable margin of 2.8%, reaching 96.3%.</p>
<p>ShanghaiTech results. Our method outperforms all previous methods on the largest dataset, ShanghaiTech, by a considerable margin. Accordingly, our method achieves 85.9% AUROC, higher than the best performance previous methods achieved, 85.1% (MS-VAD (Zhang et al., 2024)). We note that in concurrent</p>
<p>Table 2: Frame-level AUROC (%) comparison. The best and second-best results are bolded and underlined, respectively.</p>
<table>
  <thead>
      <tr>
          <th>Year</th>
          <th>Method</th>
          <th>Ped2  iM</th>
          <th>Ped2  iM</th>
          <th>Avenue  iM</th>
          <th>Avenue  iM</th>
          <th>ShanghaiTech Micro Macro</th>
          <th>ShanghaiTech Micro Macro</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Year</td>
          <td>Method</td>
          <td>Micro</td>
          <td>Macro</td>
          <td>Micro</td>
          <td>Macro</td>
          <td>Micro</td>
          <td>Macro</td>
      </tr>
      <tr>
          <td></td>
          <td>HOOF (Chaudhry et al., 2009)</td>
          <td>61.1</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td></td>
          <td>(y , )  HOFM (Colque et al., 2016)</td>
          <td>89.9</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td></td>
          <td>(q,  SCL (Lu et al., 2013)</td>
          <td>-</td>
          <td>-</td>
          <td>80.9</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td></td>
          <td>()  Conv-AE (Hasan et al., 2016a)  ()</td>
          <td>90.0</td>
          <td>-</td>
          <td>70.2</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td></td>
          <td>(, )  StackRNN (Luo et al., 2017a)  SA()</td>
          <td>92.2</td>
          <td>-</td>
          <td>81.7</td>
          <td>-</td>
          <td>68.0</td>
          <td>-</td>
      </tr>
      <tr>
          <td></td>
          <td>STAN (Lee et al., 2018)  (</td>
          <td>96.5</td>
          <td>-</td>
          <td>87.2</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td></td>
          <td>MC2ST (Liu et al., 2018b)</td>
          <td>87.5</td>
          <td>-</td>
          <td>84.4</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td></td>
          <td>Frame-Pred. (Liu et al., 2018a)</td>
          <td>95.4</td>
          <td>-</td>
          <td>85.1</td>
          <td>-</td>
          <td>72.8</td>
          <td>-</td>
      </tr>
      <tr>
          <td></td>
          <td>()  Mem-AE. (Gong et al., 2019)</td>
          <td>94.1</td>
          <td>-</td>
          <td>83.3</td>
          <td>-</td>
          <td>71.2</td>
          <td>-</td>
      </tr>
      <tr>
          <td></td>
          <td>(g , )  CAE-SVM (Ionescu et al., 2019)</td>
          <td>94.3</td>
          <td>97.8</td>
          <td>87.4</td>
          <td>90.4</td>
          <td>78.7</td>
          <td>84.9</td>
      </tr>
      <tr>
          <td></td>
          <td>(,  BMAN (Lee et al., 2019)</td>
          <td>93  96.6</td>
          <td>97.8  -</td>
          <td>87.4  900</td>
          <td>90.4</td>
          <td>78.7</td>
          <td>84.9</td>
      </tr>
      <tr>
          <td></td>
          <td>()  AM-Corr (Nguyen &amp; Meunier, 2019)</td>
          <td>96.6  962</td>
          <td>-</td>
          <td>90.0  869</td>
          <td>-</td>
          <td>76.2</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2020</td>
          <td>MNAD-Recon. (Park et al., 2020)</td>
          <td>97.0</td>
          <td>-</td>
          <td>88.5</td>
          <td>-</td>
          <td>70.5</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2020</td>
          <td>( CAC (Wang et al., 2020)</td>
          <td>-</td>
          <td>-</td>
          <td>87.0</td>
          <td>-</td>
          <td>79.3</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2020</td>
          <td>Scene-Aware (Sun et al., 2020)</td>
          <td>-</td>
          <td>-</td>
          <td>89.6</td>
          <td>-</td>
          <td>74.7</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2020</td>
          <td>VEC (Yu et al., 2020)</td>
          <td>97.3</td>
          <td>-</td>
          <td>90.2</td>
          <td>-</td>
          <td>74.8</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2020</td>
          <td>ClusterAE (Chang et al., 2020)</td>
          <td>96.5</td>
          <td>-</td>
          <td>86.0</td>
          <td>-</td>
          <td>73.3</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2021</td>
          <td>AMMCN (Cai et al., 2021)</td>
          <td>96.6</td>
          <td>-</td>
          <td>86.6</td>
          <td>-</td>
          <td>73.7</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2021</td>
          <td>SSMTL (Georgescu et al., 2021a)</td>
          <td>97.5</td>
          <td>99.8</td>
          <td>91.5</td>
          <td>91.9</td>
          <td>82.4</td>
          <td>89.3</td>
      </tr>
      <tr>
          <td>2021</td>
          <td>( MPN (Lv et al., 2021)</td>
          <td>96.9</td>
          <td>-</td>
          <td>89.5</td>
          <td>-</td>
          <td>73.8</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2021</td>
          <td>()  HF2(Liu et al., 2021a)</td>
          <td>99.3</td>
          <td>-</td>
          <td>91.1</td>
          <td>93.5</td>
          <td>76.2</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2021</td>
          <td>CT-D2GAN (Feng et al., 2021a)</td>
          <td>97.2</td>
          <td>-</td>
          <td>85.9</td>
          <td>-</td>
          <td>77.7</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2021</td>
          <td>BA-AED (Georgescu et al., 2021b)</td>
          <td>98.7</td>
          <td>99.7</td>
          <td>92.3</td>
          <td>90.4</td>
          <td>82.7</td>
          <td>89.3</td>
      </tr>
      <tr>
          <td>2022</td>
          <td>SSPCAB (Ristea et al., 2022)</td>
          <td>-</td>
          <td>-</td>
          <td>92.9</td>
          <td>91.9</td>
          <td>83.6</td>
          <td>89.5</td>
      </tr>
      <tr>
          <td>2022</td>
          <td>(, )  DLAN-AC (Yang et al., 2022)</td>
          <td>97.6</td>
          <td>-</td>
          <td>89.9</td>
          <td>-</td>
          <td>74.7</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2022</td>
          <td>(g , )  Jigsaw-Puzzle (Wang et al., 2022)</td>
          <td>99.0</td>
          <td>99.9</td>
          <td>92.2</td>
          <td>93.0</td>
          <td>84.3</td>
          <td>89.8</td>
      </tr>
      <tr>
          <td>2023</td>
          <td>USTN-DSC (Yang et al., 2023)</td>
          <td>98.1</td>
          <td>-</td>
          <td>89.9</td>
          <td>-</td>
          <td>73.8</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2023</td>
          <td>( EVAL (Singh et al., 2023)</td>
          <td>-</td>
          <td>-</td>
          <td>86.0</td>
          <td>-</td>
          <td>76.6</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2023</td>
          <td>(g, )  FB-SAE (Cao et al., 2023)</td>
          <td>97.1</td>
          <td>99.2</td>
          <td>86.8</td>
          <td>89.1</td>
          <td>79.2</td>
          <td>80.2</td>
      </tr>
      <tr>
          <td>2023</td>
          <td>(, )  FPDM (Yan et al., 2023)</td>
          <td>-</td>
          <td>-</td>
          <td>90.1</td>
          <td>-</td>
          <td>78.6</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2023</td>
          <td>(, )  LMPT (Shi et al., 2023)</td>
          <td>97.6</td>
          <td>-</td>
          <td>90.9</td>
          <td>-</td>
          <td>78.8</td>
          <td>-</td>
      </tr>
      <tr>
          <td>2023</td>
          <td>STF-NF (Hirschorn &amp; Avidan, 2023)</td>
          <td>93.1</td>
          <td>91.2</td>
          <td>60.1</td>
          <td>63.5</td>
          <td>85.9</td>
          <td>87.8</td>
      </tr>
      <tr>
          <td>2024</td>
          <td>SD-MAE (Ristea et al., 2024)</td>
          <td>95.4</td>
          <td>98.4</td>
          <td>91.3</td>
          <td>90.9</td>
          <td>79.1</td>
          <td>84.7</td>
      </tr>
      <tr>
          <td>2024</td>
          <td>MS-VAD (Zhang et al., 2024)</td>
          <td>-</td>
          <td>-</td>
          <td>92.4</td>
          <td>92.9</td>
          <td>85.1</td>
          <td>89.8</td>
      </tr>
      <tr>
          <td>2024</td>
          <td>Ours</td>
          <td>99.1</td>
          <td>99.9</td>
          <td>93.7</td>
          <td>96.3</td>
          <td>85.9</td>
          <td>89.6</td>
      </tr>
  </tbody>
</table>
<p>work, STF-NF (Hirschorn &amp; Avidan, 2023) achieved comparable results to ours on the ShanghaiTech dataset. Our method outperforms it by large margins on Ped2 (by 6.0% AUROC) and Avenue (by 33.6% AUROC).</p>
<p>To summarize, our method achieves the highest performance on the three most popular public benchmarks. It simply consists of three simple representations and does not require training.</p>

<h2 class="relative group">4.5 Analysis
    <div id="45-analysis" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#45-analysis" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Ablation study. We report in Tab. 3 the anomaly detection performance on the Ped2, Avenue and ShanghaiTech datasets of all attribute combinations. Our findings reveal that the velocity features provide the highest frame-level AUROC on Ped2, Avenue and ShanghaiTech, with 98.8%, 86.0% and 84.4% micro-averaged AUROC, respectively. In ShanghaiTech, our velocity features on their own are already state-of-the-art com-</p>
<p>Table 3: Ablation study. Result are in frame-level AUROC (%). The best and second-best results are in bold and underline, respectively.</p>
<table>
  <thead>
      <tr>
          <th>Pose Features</th>
          <th>Deep Features</th>
          <th>Velocity Features</th>
          <th>Ped2</th>
          <th>Ped2</th>
          <th>Avenue</th>
          <th>Avenue</th>
          <th>ShanghaiTech</th>
          <th>ShanghaiTech</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Pose Features</td>
          <td>Deep Features</td>
          <td>Velocity Features</td>
          <td>Micro</td>
          <td>Macro</td>
          <td>Micro</td>
          <td>Macro</td>
          <td>Micro</td>
          <td>Macro</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td>-</td>
          <td>-</td>
          <td>73.8</td>
          <td>76.2</td>
          <td>74.5</td>
          <td>81.0</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td>96.4</td>
          <td>95.3</td>
          <td>85.4</td>
          <td>87.7</td>
          <td>72.5</td>
          <td>82.5</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
          <td>98.8</td>
          <td>99.6</td>
          <td>86.0</td>
          <td>89.6</td>
          <td>84.4</td>
          <td>84.8</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
          <td>-</td>
          <td>-</td>
          <td>89.3</td>
          <td>88.8</td>
          <td>76.7</td>
          <td>84.9</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
          <td>99.1</td>
          <td>99.9</td>
          <td>93.0</td>
          <td>95.5</td>
          <td>84.5</td>
          <td>88.7</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
          <td>-</td>
          <td>-</td>
          <td>86.8</td>
          <td>93.0</td>
          <td>85.9</td>
          <td>88.8</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
          <td>-</td>
          <td>-</td>
          <td>93.7</td>
          <td>96.3</td>
          <td>85.1</td>
          <td>89.6</td>
      </tr>
  </tbody>
</table>
<p>Table 4: Comparison of different numbers of velocity features bins (B). Frame-level AUROC (%) results. Best in bold.</p>
<table>
  <thead>
      <tr>
          <th>Bins (B)</th>
          <th>Avenue</th>
          <th>Avenue</th>
          <th>ShanghaiTech MiM</th>
          <th>ShanghaiTech MiM</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Bins (B)</td>
          <td>Micro</td>
          <td>Macro</td>
          <td>Micro</td>
          <td>Macro</td>
      </tr>
      <tr>
          <td>B = 1</td>
          <td>83.5</td>
          <td>83.5</td>
          <td>81.2</td>
          <td>80.9</td>
      </tr>
      <tr>
          <td>B = 2</td>
          <td>84.1</td>
          <td>83.8</td>
          <td>82.1</td>
          <td>82.7</td>
      </tr>
      <tr>
          <td>B = 4</td>
          <td>85.5</td>
          <td>89.2</td>
          <td>84.0</td>
          <td>84.6</td>
      </tr>
      <tr>
          <td>B = 8</td>
          <td>86.0</td>
          <td>89.6</td>
          <td>84.4</td>
          <td>84.8</td>
      </tr>
      <tr>
          <td>B = 16</td>
          <td>84.1</td>
          <td>88.4</td>
          <td>83.1</td>
          <td>84.2</td>
      </tr>
  </tbody>
</table>
<p>Table 5: Our final results when kNN is replaced by k-means. Frame-level AUROC (%). Time is expressed in average ms per frame. Best in bold.</p>
<table>
  <thead>
      <tr>
          <th>k =</th>
          <th>Avenue</th>
          <th>Avenue</th>
          <th>Avenue</th>
          <th>ShanghaiTech iMTi</th>
          <th>ShanghaiTech iMTi</th>
          <th>ShanghaiTech iMTi</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>k =</td>
          <td>Mic.</td>
          <td>Mac.</td>
          <td>Time</td>
          <td>Mic.</td>
          <td>Mac.</td>
          <td>Time</td>
      </tr>
      <tr>
          <td>1</td>
          <td>91.8</td>
          <td>94.0</td>
          <td>0.51</td>
          <td>84.2</td>
          <td>87.2</td>
          <td>0.45</td>
      </tr>
      <tr>
          <td>5</td>
          <td>92.0</td>
          <td>94.2</td>
          <td>0.52</td>
          <td>84.3</td>
          <td>88.1</td>
          <td>0.45</td>
      </tr>
      <tr>
          <td>10</td>
          <td>92.1</td>
          <td>94.5</td>
          <td>0.52</td>
          <td>84.6</td>
          <td>88.1</td>
          <td>0.45</td>
      </tr>
      <tr>
          <td>100</td>
          <td>92.9</td>
          <td>95.2</td>
          <td>0.53</td>
          <td>84.8</td>
          <td>88.6</td>
          <td>0.46</td>
      </tr>
      <tr>
          <td>All</td>
          <td>93.7</td>
          <td>96.3</td>
          <td>4.93</td>
          <td>85.1</td>
          <td>89.6</td>
          <td>36.0</td>
      </tr>
  </tbody>
</table>
<p>pared with all previous VAD methods. We expect this to be due to the large number of anomalies associated with speed and motion, such as running people and fast-moving objects, e.g. cars and bikes. Adding either pose or CLIP improved performance, mostly macro-AUROC, presumably as it provided information about human activity which accounts for some of the anomalies in this dataset. Velocity features were still the most performant on Ped2 and Avenue. However, combining them with deep features improved performance significantly. Overall, we observe that using all three features performed the best on Avenue. Due to the extremely low resolution of the Ped2 dataset, pose feature extraction is not feasible, so we rely solely on velocity and deep features for this dataset.</p>
<p>Number of velocity bins. We ablated the impact of different numbers of bins (B) in our velocity features in Tab. 4. We compared AUROC scores on the Avenue and ShanghaiTech datasets. The results indicate that the choice of B influences detection accuracy. Specifically, we observed that increasing the number of bins from B = 1 to B = 8 led to consistent improvements in both micro and macro AUROC scores on both datasets. This suggests that a finer quantization of velocity orientations represents motion better and improves anomaly detection. Performance gains diminish beyond B = 8 .</p>
<p>k-Means as a faster alternative. Computing kNN has linear complexity in the number of objects in the datasets, which may be slow for large datasets. We can speed it up by reducing the number of samples via k-means. In Tab. 5, we compare the performance of our method with kNN and k-means. Note that k-means still uses kNN to calculate anomaly scores as the sum of distances to nearest neighbor means. This is much faster than the original kNN as there are fewer means than the number of objects in the training set. We observe that it improves inference time with a small accuracy loss.</p>
<p>Pose features for non-human objects. We extract pose representations exclusively for human objects and not for non-human objects. We calculate the pose anomaly score for each frame by taking the score of the object with the most anomalous pose. Non-human objects are given a pose anomaly score of −∞ and therefore do not contribute to the frame-wise pose anomaly score. While we acknowledge that non-human objects can also exhibit anomalies, our method leverages velocity and deep representations to capture these types of events.</p>
<p>Table 6: Comparison of FlowNet2 vs. RAFT for flow map extraction. Frame-level AUROC (%) based on velocity features.</p>
<table>
  <thead>
      <tr>
          <th>Backbone</th>
          <th>Avenue</th>
          <th>Avenue</th>
          <th>ShanghaiTech</th>
          <th>ShanghaiTech</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Backbone</td>
          <td>Micro</td>
          <td>Macro</td>
          <td>Micro</td>
          <td>Macro</td>
      </tr>
      <tr>
          <td>RAFT</td>
          <td>85.7</td>
          <td>89.7</td>
          <td>84.3</td>
          <td>84.2</td>
      </tr>
      <tr>
          <td>FlowNet2</td>
          <td>86.0</td>
          <td>89.6</td>
          <td>84.4</td>
          <td>84.8</td>
      </tr>
  </tbody>
</table>
<p>Table 7: Comparison of Mask R-CNN vs. YOLO-v8 for object detection. Frame-level AUROC (%) based on velocity features.</p>
<table>
  <thead>
      <tr>
          <th>Backbone</th>
          <th>Avenue</th>
          <th>Avenue</th>
          <th>ShanghaiTech</th>
          <th>ShanghaiTech</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Backbone</td>
          <td>Micro</td>
          <td>Macro</td>
          <td>Micro</td>
          <td>Macro</td>
      </tr>
      <tr>
          <td>YOLO-v8</td>
          <td>84.8</td>
          <td>87.4</td>
          <td>83.1</td>
          <td>82.7</td>
      </tr>
      <tr>
          <td>Mask-RCNN</td>
          <td>86.0</td>
          <td>89.6</td>
          <td>84.4</td>
          <td>84.8</td>
      </tr>
  </tbody>
</table>
<p>Table 8: Comparison of video encoders and CLIP. Frame-level AUROC (%) results. Best in bold.</p>
<table>
  <thead>
      <tr>
          <th>Encoder</th>
          <th>Level</th>
          <th>Avenue S</th>
          <th>Avenue S</th>
          <th>ShanghaiTech</th>
          <th>ShanghaiTech</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Encoder</td>
          <td>Level</td>
          <td>Micro</td>
          <td>Macro</td>
          <td>Micro</td>
          <td>Macro</td>
      </tr>
      <tr>
          <td>TimeSformer (Bertasius et al., 2021)</td>
          <td>Frame</td>
          <td>61.2</td>
          <td>64.1</td>
          <td>58.2</td>
          <td>60.1</td>
      </tr>
      <tr>
          <td>TimeSformer (Bertasius et al., 2021)</td>
          <td>Object</td>
          <td>63.1</td>
          <td>64.0</td>
          <td>59.1</td>
          <td>59.2</td>
      </tr>
      <tr>
          <td>VideoMAE V2 (Wang et al., 2023)</td>
          <td>Frame</td>
          <td>68.3</td>
          <td>67.9</td>
          <td>60.3</td>
          <td>60.5</td>
      </tr>
      <tr>
          <td>VideoMAE V2 (Wang et al., 2023)</td>
          <td>Object</td>
          <td>67.0</td>
          <td>68.1</td>
          <td>60.0</td>
          <td>59.9</td>
      </tr>
      <tr>
          <td>DINO</td>
          <td>Object</td>
          <td>77.6</td>
          <td>81.2</td>
          <td>71.2</td>
          <td>80.3</td>
      </tr>
      <tr>
          <td>CLIP (ours)</td>
          <td>Object</td>
          <td>85.4</td>
          <td>87.7</td>
          <td>72.5</td>
          <td>82.5</td>
      </tr>
  </tbody>
</table>
<p>Backbone analysis. We performed additional ablation studies to evaluate the impact of different backbone networks on the overall performance of our method. Specifically, we tested alternative backbones for optical flow (FlowNet2 vs. RAFT (Teed &amp; Deng, 2020)), as shown in Tab. 6 and object detection (Mask R-CNN vs. YOLO-v8) in Tab. 7. The results indicate that the effectiveness of our approach is primarily driven by the feature design rather than any specific choice of backbone.</p>
<p>Why do we use an image encoder instead of a video encoder? Recent self-supervised learning methods such as TimeSformer (Bertasius et al., 2021), VideoMAE (Tong et al., 2022; Wang et al., 2023), XCLIP (Ni et al., 2022), and CoCa (Yu et al., 2022) have significantly improved the performance of pretrained video encoders on downstream tasks like Kinetics-400 (Kay et al., 2017). It is therefore reasonable to expect that video encoders, which capture both temporal and spatial information, would outperform image encoders in video anomaly detection (VAD). However, in our experiments, we found that features extracted by pretrained video encoders did not perform as well as those extracted from pretrained image encoders on VAD benchmark datasets. We hypothesize that this weaker performance is due to the video encoders&rsquo; focus on capturing frame-level temporal dynamics, whereas our method is object-centric. Additionally, when we tested video encoders on 10-frame windows of fixed object bounding boxes (centered around time t), we observed no performance gain, likely due to resolution constraints and the need for high-quality contextual information. Tab. 8 summarizes our findings on the limited effectiveness of video encoders in this setting. Additionally, we evaluated DINO (Caron et al., 2021) as a comparison to CLIP and found that while DINO performed slightly worse than CLIP, it still outperformed video encoders. This result, with DINO showing only a slight performance drop compared to CLIP, demonstrates that our deep features are not dependent on a specific image encoder.</p>
<p>Running times. We carried out all our experiments on a NVIDIA RTX 2080 GPU. Our preprocessing stage, which includes object detection and optical flow extraction, takes approximately 80 milliseconds (ms) per frame. It takes our method approximately 5 ms to compute the velocity extraction, pose extraction, and deep features extraction stages, combined with anomaly scoring. Our method runs at 12FPS with an average of 5 objects per frame. For comparison, we evaluated two other methods on the same hardware: BA-AED (Georgescu et al., 2021b) runs at 24 FPS, while HF 2 Liu et al. (2021a), 2021) runs at 12 FPS. Our method&rsquo;s running speed is comparable to HF 2 but slightly slower than BA-AED.</p>
<p>Figure 4: Frame-level scores and anomaly localizations for Avenue&rsquo;s test video 04. Best viewed in color.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_fa8634c79b91e5e72c7c3a7195271d8e4b3e55a327be2208a6d8a1ecd41161b6.png"
    ></figure>

<h2 class="relative group">4.6 Qualitative Results
    <div id="46-qualitative-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#46-qualitative-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We visualize the anomaly detection process for Avenue and ShanghaiTech in Fig. 4 and Fig. 5, where we plot the anomaly scores across all frames of a video. Our anomaly scores are clearly highly correlated with anomalous events, demonstrating the effectiveness of our method.</p>

<h2 class="relative group">5 Discussion
    <div id="5-discussion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#5-discussion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Exploring other semantic attributes. There are other important attributes for VAD beyond velocity and pose. Identifying other relevant attributes that correlate with anomalous events can further improve anomaly detection systems. For example, attributes related to object interactions, spatial arrangements, or temporal patterns may be very discriminative for some types of anomalies. Finding ways to systematically discover such attributes may significantly speed up research.</p>
<p>Guidance. Finding relevant attributes for anomaly detection may require user guidance. In real-world scenarios, operators have domain knowledge about factors that lead to anomalous behavior. Efficiently incorporating this guidance, such as selecting velocity or pose features in our work, is essential for leveraging this knowledge effectively.</p>
<p>Other academic benchmarks. While our method, using simple attributes, was effective on the three most popular VAD datasets, extending it to more complex datasets may require more work. Publicly available datasets such as UCF-Crime (Sultani et al., 2018) and XD-Violence (Wu et al., 2020), which feature a wider variety of anomalies and larger scales, present additional challenges. These datasets are essentially different from the ones tested here as they contain distinct scenes in training and testing data, and include moving cameras, which also change the scene. So far, only weakly-supervised VAD has been successful on these datasets as they labeled anomalous data in training. The field needs new, more complex datasets within the fixed camera setting to further stress-test one-class classification VAD methods such as ours.</p>

<h2 class="relative group">6 Ethical Considerations
    <div id="6-ethical-considerations" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#6-ethical-considerations" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>While VAD offers significant potential for enhancing public safety and security, it is crucial to acknowledge and address the ethical implications of such technology. VAD systems, including our proposed method, can be used in surveillance applications, which raises important privacy concerns. The continuous monitoring of public spaces may lead to a sense of constant observation, potentially infringing on individuals&rsquo; right to privacy and freedom of movement. Moreover, there is a risk that VAD systems could be misused for unauthorized tracking or profiling of individuals.</p>
<p>To mitigate these ethical risks, several strategies should be considered in VAD systems development and deployment. First, strict data protection protocols should be implemented to ensure that collected video data is securely stored, accessed only by authorized individuals, and deleted after a defined period. Second, VAD use should be transparent, with clear warnings informing individuals when they are entering areas</p>
<p>Figure 5: Frame-level scores and anomaly localizations for ShanghaiTech&rsquo;s test video 03 _ 0059. Best viewed in color.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000004_f60b93dc26b235a2169d434f82ac8d8962792f69c1c5de7393bcee6ce312767b.png"
    ></figure>
<p>under surveillance. Third, VAD systems should be designed with privacy-preserving techniques, such as immediate data anonymization or the use of low-resolution data that can detect anomalies without identifying individuals. By implementing these measures, we can work towards harnessing VAD technology benefits while respecting individual privacy and civil rights.</p>
<p>In addition to technical safeguards, it is also necessary to consider regulatory and oversight mechanisms to ensure responsible deployment. We recommend that VAD systems be subject to civilian oversight, where independent authorities evaluate their use, especially in sensitive contexts like law enforcement or public monitoring. Such oversight would help prevent potential misuse, ensuring that VAD systems are applied in ways that benefit society without compromising human rights. Furthermore, restrictions should be placed on VAD deployment for purposes other than public safety, with guidelines that limit its use to specific cases where the benefits clearly outweigh the risks. These guidelines could include requiring legal authorization for certain VAD uses, particularly in private spaces or in applications that extend beyond standard anomaly detection use-cases.</p>

<h2 class="relative group">7 Conclusion
    <div id="7-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#7-conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We propose a simple yet highly effective attribute-based method for video anomaly detection (VAD). Our method represents each object in each frame using velocity and pose representations and uses density estimation to compute anomaly scores. These simple representations are sufficient to achieve state-of-the-art performance on the ShanghaiTech and Ped2 datasets. By combining attribute-based representations with implicit deep representations, we achieve top VAD performance with AUROC scores of 99.1%, 93.7%, and 85.9% on Ped2, Avenue, and ShanghaiTech, respectively. Our extensive ablation study highlights the relative merits of the three representations. Overall, our method is both accurate and easy to implement.</p>

<h2 class="relative group">Acknowledgment
    <div id="acknowledgment" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#acknowledgment" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This research was partially supported by funding from the Israeli Science Foundation and the KLA Corporation. Tal Reiss is supported by the Google Fellowship and the Israeli Council for Higher Education.</p>

<h2 class="relative group">References
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>
<p>Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, volume 2, pp. 4, 2021.</p>
</li>
<li>
<p>Ruichu Cai, Hao Zhang, Wen Liu, Shenghua Gao, and Zhifeng Hao. Appearance-motion memory consistency network for video anomaly detection. In AAAI, 2021.</p>
</li>
<li>
<p>Congqi Cao, Yue Lu, Peng Wang, and Yanning Zhang. A new comprehensive benchmark for semi-supervised video anomaly detection and anticipation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20392–20401, 2023.</p>
</li>
<li>
<p>Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9650–9660, 2021.</p>
</li>
<li>
<p>Yunpeng Chang, Zhigang Tu, Wei Xie, and Junsong Yuan. Clustering driven deep autoencoder for video anomaly detection. In European Conference on Computer Vision, pp. 329–345. Springer, 2020.</p>
</li>
<li>
<p>Rizwan Chaudhry, Avinash Ravichandran, Gregory Hager, and René Vidal. Histograms of oriented optical flow and binet-cauchy kernels on nonlinear dynamical systems for the recognition of human actions. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1932–1939. IEEE, 2009.</p>
</li>
<li>
<p>Dongyue Chen, Pengtao Wang, Lingyi Yue, Yuxin Zhang, and Tong Jia. Anomaly detection in surveillance video based on bidirectional prediction. Image and Vision Computing, 98:103915, 2020.</p>
</li>
<li>
<p>Rensso Victor Hugo Mora Colque, Carlos Caetano, Matheus Toledo Lustosa de Andrade, and William Robson Schwartz. Histograms of optical flow orientation and magnitude and entropy to detect anomalous events in videos. IEEE Transactions on Circuits and Systems for Video Technology, 27(3):673–682, 2016.</p>
</li>
<li>
<p>Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE international conference on computer vision, pp. 1422–1430, 2015.</p>
</li>
<li>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p>
</li>
<li>
<p>Eleazar Eskin, Andrew Arnold, Michael Prerau, Leonid Portnoy, and Sal Stolfo. A geometric framework for unsupervised anomaly detection. In Applications of data mining in computer security, pp. 77–101. Springer, 2002.</p>
</li>
<li>
<p>Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. RMPE: Regional multi-person pose estimation. In ICCV, 2017.</p>
</li>
<li>
<p>Xinyang Feng, Dongjin Song, Yuncong Chen, Zhengzhang Chen, Jingchao Ni, and Haifeng Chen. Convolutional transformer based dual discriminator generative adversarial networks for video anomaly detection. Proceedings of the 29th ACM International Conference on Multimedia, 2021a.</p>
</li>
<li>
<p>Xinyang Feng, Dongjin Song, Yuncong Chen, Zhengzhang Chen, Jingchao Ni, and Haifeng Chen. Convolutional transformer based dual discriminator generative adversarial networks for video anomaly detection. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 5546–5554, 2021b.</p>
</li>
<li>
<p>Mariana-Iuliana Georgescu, Antonio Bărbălău, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, and Mubarak Shah. Anomaly detection in video via self-supervised and multi-task learning. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12737–12747, 2021a.</p>
</li>
<li>
<p>Mariana Iuliana Georgescu, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, and Mubarak Shah. A background-agnostic framework with adversarial training for abnormal event detection in video. IEEE transactions on pattern analysis and machine intelligence, 44(9):4505–4523, 2021b.</p>
</li>
<li>
<p>Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.</p>
</li>
<li>
<p>Michael Glodek, Martin Schels, and Friedhelm Schwenker. Ensemble gaussian mixture models for probability density estimation. Computational Statistics, 28(1):127–138, 2013.</p>
</li>
<li>
<p>Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1705–1714, 2019.</p>
</li>
<li>
<p>Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K Roy-Chowdhury, and Larry S Davis. Learning temporal regularity in video sequences. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 733–742, 2016a.</p>
</li>
<li>
<p>Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K Roy-Chowdhury, and Larry S Davis. Learning temporal regularity in video sequences. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 733–742, 2016b.</p>
</li>
<li>
<p>Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961–2969, 2017.</p>
</li>
<li>
<p>Or Hirschorn and Shai Avidan. Normalizing flows for human pose anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 13545–13554, October 2023.</p>
</li>
<li>
<p>Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2462–2470, 2017.</p>
</li>
<li>
<p>Radu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana Georgescu, and Ling Shao. Object-centric autoencoders and dummy anomalies for abnormal event detection in video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7842–7851, 2019.</p>
</li>
<li>
<p>Ian Jolliffe. Principal component analysis. Springer, 2011.</p>
</li>
<li>
<p>Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.</p>
</li>
<li>
<p>Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic colorization. In ECCV, 2016.</p>
</li>
<li>
<p>Longin Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier detection with kernel density functions. In International Workshop on Machine Learning and Data Mining in Pattern Recognition, pp. 61–75. Springer, 2007.</p>
</li>
<li>
<p>Sangmin Lee, Hak Gu Kim, and Yong Man Ro. Stan: Spatio-temporal adversarial networks for abnormal event detection. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 1323–1327. IEEE, 2018.</p>
</li>
<li>
<p>Sangmin Lee, Hak Gu Kim, and Yong Man Ro. Bman: bidirectional multi-scale aggregation networks for abnormal event detection. IEEE Transactions on Image Processing, 29:2395–2408, 2019.</p>
</li>
<li>
<p>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740–755. Springer, 2014.</p>
</li>
<li>
<p>Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Future frame prediction for anomaly detection–a new baseline. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6536–6545, 2018a.</p>
</li>
<li>
<p>Yusha Liu, Chun-Liang Li, and Barnabás Póczos. Classifier two-sample test for video anomaly detections. In BMVC, 2018b.</p>
</li>
<li>
<p>Zhian Liu, Yongwei Nie, Chengjiang Long, Qing Zhang, and Guiqing Li. A hybrid video anomaly detection framework via memory-augmented flow reconstruction and flow-guided frame prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 13588–13597, October 2021a.</p>
</li>
<li>
<p>Zhian Liu, Yongwei Nie, Chengjiang Long, Qing Zhang, and Guiqing Li. A hybrid video anomaly detection framework via memory-augmented flow reconstruction and flow-guided frame prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13588–13597, 2021b.</p>
</li>
<li>
<p>David G Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2):91–110, 2004.</p>
</li>
<li>
<p>Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event detection at 150 fps in matlab. In Proceedings of the IEEE international conference on computer vision, pp. 2720–2727, 2013.</p>
</li>
<li>
<p>Yiwei Lu, K Mahesh Kumar, Seyed shahabeddin Nabavi, and Yang Wang. Future frame prediction using convolutional vrnn for anomaly detection. In 2019 16Th IEEE international conference on advanced video and signal based surveillance (AVSS), pp. 1–8. IEEE, 2019.</p>
</li>
<li>
<p>Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse coding based anomaly detection in stacked rnn framework. In Proceedings of the IEEE international conference on computer vision, pp. 341–349, 2017a.</p>
</li>
<li>
<p>Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse coding based anomaly detection in stacked rnn framework. In Proceedings of the IEEE international conference on computer vision, pp. 341–349, 2017b.</p>
</li>
<li>
<p>Hui Lv, Chen Chen, Zhen Cui, Chunyan Xu, Yong Li, and Jian Yang. Learning normal dynamics in videos with meta prototype network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15425–15434, June 2021.</p>
</li>
<li>
<p>Vijay Mahadevan, Weixin Li, Viral Bhalodia, and Nuno Vasconcelos. Anomaly detection in crowded scenes. In 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 1975–1981. IEEE, 2010.</p>
</li>
<li>
<p>Amir Markovitz, Gilad Sharir, Itamar Friedman, Lihi Zelnik-Manor, and Shai Avidan. Graph embedded pose clustering for anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10539–10547, 2020.</p>
</li>
<li>
<p>Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. ICLR, 2016.</p>
</li>
<li>
<p>Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuffle and learn: unsupervised learning using temporal order verification. In European conference on computer vision, pp. 527–544. Springer, 2016.</p>
</li>
<li>
<p>Romero Morais, Vuong Le, Truyen Tran, Budhaditya Saha, Moussa Mansour, and Svetha Venkatesh. Learning regularity in skeleton trajectories for anomaly detection in videos. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11996–12004, 2019.</p>
</li>
<li>
<p>Trong-Nguyen Nguyen and Jean Meunier. Anomaly detection in video sequence with appearance-motion correspondence. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1273– 1283, 2019.</p>
</li>
<li>
<p>Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general video recognition. arXiv preprint, 2022.</p>
</li>
<li>
<p>Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016.</p>
</li>
<li>
<p>Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning memory-guided normality for anomaly detection. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 14360–14369, 2020.</p>
</li>
<li>
<p>Janez Perš, Vildana Sulić, Matej Kristan, Matej Perše, Klemen Polanec, and Stanislav Kovačič. Histograms of optical flow for efficient representation of body motion. Pattern Recognition Letters, 31(11):1369–1376, 2010.</p>
</li>
<li>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.</p>
</li>
<li>
<p>Tal Reiss and Yedid Hoshen. Mean-shifted contrastive loss for anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 2155–2162, 2023.</p>
</li>
<li>
<p>Tal Reiss, Niv Cohen, Liron Bergman, and Yedid Hoshen. Panda: Adapting pretrained features for anomaly detection and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2806–2814, 2021.</p>
</li>
<li>
<p>Tal Reiss, Niv Cohen, Eliahu Horwitz, Ron Abutbul, and Yedid Hoshen. Anomaly detection requires better representations. In European Conference on Computer Vision, pp. 56–68. Springer, 2022.</p>
</li>
<li>
<p>Tal Reiss, George Kour, Naama Zwerdling, Ateret Anaby-Tavor, and Yedid Hoshen. From zero to hero: Cold-start anomaly detection. arXiv preprint arXiv:2405.20341, 2024.</p>
</li>
<li>
<p>Nicolae-C Ristea, Florinel-Alin Croitoru, Radu Tudor Ionescu, Marius Popescu, Fahad Shahbaz Khan, Mubarak Shah, et al. Self-distilled masked auto-encoders are efficient video anomaly detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15984– 15995, 2024.</p>
</li>
<li>
<p>Nicolae-Cătălin Ristea, Neelu Madan, Radu Tudor Ionescu, Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B Moeslund, and Mubarak Shah. Self-supervised predictive convolutional attentive block for anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13576–13586, 2022.</p>
</li>
<li>
<p>Royston Rodrigues, Neha Bhargava, Rajbabu Velmurugan, and Subhasis Chaudhuri. Multi-timescale trajectory prediction for abnormal human activity detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2626–2634, 2020.</p>
</li>
<li>
<p>Bernhard Scholkopf, Robert C Williamson, Alex J Smola, John Shawe-Taylor, and John C Platt. Support vector method for novelty detection. In NIPS, 2000.</p>
</li>
<li>
<p>Chenrui Shi, Che Sun, Yuwei Wu, and Yunde Jia. Video anomaly detection via sequentially learning multiple pretext tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 10330–10340, October 2023.</p>
</li>
<li>
<p>Ashish Singh, Michael J Jones, and Erik G Learned-Miller. Eval: Explainable video anomaly localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18717–18726, 2023.</p>
</li>
<li>
<p>Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6479–6488, 2018.</p>
</li>
<li>
<p>Che Sun, Y. Jia, Yao Hu, and Y. Wu. Scene-aware context reasoning for unsupervised abnormal event detection in videos. Proceedings of the 28th ACM International Conference on Multimedia, 2020.</p>
</li>
<li>
<p>Shengyang Sun and Xiaojin Gong. Hierarchical semantic contrast for scene-aware video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22846– 22856, 2023.</p>
</li>
<li>
<p>Yao Tang, Lin Zhao, Shanshan Zhang, Chen Gong, Guangyu Li, and Jian Yang. Integrating prediction and reconstruction for anomaly detection. Pattern Recognition Letters, 129:123–130, 2020.</p>
</li>
<li>
<p>Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pp. 402–419. Springer, 2020.</p>
</li>
<li>
<p>Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602, 2022.</p>
</li>
<li>
<p>Guodong Wang, Yunhong Wang, Jie Qin, Dongming Zhang, Xiuguo Bao, and Di Huang. Video anomaly detection by solving decoupled spatio-temporal jigsaw puzzles. In European Conference on Computer Vision (ECCV), 2022.</p>
</li>
<li>
<p>Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14549–14560, 2023.</p>
</li>
<li>
<p>Xuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke Yang, Jian Tang, Jieping Ye, Jingyu Wang, and Qi Qi. Robust unsupervised video anomaly detection by multipath frame prediction. IEEE transactions on neural networks and learning systems, 2021.</p>
</li>
<li>
<p>Ziming Wang, Yuexian Zou, and Zeming Zhang. Cluster attention contrast for video anomaly detection. Proceedings of the 28th ACM International Conference on Multimedia, 2020.</p>
</li>
<li>
<p>Donglai Wei, Joseph J Lim, Andrew Zisserman, and William T Freeman. Learning and using the arrow of time. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8052–8060, 2018.</p>
</li>
<li>
<p>Peng Wu, jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. Not only look, but also listen: Learning multimodal violence detection under weak supervision. In European Conference on Computer Vision (ECCV), 2020.</p>
</li>
<li>
<p>Cheng Yan, Shiyu Zhang, Yang Liu, Guansong Pang, and Wenjun Wang. Feature prediction diffusion model for video anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5527–5537, October 2023.</p>
</li>
<li>
<p>Zhiwei Yang, Peng Wu, Jing Liu, and Xiaotao Liu. Dynamic local aggregation network with adaptive clusterer for anomaly detection. In European Conference on Computer Vision, pp. 404–421. Springer, 2022.</p>
</li>
<li>
<p>Zhiwei Yang, Jing Liu, Zhaoyang Wu, Peng Wu, and Xiaotao Liu. Video event restoration based on keyframes for video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14592–14601, 2023.</p>
</li>
<li>
<p>Muchao Ye, Xiaojiang Peng, Weihao Gan, Wei Wu, and Yu Qiao. Anopcn: Video anomaly detection via deep predictive coding network. In ACM MM, 2019.</p>
</li>
<li>
<p>Guang Yu, Siqi Wang, Zhiping Cai, En Zhu, Chuanfu Xu, Jianping Yin, and Marius Kloft. Cloze test helps: Effective video anomaly detection via learning to complete video events. In Proceedings of the 28th ACM International Conference on Multimedia, pp. 583–591, 2020.</p>
</li>
<li>
<p>Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.</p>
</li>
<li>
<p>Shoubin Yu, Zhongyin Zhao, Haoshu Fang, Andong Deng, Haisheng Su, Dongliang Wang, Weihao Gan, Cewu Lu, and Wei Wu. Regularity learning via explicit distribution modeling for skeletal video anomaly detection. arXiv preprint arXiv:2112.03649, 2021.</p>
</li>
<li>
<p>Menghao Zhang, Jingyu Wang, Qi Qi, Haifeng Sun, Zirui Zhuang, Pengfei Ren, Ruilong Ma, and Jianxin Liao. Multi-scale video anomaly detection by multi-grained spatio-temporal representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17385–17394, 2024.</p>
</li>
<li>
<p>Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.</p>
</li>
<li>
<p>Yiru Zhao, Bing Deng, Chen Shen, Yao Liu, Hongtao Lu, and Xian-Sheng Hua. Spatio-temporal autoencoder for video anomaly detection. In ACM MM, 2017.</p>
</li>
</ul>

<h2 class="relative group">A More Qualitative Results
    <div id="a-more-qualitative-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-more-qualitative-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We provide more qualitative results of our methods over the evaluation datasets.</p>
<p>In Ped2, Fig. 6 and Fig. 7 demonstrate the effectiveness of our method, which can easily detect fast-moving objects such as trucks and bicycles. Accordingly, we can conclude that Ped2 has been practically solved based on the near-perfect results obtained by our method (as well as many others). Fig. 8 shows that our method is capable of detecting anomalies within a short timeframe. Fig. 9 and Fig. 10 provide more qualitative information regarding our method&rsquo;s ability to detect anomalies of various types. In this way, our method achieves a new state-of-the-art in Avenue and ShanghaiTech, surpassing other approaches by a wide margin.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000005_d699576706d021107af51ea8d1392b16046f3811670e48790376887f3ce270cf.png"
    ></figure>
<p>Figure 6: Frame-level scores and anomaly localization examples for test video 04 from Ped2.</p>
<p>Figure 7: Frame-level scores and anomaly localization examples for test video 05 from Ped2.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000006_9c3017cba714ae3e632c0be53514474348cbc3653cf9e41eff778656ed75718f.png"
    ></figure>
<p>Figure 8: Frame-level scores and anomaly localization examples for test video 03 from Avenue.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000007_49cd691244f14267b3e0e1e536f4eee006f0ca84e98027fa84cc8dd7212e7cb9.png"
    ></figure>
<p>Figure 9: Frame-level scores and anomaly localization examples for test video 01_0025 from ShanghaiTech.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000008_deaa64f81a4e4c55264bd8db14c3c219cf349c954f2ac99c48369fbfe71ff376.png"
    ></figure>
<p>Figure 10: Frame-level scores and anomaly localization examples for test video 07_0048 from ShanghaiTech.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000009_bc7d509f8d08be12d334ce553cc79f7280daab5e91edc42e355ac0dc22f5967d.png"
    ></figure>

<h2 class="relative group">B More Analysis &amp; Discussion
    <div id="b-more-analysis--discussion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-more-analysis--discussion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Per-scene breakdown. In Tab. 9, we present the per-scene performance of our method on the ShanghaiTech dataset, which is the only dataset among the three benchmarks that includes multiple scenes. The results demonstrate that our method performs consistently well across most scenes, with a few exceptions. Specifically, scenes 8, 9, and 10 demonstrate lower performance compared to others. These scenes (i.e., videos with the prefix 08_<strong>, 09_</strong>, 10_**) feature anomalies involving complex activities such as erratic jumping, throwing objects, and pushing people, as well as frequent occlusions. Such activities involve high</p>
<p>Table 9: ShanghaiTech per-scene frame-level micro AUROC (%) results.</p>
<table>
  <thead>
      <tr>
          <th>Scene Number</th>
          <th>Total Test Frames</th>
          <th>Total Test Anomalies</th>
          <th>Anomaly Ratio</th>
          <th>AUROC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>01</td>
          <td>11, 894</td>
          <td>4, 884</td>
          <td>0.41</td>
          <td>88.2</td>
      </tr>
      <tr>
          <td>02</td>
          <td>1, 155</td>
          <td>662</td>
          <td>0.57</td>
          <td>87.8</td>
      </tr>
      <tr>
          <td>03</td>
          <td>4, 090</td>
          <td>1, 212</td>
          <td>0.29</td>
          <td>90.8</td>
      </tr>
      <tr>
          <td>04</td>
          <td>4, 761</td>
          <td>1, 874</td>
          <td>0.39</td>
          <td>87</td>
      </tr>
      <tr>
          <td>05</td>
          <td>4, 160</td>
          <td>1, 016</td>
          <td>0.24</td>
          <td>94.6</td>
      </tr>
      <tr>
          <td>06</td>
          <td>1, 470</td>
          <td>702</td>
          <td>0.47</td>
          <td>94.5</td>
      </tr>
      <tr>
          <td>07</td>
          <td>3, 368</td>
          <td>886</td>
          <td>0.26</td>
          <td>92.7</td>
      </tr>
      <tr>
          <td>08</td>
          <td>3, 708</td>
          <td>1, 992</td>
          <td>0.53</td>
          <td>67.8</td>
      </tr>
      <tr>
          <td>09</td>
          <td>361</td>
          <td>84</td>
          <td>0.23</td>
          <td>72.6</td>
      </tr>
      <tr>
          <td>10</td>
          <td>2, 213</td>
          <td>1, 539</td>
          <td>0.69</td>
          <td>64.1</td>
      </tr>
      <tr>
          <td>11</td>
          <td>337</td>
          <td>141</td>
          <td>0.41</td>
          <td>99.3</td>
      </tr>
      <tr>
          <td>12</td>
          <td>3, 74</td>
          <td>2, 334</td>
          <td>0.71</td>
          <td>81.1</td>
      </tr>
  </tbody>
</table>
<p>levels of motion and interaction between multiple subjects, which likely challenges the velocity-based feature representations, leading to reduced performance.</p>
<p>What are the benefits of pretrained features? Previous anomaly detection works (Reiss et al., 2021; Reiss &amp; Hoshen, 2023; Reiss et al., 2022; 2024) demonstrated that using feature extractors pretrained on external, generic datasets (e.g. ResNet on ImageNet classification) achieves high anomaly detection performance. This was demonstrated on a large variety of datasets across sizes, domains, resolutions, and symmetries. These representations achieved state-of-the-art performance on distant domains, such as aerial, microscopy, and industrial images. As the anomalies in these datasets typically had nothing to do with velocity or human pose, it is clear the pretrained features model many attributes beyond velocity and pose. Consequently, by combining our attribute-based representations with CLIP&rsquo;s image encoder, we are able to emphasize both explicit attributes (velocity and pose) derived from real-world priors and attributes that cannot be described by them, allowing us to achieve the best of both worlds.</p>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/An Attribute-based Method for Video Anomaly Detection.md"
          data-oid-likes="likes_papers/An Attribute-based Method for Video Anomaly Detection.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/anomaly-led_prompting_learning_caption_generating_model_and_benchmark/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/aligning-effective-tokens-with-video-anomaly-in-large-language-models/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
