<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/cross-domain-learning-for-vad-with-limited-supervision/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/cross-domain-learning-for-vad-with-limited-supervision/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/cross-domain-learning-for-vad-with-limited-supervision\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "9101"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>9101 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">43 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">Cross-Domain Learning for Video Anomaly Detection with Limited Supervision
    <div id="cross-domain-learning-for-video-anomaly-detection-with-limited-supervision" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#cross-domain-learning-for-video-anomaly-detection-with-limited-supervision" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Yashika Jain University of Delhi <a
  href="mailto:yashikajain201@gmail.com">yashikajain201@gmail.com</a></p>
<p>Ali Dabouei * Carnegie Mellon University <a
  href="mailto:ali.dabouei@gmail.com">ali.dabouei@gmail.com</a></p>

<h2 class="relative group">Abstract
    <div id="abstract" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#abstract" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video Anomaly Detection (VAD) automates the identification of unusual events, such as security threats in surveillance videos. In real-world applications, VAD models must effectively operate in cross-domain settings, identifying rare anomalies and scenarios not well-represented in the training data. However, existing cross-domain VAD methods focus on unsupervised learning, resulting in performance that falls short of real-world expectations. Since acquiring weak supervision, i.e., video-level labels, for the source domain is cost-effective, we conjecture that combining it with external unlabeled data has notable potential to enhance crossdomain performance. To this end, we introduce a novel weakly-supervised framework for Cross-Domain Learning (CDL) in VAD that incorporates external data during training by estimating its prediction bias and adaptively minimizing that using the predicted uncertainty. We demonstrate the effectiveness of the proposed CDL framework through comprehensive experiments conducted in various configurations on two large-scale VAD datasets: UCF-Crime and XD-Violence. Our method significantly surpasses the stateof-the-art works in cross-domain evaluations, achieving an average absolute improvement of 19.6% on UCF-Crime and 12.87% on XD-Violence.</p>

<h2 class="relative group">1. Introduction
    <div id="1-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video anomaly detection (VAD) aims to locate anomalous events in the videos [3 , 10 , 11 , 15 , 21 , 25 , 32 , 33 , 42 , 47]. Unlike manual surveillance, which is costly and timeconsuming, video anomaly detection eliminates the need for extensive human effort, saving resources and time. It holds significant potential for playing a vital role in video surveillance by identifying unusual behaviors and activities such as accidents, burglaries, explosions, and other events that signal security threats.</p>
<p>VAD has been extensively studied previously [11 , 15 , 21 , 32 , 33 , 47]. Owing to the high costs and time associated</p>
<ul>
<li>Corresponding authors.</li>
</ul>
<p>Min Xu * Carnegie Mellon University <a
  href="mailto:mxu1@cs.cmu.edu">mxu1@cs.cmu.edu</a></p>
<p>Figure 1. Anomaly score comparison on a video of XD-Violence dataset, with and without employing the proposed CDL framework. The model trained without CDL on UCF-Crime as the weakly labeled set consistently yields high anomaly scores. In contrast, the model trained with CDL, using UCF-Crime as the weakly labeled set and HACS as the unlabeled set, is better able to localize the anomalous frames.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_fe82504e23b8e679a7fdaf12e289294b101534f3969917d8ef9798e92f0d11a4.png"
    ></figure>
<p>with obtaining frame-level labels, most approaches formulate the problem as either an unsupervised [10 , 15 , 21] or weakly-supervised learning setup [11 , 32 , 33]. In the unsupervised or one-class classification-based) learning setup, only normal videos are used to model the underlying distribution of normal spatiotemporal patterns, and any deviations from the modeled distribution are regarded as anomalies. Despite the convenience of the unsupervised setup, the lack of anomalous videos during training limits the model&rsquo;s ability to learn the specific characteristics of anomalies. This results in limited performance which does not meet real-world expectations. To address this issue, weaklysupervised setup has attracted significant attention. In this setup, merely video-level labels indicating the presence of anomalies within the videos are incorporated as weak supervision to train models capable of making frame-level predictions at inference. Multiple Instance Learning (MIL) [32] is a prominent technique in this domain. By treating each video as a &ldquo;bag&rdquo; and each snippet as a &ldquo;segment&rdquo;, MIL-based algorithms operate under the premise of a worstcase scenario where the segment with the highest predicted probability of being abnormal is considered as the candidate to represent the whole video.</p>
<p>In real-world applications, it is inevitable to encounter environments and scenarios not fully represented in the model&rsquo;s training set. However, it is essential that the model makes correct predictions in such novel situations. For in- stance, when the training data lacks samples of rare events like &ldquo;riots&rdquo; or accidents in novel scenes, the model should be able to characterize such occurrences as anomalous when they occur. Previous works study these novel situations under the cross-domain problem definition [3 , 13 , 23].</p>
<p>Existing cross-domain VAD methods [3 , 13 , 23 , 25] rely on unsupervised techniques and consequently exhibit limited performance, as demonstrated later in our empirical evaluations in Tables 2 and 3. A solution to this could be the adoption of weakly-supervised techniques for crossdomain VAD. While weakly-supervised approaches have proven promising in single-domain scenarios [11 , 32 , 33], their effectiveness in cross-domain scenarios has not been extensively explored. Our evaluations in Tables 2 and 3 suggest that directly employing existing weakly-supervised methods to address the cross-domain challenges results in a significant performance drop when tested in scenarios of even similar nature, such as surveillance videos. We argue that this performance gap is due to the following reasons. First, anomalous events, by their very nature, lack a specific pattern or predefined structure. Hence, the definition of anomaly is context-dependent and a naive adaptation of the previous method cannot capture the context-dependencies in multiple domains. Second, anomalous events are relatively infrequent, making VAD a class imbalance problem. This issue becomes more severe when dealing with multiple domains. Third, because of the limited amount of weakly labeled training data, the model&rsquo;s learning capacity to detect novel (open-set) anomalies is also constrained. Due to these challenges, weakly-supervised methods cannot be readily applied to cross-domain or cross-dataset scenarios.</p>
<p>To overcome these challenges and develop a generalized VAD model, substantial amounts of weakly-labeled data are required. However, acquiring even video-level labels for a large number of videos is inefficient and labor-intensive. On the other hand, vast streams of unlabeled videos are generally available. Utilizing the limited weakly-labeled data alongside this abundant unlabeled data provides a notable opportunity to address the aforementioned challenges in cross-domain VAD. Prudent utilization of the unlabeled data can provide valuable insights into the underlying data distribution, leading to improved decision-making and identification of anomalous events.</p>
<p>To this end, we propose a weakly-supervised CrossDomain Learning (CDL) framework for VAD that integrates external, unlabeled data, from the wild with limited weakly-labeled data to provide competitive generalization across the domains. This is achieved by adaptively minimizing the prediction bias over the external data using the estimated prediction variance, which serves as an uncertainty regularization score. In the proposed framework, we first train fine-grained pseudo-label generation models on the weakly-labeled data to obtain sets of segment-level pre-</p>
<p>Table 1. Brief overview of the taxonomy of current works for VAD using a source domain dataset (D) and a secondary domain dataset (D ′ ). All these methods do not utilize any labels for training on (D ′ ) and assume distinct distributions for D and D ′ .</p>
<table>
  <thead>
      <tr>
          <th>Method(s)</th>
          <th>Sup. on D</th>
          <th>Target</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Acsintoae et al. [1]</td>
          <td>unsupervised</td>
          <td>D</td>
      </tr>
      <tr>
          <td>rGAN [23], MPN [25]</td>
          <td>unsupervised</td>
          <td>D ′</td>
      </tr>
      <tr>
          <td>zxVAD [3]</td>
          <td>unsupervised</td>
          <td>D ∪ D′</td>
      </tr>
      <tr>
          <td>Ours</td>
          <td>weakly-supervised</td>
          <td>D ∪ D′</td>
      </tr>
  </tbody>
</table>
<p>dictions for the external dataset. Second, we compute the variance of the predictions across multiple predictors as a proxy to represent uncertainty associated with the segments in the external data. Third, during the optimization process, involving training on both labeled and external data, we adaptively reweigh the bias on each external data using the uncertainty regularization scores. This dynamic reweighing ensures that segments from the external dataset closer to the source dataset are emphasized during the training, while those with higher uncertainty are down-weighted. Finally, we iteratively regenerate pseudo-labels using the models trained on labeled and pseudo-labeled data, re-estimate the uncertainties, and re-train the model on the union of labeled and external datasets. This iterative process helps refine the pseudo-labels as the training progresses. With this training process, the model learns to generalize to both source and external data, given only supervision on the source data. Figure 1 illustrates the effectiveness of the CDL framework.</p>
<p>To summarize, we make the following contributions:</p>
<ul>
<li>We present a practical CDL framework for weaklysupervised VAD, in which unlabeled external videos are employed to enhance the cross-domain generalization of the model.</li>
<li>We design a novel uncertainty quantification method that enables the adaptive uncertainty-driven integration of external videos into the training set.</li>
<li>Through extensive experiments and ablation studies on benchmark datasets, we validate the proposed approach, demonstrating state-of-the-art performance in cross-domain settings while retaining a competitive performance on the in-domain data.</li>
</ul>

<h2 class="relative group">2. Related Works
    <div id="2-related-works" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-related-works" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video Anomaly Detection (VAD). Video Anomaly Detection (VAD). VAD is a well-established problem, with most works formulating it either as unsupervised learning [15 , 21 , 22 , 41 , 44] or weakly-supervised learning [29 , 32 , 33 , 43 , 48] problem. In unsupervised setups, the training data consists solely of normal videos, with the majority of works encoding normal patterns through techniques like frame reconstruction [15 , 39], future frame pre- diction [21], dictionary learning [22 , 44], and one-class classification [17 , 24]. Any deviation from the encoded patterns is considered anomalous. Since the model categorizes anything beyond its learned representations as anomalous, it can label novel video actions and scenarios encountered during training but in altered environments as anomalous. Weakly-supervised VAD methods help mitigate these issues by incorporating video-level labels as weak supervision for the model, with the majority of methods utilizing the Multiple Instance Ranking Loss [11 , 32 , 35 , 47]. Given that a VAD model is expected to encounter previously unseen scenarios during deployment, it is of paramount importance for the model to have a high generalization across domains. Previous works refer this as cross-domain [3] or cross-dataset generalization [9]. We provide an overview of the existing works employing external data in VAD in Table 1. Previous works on cross-domain generalization focus on unsupervised methods based on few-shot targetdomain scene adaptation. [23 , 25] employ data from the target domain via meta-learning to adapt to that specific domain. Aich et al. [3] proposed a zero-shot target domain adaptation method that incorporates external data to generate pseudo-abnormal frames. Despite the intriguing setup, these unsupervised cross-domain generalization methods lack explicit knowledge about what constitutes an anomaly, hindering the model&rsquo;s ability to learn the specific characteristics of anomalies. To this end, we propose the use of weakly-supervised learning for cross-domain generalization. We integrate external datasets from diverse domains to enable the cross-domain generalization of a model trained in a weakly-supervised fashion.</p>
<p>Pseudo-Labeling and Self-training. Pseudo-labeling [4 , 28] is a common technique where the model trained on labeled data assigns labels to unlabeled data. Subsequently, the model is trained on both the initially labeled data and the pseudo-labeled data. This self-training strategy [26 , 40] operates iteratively, allowing the model to progressively enhance its generalization. In VAD, several works leverage pseudo-labeling and self-training for generating finegrained pseudo-labels [11 , 20 , 42]. However, in contrast to the previous methods, instead of generating pseudo-labels for the weakly labeled data, we leverage pseudo-labels for incorporating the external data.</p>
<p>Uncertainty Estimation. To address pseudo-label noise, prior research in different contexts has explored uncertainty estimation using various approaches, such as data augmentation [5 , 30], inference augmentation [12], and model augmentation [46]. While data augmentation is effective for images, it can disrupt temporal relationships in video frames and is not efficient for training on high-cardinality data like videos. On the other hand, inference augmentation methods, such as MC Dropout [12 , 42], introduce perturbations during model inference to obtain slightly dif- ferent predictions, but that is inefficient for training with fixed backbones. In contrast, model augmentation uses different models. Since different models may have varying biases and receptive fields, this would result in diverse predictions. This prediction discrepancy can help quantify uncertainty, making model augmentation well-aligned with our problem. To avoid any manual thresholding for learning from pseudo-labels during training, following [16 , 46] we use adaptive reweighing of loss with uncertainty values. In [46], Zheng et al. quantify uncertainty by estimating discrepancies between predictions made by two classifiers using Kullback–Leibler (KL) divergence. However, given that VAD is a binary classification task, the divergence based on only two outcomes for the posterior probability is not optimally informative. Hence, we propose a method to quantify uncertainty in the high-dimensional feature space instead of the probability space.</p>

<h2 class="relative group">3. Method
    <div id="3-method" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-method" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">3.1. Problem Definition
    <div id="31-problem-definition" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#31-problem-definition" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this work, we address a real-world VAD problem, where a weakly-labeled dataset Dl = {(X i l , Y
l i Y
l )} nl i=1 and an external unlabeled dataset D u = {X i u } n u i=1 are available for training. Here, nl and n u indicate the number of videos in the two datasets, respectively, with n u ≫ nl due to the convenience of gathering unlabeled video data. The videolevel labels of Xl are denoted by Yl ∈ {0 , 1}. We do not make any assumption about distributions of Dl and D u , and therefore, they can be drawn from different distributions. We aim to find the model F(·|θ), parameterized by θ, that provides accurate predictions on weakly-labeled data while adaptively minimizing the prediction bias on the external data using the uncertainty regularization scores. We illustrate the proposed framework in Figure 2 .</p>

<h2 class="relative group">3.2. Feature Extraction and Temporal Processing
    <div id="32-feature-extraction-and-temporal-processing" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#32-feature-extraction-and-temporal-processing" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The proposed uncertainty quantification method (Section 3.4) compares two diverse representations of each sample to estimate the uncertainty associated with the segmentlevel predictions on external data. To this aim, we employ two different backbones for feature extraction from videos, which are widely used for anomaly detection tasks. The first one is the conventional I3D backbone [6], which extracts segment-level features using 3D convolution, and the other is the CLIP backbone [27], which extracts frame-level features using the frozen CLIP Model&rsquo;s ViT encoder. The contrasting inductive biases of the 3D convolution-based I3D and the transformer-based CLIP help to effectively capture the prediction variance. It is to be noted that only the CLIP backbone is used during inference. We develop two prediction heads, namely the main model, Pm Pm , built on top of the CLIP backbone, and the auxiliary model, Pa Pa , built on top</p>
<p>Figure 2. Overview of the proposed CDL Framework. CDL Step 0: The Ranking Loss, Lrank (Supp Mat. §6), is employed to train two pseudo-label generation models, Pm Pm and Pa Pa , §3.2, on weakly-labeled data, Dl . CDL Step k, k &gt; 0: Pm Pm and Pa Pa are trained iteratively on Dl ∪ D u , incorporating pseudo-labels for D u generated at the end of the previous CDL step. To deal with noise in pseudo-labels, uncertainty regularization scores are estimated using the divergence between the predictions of the two models, §3.4. When optimizing on D u , the prediction bias, Lbce (§3.3), for external data is reweighed using the computed uncertainty regularization scores, §3.5 .</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_4d81f0506afc6911170bc824a502a678b6412b224079e89bcaa7905b68473da0.png"
    ></figure>
<p>of the I3D backbone.</p>
<p>Video frames are highly correlated in the temporal dimension. To reduce the redundancy in frame-level features extracted by the CLIP backbone, we pool the representations by bilinearly interpolating them to a fixed, empirically determined length, n s . Each of the n s interpolated features represents one segment. To ensure consistency, we also fix the length of representations extracted by the I3D backbone. Evaluation in Section 4.6 analyzes the role of n s on the model&rsquo;s performance. To capture long-range temporal information over the sequence, we employ a lightweight temporal network, i.e., transformer encoder, to implement Pm Pm and Pa Pa.</p>

<h2 class="relative group">3.3. Bias Estimation for External Data
    <div id="33-bias-estimation-for-external-data" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#33-bias-estimation-for-external-data" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Similar to [46], we formulate the prediction bias on external data as:</p>
<!-- formula-not-decoded -->
<p>where F(X u |θ)represents a set of predicted probability distributions, each one corresponding to a distinct segment of X u , and Yu Yu denotes the set of unknown segment-level labels of X u. Bias(D u ) can be re-written as:</p>
<!-- formula-not-decoded -->
<p>where Y ˆ u denotes the set of segment-level pseudo-labels for X u. Y ˆ u can be generated by performing inference on the model trained on D l . The first term in Equation 2 denotes the difference between the predicted posterior probability and the pseudo-labels, while the second term denotes the error between the pseudo-labels and the ground-truth labels. While minimizing the prediction bias, due to the lack of ground truth supervision, we employ a self-training mechanism, considering Y ˆ u as the soft labels, thereby treating the second term as a constant and minimizing the first term. Specifically, we use the binary cross-entropy (BCE) loss, Lbce, given by:</p>
<!-- formula-not-decoded -->
<p>to estimate the prediction bias associated with each video segment, for both Pm Pm and Pa Pa.</p>

<h2 class="relative group">3.4. Uncertainty Estimation
    <div id="34-uncertainty-estimation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#34-uncertainty-estimation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Since D u and D l do not necessarily share the same distribution, the generated pseudo-labels are noisy. This noise can adversely affect the subsequent training process as it causes bias to further magnify and propagate within the model. This issue, known as Confirmation Bias [4], is often mitigated by quantifying the uncertainty associated with pseudo-labels and then incorporating this uncertainty into the training process to compensate for the noise. As discussed in Section 2, we opt to address the confirmation bias by computing uncertainty using model augmentation. To quantify uncertainty through model augmentation, following [46], we estimate prediction variance, which is formulated as:</p>
<!-- formula-not-decoded -->
<p>Due to the lack of ground-truth labels, Equation 4 can be approximated as:</p>
<!-- formula-not-decoded -->
<p>When optimizing the prediction bias in Equation 2, the variance in Equation 5 will also be minimized, potentially re- sulting in inaccurate quantification of the true prediction variance. To address this, we adopt an alternative approximation, expressed as:</p>
<!-- formula-not-decoded -->
<p>Since VAD is a binary classification task, the probability distributions corresponding to each segment have limited support. Consequently, estimating prediction variance using only the predicted anomaly scores, as in Equation 6 , may not be robust. Hence, instead of measuring the divergence between the predicted posterior probabilities for the two classes, we propose quantifying pseudo-label uncertainty in the high-dimensional space. To this end, we compute the cosine similarity between the segments in each set of the representations, Zm Zm and Z a , obtained from the penultimate layer of Pm Pm and Pa Pa , respectively. Here, Zm Zm = {z 1 m, z 2 m, . . . , z n s m } and Z a = {z 1 a
, z 2 a
, . . . , z n s a } .</p>
<p>To obtain a set of stabilized, segment-level uncertainty regularization scores within a bounded range from the computed cosine similarity, we introduce the following function. Let S = {s 1 , s 2 , . . . , s n s } be the set of surrogate variances that we use as proxies for the uncertainty of segments. The surrogate variance is computed as:</p>
<!-- formula-not-decoded -->
<p>where s
j
indicates the uncertainty regularization score for the j th segment, ⟨z j m, z j a ⟩ indicates the cosine similarity, and τ denotes the temperature parameter.</p>
<p>Higher uncertainty regularization scores indicate the similar encoding of data between the models, implying less uncertainty in the predicted labels, while, lower scores imply high uncertainty in the predicted labels. Empirical evidence in Section 4.4 demonstrates a significant negative correlation between uncertainty regularization scores and Binary Cross-Entropy (BCE) loss between the predicted labels and ground truths. This affirms that the proposed uncertainty regularization score effectively serves as a proxy for the quality of pseudo-labels.</p>

<h2 class="relative group">3.5. Training Process
    <div id="35-training-process" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#35-training-process" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>CDL Step 0. We initially train Pm Pm and Pa Pa separately on the labeled set, optimizing both of them using the Ranking Loss, Lrank, discussed in Supp. Mat. Sec. 6. We then perform inference on the trained models to generate the sets of soft segment-level pseudo-labels for training on D u.</p>
<p>CDL Step &gt; 0. Following the generation of the sets of pseudo-labels for D u , we enter an iterative pseudo-label refinement phase, where we train Pm Pm and Pa Pa on D l ∪ D u for multiple CDL steps. Each CDL step comprises a fixed number of epochs. In each epoch, we regenerate the sets of segment-level uncertainty regularization scores. To enable the uncertainty-driven learning from external data, similar to [46], we use the estimated uncertainty regularization scores, S, as automatic thresholds as this dynamically adjusts learning from noisy labels by scaling the prediction bias associated with external data based on S. This helps filter out unreliable predictions while prioritizing highly confident predictions. To encourage lower prediction variance, which would in turn lead to increased pseudo-label quality, we explicitly add the prediction variance to the optimization objective corresponding to the external data, Lext, as:</p>
<!-- formula-not-decoded -->
<p>Equation 8 is rewritten with the approximated terms as:</p>
<!-- formula-not-decoded -->
<p>Alternatively, Equation 9 can be rewritten as:</p>
<!-- formula-not-decoded -->
<p>where λ 3 is a hyper-parameter to balance the losses. Similar to CDL step 0, to optimize the training on Dl, we use Lrank . The total optimization objective for training on Dl ∪D u can be expressed as:</p>
<!-- formula-not-decoded -->
<p>where λ 4 is a trade-off parameter for Lext. We employ the optimization objective defined in Equation 11 during training on Dl ∪ D u for each epoch within every CDL step. After each CDL step is completed, we re-generate the set of soft segment-level pseudo-labels using the models trained on D l ∪ D u . This iterative refinement process repeats k times, where k is a hyper-parameter determining the number of CDL steps. With each CDL step, the models&rsquo; performance gets further refined as the pseudo-labels get iteratively improved.</p>

<h2 class="relative group">3.6. Inference - Extending Segment-level Scores to Frame-level Scores
    <div id="36-inference---extending-segment-level-scores-to-frame-level-scores" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#36-inference---extending-segment-level-scores-to-frame-level-scores" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>During inference, we compute segment-level anomaly scores for the videos using Pm Pm . Since we encounter longuntrimmed videos with varying numbers of frames, for extending the segment-level anomaly score to the frame level, for each video, we divide the total number of frames nf by the number of segments n s to obtain the number of frames per segment, nf s. We assign the anomaly score of each segment to its consecutive frames. The first segment corresponds to the first nf s frames, and so forth until the (n s − 1) th segment. For the last segment, its anomaly score is assigned to any remaining frames, potentially exceeding n f s , if there is a remainder.</p>

<h2 class="relative group">4. Experiments
    <div id="4-experiments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#4-experiments" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We evaluate the proposed method on the major video anomaly datasets, UCF-Crime (UCF) [32] and XDViolence (XDV) [38]. Additionally, we use 11,000 videos from the HACS [45] dataset as a source of external data. We provide detailed information about the datasets in Supp. Mat. §7. In §4.1, we discuss the implementation details. In §4.2, we discuss the inherent noise in the test annotations of benchmark datasets. We proceed to compare the proposed framework with prior works in cross-domain scenarios (§4.3.1) and open-set scenarios (§4.3.2). Subsequently, in §4.4, we demonstrate a strong correlation between the quality of pseudo labels and the computed uncertainty scores. We then explore the evolution of these uncertainty scores through the training process in §4.5. Finally, in §4.6, we conduct ablation studies and hyper-parameter analysis to analyze the impact of individual components of the proposed framework.</p>

<h2 class="relative group">4.1. Implementation Details
    <div id="41-implementation-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#41-implementation-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We implement the proposed method using PyTorch. We extract CLIP and I3D features at a fixed frame rate of 30 FPS. CLIP features are extracted from the frozen CLIP model&rsquo;s image encoder (ViT-B/32). For the hyper-parameters, in the open-set scenarios, we empirically set the value of n s to 64, τ to 1.25, λ1 and λ2 to 5e − 4 , λ 3 to 1e − 3, and λ4 to 700. Ablation studies for selecting n s and λ 3 are included in Section 4.6. We use the Adam optimizer with a weight decay of 1e − 3, and we set a learning rate of 3e − 5 for the transformer encoder and 5e − 4 for the fully connected layers. We use a batch size of 64. In both Pm Pm and Pa Pa , we explicitly encode positional information in the segments using sinusoidal positional encodings [34]. We train on the weaklylabeled source dataset for 200 epochs, followed by training on the union of weakly-labeled and external datasets for 40 CDL steps, each CDL step comprising 4 epochs. Additional information regarding hyper-parameters is provided in Supp. Material Section 8.</p>
<p>Model Architecture. Both Pm Pm and Pa Pa consist of a transformer encoder layer with four heads, followed by four fully connected layers, each consisting of 4096, 512, 32, and 1 neurons, respectively. In both the models, for all the layers except the last, we use ReLU [2] activation while for the last layer, we use Sigmoid activation.</p>
<p>Evaluation Setup. To reduce bias, we perform each experiment three times with different seeds and average the results. In open-set experiments, we repeat each experiment three times, using different sets of anomaly classes each time.</p>
<p>Evaluation Metric. Following previous works on UCFCrime [32], we adopt the frame-level area under the ROC curve (AUC) to evaluate on UCF-Crime. In line with previous works on XD-Violence [38], we use the frame-</p>
<p>Table 2. Comparison with prior works on XDV, considering UCFCrime as the source data. Asterisk (∗) indicates that evaluations were conducted by us using the official code. Dagger (†) indicates that evaluations were conducted by our implementation due to the lack of an official implementation.</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Methods</th>
          <th>Features</th>
          <th>UCF AUC(%)</th>
          <th>UCF-R AUC(%)</th>
          <th>XDV AP(%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Cross-Domain (Unsup.)</td>
          <td>rGAN [23]</td>
          <td>-</td>
          <td>64.35∗</td>
          <td>65.19∗</td>
          <td>37.74</td>
      </tr>
      <tr>
          <td>Cross-Domain (Unsup.)</td>
          <td>MPN [25]</td>
          <td>-</td>
          <td>65.67</td>
          <td>67.98∗</td>
          <td>38.89</td>
      </tr>
      <tr>
          <td>Cross-Domain (Unsup.)</td>
          <td>zxVAD [3]</td>
          <td>-</td>
          <td>68.74</td>
          <td>69.39</td>
          <td>40.68</td>
      </tr>
      <tr>
          <td>Non Cross</td>
          <td>Sultani et al.[32]</td>
          <td>I3D</td>
          <td>80.70</td>
          <td>84.63∗</td>
          <td>53.88</td>
      </tr>
      <tr>
          <td>Non</td>
          <td>MIST [11]</td>
          <td>I3D</td>
          <td>82.30</td>
          <td>86.17∗</td>
          <td>50.33</td>
      </tr>
      <tr>
          <td>Cross</td>
          <td>RTFM [33]</td>
          <td>I3D</td>
          <td>84.03</td>
          <td>86.47∗</td>
          <td>37.3</td>
      </tr>
      <tr>
          <td>Domain</td>
          <td>S3R [37]</td>
          <td>I3D</td>
          <td>85.99</td>
          <td>87.11∗</td>
          <td>49.84</td>
      </tr>
      <tr>
          <td>Domain</td>
          <td>CU-Net [42]</td>
          <td>I3D</td>
          <td>86.22</td>
          <td>88.15∗</td>
          <td>37.98</td>
      </tr>
      <tr>
          <td>Domain</td>
          <td>MGFN [8]</td>
          <td>I3D</td>
          <td>86.98</td>
          <td>87.33∗</td>
          <td>32.16</td>
      </tr>
      <tr>
          <td>Domain</td>
          <td>SSRL [19]</td>
          <td>I3D</td>
          <td>87.43</td>
          <td>87.02∗</td>
          <td>51.6</td>
      </tr>
      <tr>
          <td>Domain</td>
          <td>CLIP-TSA [18]</td>
          <td>CLIP</td>
          <td>87.58</td>
          <td>73.20∗</td>
          <td>44.33</td>
      </tr>
      <tr>
          <td>Domain</td>
          <td>Ours (No ext. data)</td>
          <td>CLIP</td>
          <td>84.49</td>
          <td>89.96</td>
          <td>58.13</td>
      </tr>
      <tr>
          <td>Cross-Domain (WeaklySup)</td>
          <td>Ours (UCF + HACS)</td>
          <td>)CLIP</td>
          <td>84.63</td>
          <td>90.53</td>
          <td>65.14</td>
      </tr>
      <tr>
          <td>(Weakly-Sup.)</td>
          <td>Ours (UCF + XDV)</td>
          <td>CLIP</td>
          <td>84.73</td>
          <td>90.26</td>
          <td>68.37</td>
      </tr>
  </tbody>
</table>
<p>Table 3. Comparison with prior works on UCF-Crime, considering XDV as the source data. Asterisk (∗) indicates that evaluations were conducted by us using the official code. Dagger (†) indicates that evaluations were conducted by our implementation due to the lack of an official implementation.</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Methods</th>
          <th>Features</th>
          <th>XDV AP(%)</th>
          <th>UCF-R AUC(%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Cross</td>
          <td>rGAN [23]</td>
          <td>-</td>
          <td>40.10∗</td>
          <td>59.82∗</td>
      </tr>
      <tr>
          <td>Domain</td>
          <td>MPN [25]</td>
          <td>-</td>
          <td>44.79∗</td>
          <td>60.35∗</td>
      </tr>
      <tr>
          <td>(Unsup.)</td>
          <td>zxVAD [3]</td>
          <td>-</td>
          <td>47.53†</td>
          <td>63.61</td>
      </tr>
      <tr>
          <td>Non CrossDomain</td>
          <td>Sultani et al.[32]</td>
          <td>I3D</td>
          <td>73.20</td>
          <td>71.23∗</td>
      </tr>
      <tr>
          <td>Non CrossDomain</td>
          <td>RTFM [33]</td>
          <td>I3D</td>
          <td>77.81</td>
          <td>70.46∗</td>
      </tr>
      <tr>
          <td>Non Cross</td>
          <td>MGFN [8]</td>
          <td>I3D</td>
          <td>80.11</td>
          <td>69.12∗</td>
      </tr>
      <tr>
          <td>Cross Domain</td>
          <td>S3R [37]</td>
          <td>I3D</td>
          <td>80.26</td>
          <td>69.04</td>
      </tr>
      <tr>
          <td>Cross Domain</td>
          <td>CLIP-TSA [18]</td>
          <td>CLIP</td>
          <td>80.67</td>
          <td>67.58</td>
      </tr>
      <tr>
          <td>Cross Domain</td>
          <td>Ours (No ext. data)</td>
          <td>CLIP</td>
          <td>75.13</td>
          <td>76.39</td>
      </tr>
      <tr>
          <td>Cross-Domain</td>
          <td>Ours (XDV + UCF)</td>
          <td>CLIP</td>
          <td>77.04</td>
          <td>88.06</td>
      </tr>
      <tr>
          <td>(Weakly-Sup.)</td>
          <td>Ours (XDV + HACS)</td>
          <td>CLIP</td>
          <td>78.61</td>
          <td>88.50</td>
      </tr>
  </tbody>
</table>
<p>level area under the Precision-Recall curve (PRAUC), also known as Average Precision (AP), to evaluate on XDV.</p>

<h2 class="relative group">4.2. Noise in the Test Annotations of Benchmark Datasets
    <div id="42-noise-in-the-test-annotations-of-benchmark-datasets" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#42-noise-in-the-test-annotations-of-benchmark-datasets" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Our manual inspection reveals that the frame-level testing annotations of the UCF-Crime (UCF) [32] and XDViolence (XDV) [38] datasets, which are commonly used for benchmarking VAD models, exhibit significant noise. This noise largely stems from the fact that the original annotations do not consistently label the frames leading up to the primary anomalous events and their subsequent consequences as anomalous. For instance, in a video assigned a label like &ldquo;shooting&rdquo;, we assert that frames showing the person holding the gun and frames illustrating the injured victim should also be marked as anomalous. This perspective aligns with the fundamental goal of VAD, which is to</p>
<p>Table 4. Comparison with other methods in Open-set setting on UCF-Crime dataset; c denotes the no. of anomalous classes included for weakly-supervised training.</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>UCF (AUC%)</th>
          <th>UCF (AUC%)</th>
          <th>UCF (AUC%)</th>
          <th>UCF (AUC%)</th>
          <th>UCF-R (AUC%) w/o CDL) Ours (CDL)</th>
          <th>UCF-R (AUC%) w/o CDL) Ours (CDL)</th>
          <th>UCF-R (AUC%) w/o CDL) Ours (CDL)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>c</td>
          <td>Wu et al.[38]</td>
          <td>RTFM [33]</td>
          <td>Zhu et al. [49]</td>
          <td>Ours (w/o CDL)</td>
          <td>Ours (CDL)</td>
          <td>Ours (w/o CDL)</td>
          <td>Ours (CDL)</td>
      </tr>
      <tr>
          <td>1</td>
          <td>73.22</td>
          <td>75.91</td>
          <td>76.73</td>
          <td>75.17</td>
          <td>77.45</td>
          <td>84.32</td>
          <td>85.39</td>
      </tr>
      <tr>
          <td>3</td>
          <td>75.15</td>
          <td>76.98</td>
          <td>77.78</td>
          <td>81.51</td>
          <td>82.57</td>
          <td>86.84</td>
          <td>87.69</td>
      </tr>
      <tr>
          <td>6</td>
          <td>78.46</td>
          <td>77.68</td>
          <td>78.82</td>
          <td>82.97</td>
          <td>83.44</td>
          <td>87.85</td>
          <td>88.21</td>
      </tr>
      <tr>
          <td>9</td>
          <td>79.96</td>
          <td>79.55</td>
          <td>80.14</td>
          <td>83.02</td>
          <td>83.37</td>
          <td>89.22</td>
          <td>89.82</td>
      </tr>
  </tbody>
</table>
<p>identify all anomalous frames within a video, irrespective of the video&rsquo;s primary label. However, it should also be noted that in the original annotations, for some videos, certain frames related to the video&rsquo;s primary anomaly label are also not marked anomalous.</p>
<p>To address this, we re-annotate the test set of UCF-Crime by assigning each video to three independent annotators. We then combine their annotations to generate more accurate frame-level labels. Compared to the original annotations where 7.58% of the total frames are labeled as anomalous, the proposed annotations label 16.55% of the total frames as anomalous. The proposed annotations are available here 1 . We provide a comparison of the proposed and original annotations here 2 . For the remainder of this paper, we refer the re-annotated test set of the UCF-Crime dataset as UCF-R.</p>

<h2 class="relative group">4.3. Comparison with Prior Works
    <div id="43-comparison-with-prior-works" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#43-comparison-with-prior-works" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">4.3.1 Cross-Domain Scenarios
    <div id="431-cross-domain-scenarios" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#431-cross-domain-scenarios" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>While the UCF-Crime [32] and XD-Violence [38] datasets share similar definitions of what constitutes anomalies, that definition differs from those of smaller datasets like ShanghaiTech [21], CUHK-Avenue [22], UCSD Pedestrian [7], UBnormal [1], where anomalies are more subtle. For instance, running is considered anomalous in UBnormal but not in XD-Violence. Due to these divergent notions of anomalies across datasets, we conduct cross-domain experiments by simultaneously evaluating on the UCF-Crime and XD-Violence datasets, given their more aligned anomaly definitions.</p>
<p>UCF-Crime as the Weakly-Labeled Source Set, XDV as the Cross-Domain Set. Table 2 summarizes the results for this scenario. First, we observe that the proposed method achieves state-of-the-art results on XDV and UCFR even without utilizing any external data (without CDL). We believe this is due to the inductive bias of previous methods towards the noisy annotations of UCF-Crime. Next, we observe that the addition of external data, HACS and XDV, leads to a significant enhancement in the performance of</p>
<p>1 https : / / drive . google . com / drive / folders / 1IVjQQFHXVcsaT63HUjpfk8C5KH6HsQ7t?usp=drive_link 2 <a
  href="https://rb.gy/4vkr1r"
    target="_blank"
  >https://rb.gy/4vkr1r</a></p>
<p>the cross-domain dataset, XDV, by 11.26% and 14.49%, respectively, compared to the previous state-of-the-art baseline. Additionally, there is also a marginal improvement in the performance of the source set upon integration of external datasets.</p>
<p>XDV as the Weakly-Labeled Source Set, UCF-Crime as the Cross-Domain Set. Table 3 summarizes the results for this scenario. Notably, the proposed method achieves stateof-the-art performance on the cross-domain dataset, UCFR, even without the utilization of any external data during training. This is attributed to the simplicity of the proposed architecture compared to other baselines. The proposed architecture prevents overfitting to the source dataset, thereby increasing its generalizability to the cross-domain dataset. Additionally, integrating external data further enhances performance on both the cross-domain and source sets. Specifically, leveraging the CDL framework with UCF-Crime and HACS as external datasets boosts UCF-R&rsquo;s AUC by 18.94% and 19.39% respectively, compared to previous state-of-theart baselines. We also observe that the proposed method&rsquo;s performance is inferior on XDV. We attribute this to the noise in the annotations of XDV&rsquo;s test set.</p>
<p>These results highlight that the proposed CDL framework is capable of effectively exploiting external data with vast domain gaps to achieve a significant cross-domain generalization. It&rsquo;s noteworthy that the performance gain observed with the proposed CDL framework remains consistent across all tested datasets, suggesting that the performance improvement is not dependent on any specific source or external dataset.</p>

<h2 class="relative group">4.3.2 Open-Set Scenarios
    <div id="432-open-set-scenarios" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#432-open-set-scenarios" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In Table 4, we evaluate the proposed framework&rsquo;s performance on the UCF-Crime dataset in a realistic openset scenario, where the model is evaluated on both, previously seen and unseen anomaly classes. To simulate this scenario, we randomly include c anomalous classes in the weakly-labeled set, while the remaining anomalous classes are placed in the unlabeled set. In both the weaklysupervised source set and the unlabeled set, the number of normal videos equals the number of anomalous videos. We evaluate two model configurations; one trained solely on</p>
<p>Figure 3. (a) Correlation between uncertainty scores and BCE loss computed between the estimated scores and ground truth. When λ 3 = 1e − 3, as expected, a consistently high negative correlation emerges, demonstrating the effectiveness of the proposed uncertainty quantification method as a reliable proxy for pseudo-label quality. (b) Cumulative Distribution Function (CDF) plots illustrating the progression of average uncertainty regularization scores for each video during training. CDL step 20 has a higher concentration of scores around 1 compared to CDL step 2, while CDL step 2 has a higher concentration around 1 than CDL step 1. This suggests that, as training progresses, there is a higher tendency for scores to have elevated values, indicating more confident pseudo-label predictions. (c) Ablation study on the coefficient of the cosine similarity loss term, λ3 . (d) Ablation study on the number of segments, n s.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_0e857932dfb5ab103392f18eadf1818301eabab34cee200f882ee56a0e04927a.png"
    ></figure>
<p>the weakly-labeled set (without CDL) and the other on the union of weakly-labeled and unlabeled sets using the CDL Framework.</p>
<p>On UCF-Crime, the proposed model, without CDL, surpasses the state-of-the-art baselines for c &gt; 1. This highlights its efficacy in open-set settings. While, with CDL, the model surpasses the baselines across all values of c by a considerable margin.</p>
<p>For both UCF-Crime and UCF-R, when unlabeled data is incorporated, we observe a consistent performance gain across all values of c, suggesting the effectiveness of the CDL framework across varying amounts of weakly-labeled and unlabeled data.</p>

<h2 class="relative group">4.4. Correlation between Uncertainty Scores and BCE Loss (Proxy to Label Quality)
    <div id="44-correlation-between-uncertainty-scores-and-bce-loss-proxy-to-label-quality" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#44-correlation-between-uncertainty-scores-and-bce-loss-proxy-to-label-quality" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To assess the efficacy of the proposed uncertainty quantification method as a proxy for pseudo-label quality, we compute the non-parametric Spearman correlation between estimated uncertainty regularization scores and BCE loss between the predicted pseudo-labels and the corresponding ground truths. For this experiment, we consider UCF-Crime as the weakly-labeled source set and XDV as the external set. In Figure 3(a), with λ3 = 1e − 3, CDL step 1 onwards, a consistently high negative correlation (-0.46 in CDL step 6, with a p-value &lt; 1e-5) emerges, indicating the robustness of the proposed uncertainty quantification framework. Conversely, setting λ3 to 0 results in a sustained positive correlation, signifying sub-optimal pseudo-labels in the absence of cosine similarity loss term.</p>

<h2 class="relative group">4.5. Progression of Uncertainty Scores
    <div id="45-progression-of-uncertainty-scores" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#45-progression-of-uncertainty-scores" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To assess the evolution of uncertainty regularization scores through the training process, in Figure 3(b), we plot the Cumulative Distribution Function (CDF) of average uncertainty regularization scores for external videos across the first epoch of three different CDL steps. We conduct this experiment considering UCF-Crime as the weakly-labeled source set and XDV as the external set. We observe that in CDL step 1, 16.65% of the uncertainty scores fall within the range [0, 0.1]. As training progresses to CDL steps 2 and 20, this proportion decreases to 13.06% and 11.39%, respectively. Meanwhile, the proportion of uncertainty scores in the range [0.9, 1] increases from 35.11% in CDL step 1 to 56.70% in CDL step 2 and further to 57.68% in CDL step 20. This trend indicates a discernible shift towards higher uncertainty scores as training progresses, suggesting an improvement in model confidence due to increased pseudolabel quality.</p>

<h2 class="relative group">4.6. Ablation Studies and Hyper-parameter Analysis
    <div id="46-ablation-studies-and-hyper-parameter-analysis" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#46-ablation-studies-and-hyper-parameter-analysis" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>For the sake of consistency, we conduct all ablation studies on UCF-Crime in an open-set setting, with c = 1. However, it should be noted that for different training setups, hyperparameters are tuned separately as well.</p>
<p>Impact of Various Components of the CDL Framework . We assess the effectiveness of each component of the CDL framework by adding them sequentially. The results are summarized in Table 5. We consider training on c = 1 anomaly class in a weakly-supervised fashion as our baseline. The remaining c − 1 anomalous classes are placed in the external set. We first observe that integrating external data into the source set without accounting for pseudo-label uncertainty (S
i,j = 1 , ∀i, j) and without minimizing cosine similarity between representations (λ3 = 0) yields a 0.35% gain in AUC, highlighting the effectiveness of external data in improving the model&rsquo;s performance. Next, we study the impact of uncertainty-aware integration of external data, i.e., adaptively reweighing the prediction bias of external data with the computed uncertainty values and with λ 3 set to 0. This results in a gain of 0.13% in AUC,</p>
<p>Table 5. Ablation study of various components on the UCF-R dataset in an open-set setting (c = 1).</p>
<table>
  <thead>
      <tr>
          <th>External data</th>
          <th>Uncertainty Coeff. Cos. Similarity Loss AUC</th>
          <th>y Coeff. Cos. Similarity Loss AUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>✗</td>
          <td>✗</td>
          <td>84.32</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✗</td>
          <td>84.67</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>84.8</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>85.39</td>
      </tr>
  </tbody>
</table>
<p>demonstrating the superiority of uncertainty-driven integration compared to the standard integration. Finally, we assess the impact of adding the cosine similarity loss term during uncertainty-aware training. This further leads to a significant boost of 0.59%, validating its effectiveness.</p>
<p>Impact of Cosine Similarity Loss. In Figure 3(c), we explore the impact of varying the coefficient of the cosine similarity loss on the model&rsquo;s performance. We observe a gradual increase in AUC as λ 3 increases from 1e-9 to 1e-3. This could be due to the effect of cosine similarity loss getting more pronounced with higher values of λ3. However, beyond 1e-3, there is a rapid decline in AUC, likely due to the dominance of the cosine similarity loss over other losses when its coefficient is high. Therefore, we select 1e-3 as the optimal choice for λ3 .</p>
<p>Impact of Number of Segments. In Figure 3(d), we observe that the performance consistently improves as no. of segments, n s , increases from 16 to 64, but it begins to decline rapidly afterward. Therefore, we set n s as 64.</p>
<p>Impact of the Size of External Data. To determine the optimal number of unlabeled external videos from the HACS dataset to integrate into the weakly-labeled training set of UCF-Crime, we conduct an ablation study, depicted in Figure 4. We observe that increasing the size of the external set increases the performance on XDV. However, this increase tends to plateau after the inclusion of 11,000 videos. Consequently, we do not include additional videos beyond the 11,000 threshold.</p>

<h2 class="relative group">5. Conclusion
    <div id="5-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#5-conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this work, we demonstrated the effectiveness of integrating external, unlabeled data with weakly-labeled source data to enhance the cross-domain generalization of VAD models. To enable this integration, we proposed a weaklysupervised CDL (Cross-Domain Learning) framework that adaptively minimizes the prediction bias on external data by scaling it with the prediction variance, which serves as an uncertainty regularization score. The proposed method outperforms baseline models significantly in cross-domain and open-set settings while retaining competitive performance in in-domain settings.</p>
<p>Figure 4. Ablation study on the impact of the size of external data.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_f7e5ce5412a9086b1f7817d32ee1e65807987e958a917d8b5c0be5e4be6f9a6e.png"
    ></figure>

<h2 class="relative group">Acknowledgement
    <div id="acknowledgement" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#acknowledgement" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This work was supported in part by U.S. NIH grants R01GM134020 and P41GM103712, NSF grants DBI1949629, DBI-2238093, IIS-2007595, IIS-2211597, and MCB-2205148. This work was supported in part by Oracle Cloud credits and related resources provided by Oracle for Research, and the computational resources support from AMD HPC Fund. We thank Eshaan Mandal and Bhavay Malhotra for their assistance, which has been instrumental in completing this work.</p>

<h2 class="relative group">References
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>[1] Andra Acsintoae, Andrei Florescu, Mariana-Iuliana Georgescu, Tudor Mare, Paul Sumedrea, Radu Tudor Ionescu, Fahad Shahbaz Khan, and Mubarak Shah. Ubnormal: New benchmark for supervised open-set video anomaly detection. In CVPR, 2022. 2 , 7</li>
<li>[2] Abien Fred Agarap. Deep learning using rectified linear units (relu), 2019. 6</li>
<li>[3] Abhishek Aich, Kuan-Chuan Peng, and Amit K. RoyChowdhury. Cross-domain video anomaly detection without target domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 2579–2591, 2023. 1 , 2 , 3 , 6</li>
<li>[4] Eric Arazo, Diego Ortego, Paul Albert, Noel E O&rsquo;Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In IJCNN, 2020. 3 , 4</li>
<li>[5] David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In ICLR, 2020. 3</li>
<li>[6] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR , pages 6299–6308, 2017. 3</li>
<li>[7] Antoni B. Chan and Nuno Vasconcelos. Modeling, clustering, and segmenting video with mixtures of dynamic textures. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 909–926, 2008. 7</li>
<li>[8] Yingxian Chen, Zhengzhe Liu, Baoheng Zhang, Wilton Fok, Xiaojuan Qi, and Yik-Chung Wu. Mgfn: magnitudecontrastive glance-and-focus network for weakly-supervised video anomaly detection. In Proceedings of the ThirtySeventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, 2023. 6</li>
<li>[9] MyeongAh Cho, Minjung Kim, Sangwon Hwang, Chaewon Park, Kyungjae Lee, and Sangyoun Lee. Look around for anomalies: Weakly-supervised anomaly detection via context-motion relational learning. In CVPR, pages 12137– 12146, 2023. 3</li>
<li>[10] Yang Cong, Junsong Yuan, and Ji Liu. Sparse reconstruction cost for abnormal event detection. In CVPR, pages 3449– 3456, 2011. 1</li>
<li>[11] Jia-Chang Feng, Fa-Ting Hong, and Wei-Shi Zheng. MIST: Multiple instance self-training framework for video anomaly detection. In CVPR, pages 14009–14018, 2021. 1 , 2 , 3 , 6 , 14</li>
<li>[12] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In ICML, pages 1050–1059, 2016. 3</li>
<li>[13] Mariana Iuliana Georgescu, Radu Tudor Ionescu, Fahad Shahbaz Khan, Marius Popescu, and Mubarak Shah. A background-agnostic framework with adversarial training for abnormal event detection in video. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 4505– 4523, 2022. 2</li>
<li>[14] Mahmudul Hasan, Jonghyun Choi, jan Neumann, Amit K Roy-Chowdhury, and Larry Davis. Learning temporal regularity in video sequences. In Proceedings of IEEE Computer Vision and Pattern Recognition, 2016. 14</li>
<li>[15] Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K. Roy-Chowdhury, and Larry S. Davis. Learning temporal regularity in video sequences. In CVPR, 2016. 1 , 2</li>
<li>[16] Kexin Huang, Vishnu Sresht, Brajesh Rai, and Mykola Bordyuh. Uncertainty-aware pseudo-labeling for quantum calculations. In UAI, pages 853–862, 2022. 3</li>
<li>[17] Radu Tudor Ionescu, Fahad Shahbaz Khan, Mariana-Iuliana Georgescu, and Ling Shao. Object-centric auto-encoders and dummy anomalies for abnormal event detection in video. In CVPR, pages 7842–7851, 2019. 3</li>
<li>[18] Hyekang Kevin Joo, Khoa Vo, Kashu Yamazaki, and Ngan Le. Clip-tsa: Clip-assisted temporal self-attention for weakly-supervised video anomaly detection. In ICIP, pages 3230–3234, 2023. 6</li>
<li>[19] Guoqiu Li, Guanxiong Cai, Xingyu Zeng, and Rui Zhao. Scale-aware spatio-temporal relation learning for video anomaly detection. In ECCV, pages 333–350, 2022. 6</li>
<li>[20] Shuo Li, Fang Liu, and Licheng Jiao. Self-training multisequence learning with transformer for weakly supervised video anomaly detection. AAAI, pages 1395–1403, 2022. 3 , 14</li>
<li>[21] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Future frame prediction for anomaly detection–a new baseline. In CVPR, pages 6536–6545, 2018. 1 , 2 , 3 , 7</li>
<li>[22] Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event detection at 150 fps in matlab. In ICCV, pages 2720–2727, 2013. 2 , 3 , 7 , 14</li>
<li>[23] Yiwei Lu, Frank Yu, Mahesh Kumar Krishna Reddy, and Yang Wang. Few-shot scene-adaptive anomaly detection. In ECCV, pages 125–141, 2020. 2 , 3 , 6</li>
<li>[24] Weixin Luo, Wen Liu, and Shenghua Gao. Remembering history with convolutional lstm for anomaly detection. In ICME, pages 439–444, 2017. 3</li>
<li>[25] Hui Lv, Chen Chen, Zhen Cui, Chunyan Xu, Yong Li, and Jian Yang. Learning normal dynamics in videos with meta prototype network. In CVPR, pages 15425–15434, 2021. 1 , 2 , 3 , 6</li>
<li>[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In NAACL, pages 152–159, 2006. 3</li>
<li>[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763, 2021. 3</li>
<li>[28] Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning. In ICLR, 2021. 3</li>
<li>[29] Hitesh Sapkota and Qi Yu. Bayesian nonparametric submodular video partition for robust anomaly detection. In CVPR , pages 3212–3221, 2022. 2</li>
<li>[30] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: simplifying semisupervised learning with consistency and confidence. In NeurIPS, 2020. 3</li>
<li>[31] Fahad Sohrab, Jenni Raitoharju, Moncef Gabbouj, and Alexandros Iosifidis. Subspace support vector data description. In ICPR, pages 722–727, 2018. 14</li>
<li>[32] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In CVPR, pages 6479–6488, 2018. 1 , 2 , 3 , 6 , 7 , 12</li>
<li>[33] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W Verjans, and Gustavo Carneiro. Weakly-supervised video anomaly detection with robust temporal feature magnitude learning. In ICCV, pages 4975–4986, 2021. 1 , 2 , 6 , 7 , 14</li>
<li>[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, page 6000–6010, 2017. 6 , 12</li>
<li>[35] Boyang Wan, Yuming Fang, Xue Xia, and Jiajie Mei. Weakly supervised video anomaly detection via centerguided discriminative learning. ICME, pages 1–6, 2020. 3</li>
<li>[36] Jue Wang and Anoop Cherian. Gods: Generalized one-class discriminative subspaces for anomaly detection. In ICCV, V, pages 8200–8210, 2019. 14</li>
<li>[37] Jhih-Ciang Wu, He-Yen Hsieh, Ding-Jie Chen, Chiou-Shann Fuh, and Tyng-Luh Liu. Self-supervised sparse representa-</li>
</ul>
<p>tion for video anomaly detection. In ECCV, pages 729–745, 2022. 6</p>
<ul>
<li>[38] Peng Wu, jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. Not only look, but also listen: Learning multimodal violence detection under weak supervision. In ECCV, 2020. 6 , 7 , 12 , 14</li>
<li>[39] Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, and Nicu Sebe. Learning deep representations of appearance and motion for anomalous event detection. In BMVC, pages 8.1– 8.12, 2015. 2</li>
<li>[40] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In ACL, page 189–196, 1995.</li>
</ul>
<p>3</p>
<ul>
<li>[41] Guang Yu, Siqi Wang, Zhiping Cai, En Zhu, Chuanfu Xu, Jianping Yin, and Marius Kloft. Cloze test helps: Effective video anomaly detection via learning to complete video events. In Proceedings of the 28th ACM International Conference on Multimedia, pages 583–591, 2020. 2</li>
<li>[42] Chen Zhang, Guorong Li, Yuankai Qi, Shuhui Wang, Laiyun Qing, Qingming Huang, and Ming-Hsuan Yang. Exploiting completeness and uncertainty of pseudo labels for weakly supervised video anomaly detection. In CVPR, pages 16271– 16280, 2023. 1 , 3 , 6 , 14</li>
<li>[43] Jiangong Zhang, Laiyun Qing, and Jun Miao. Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection. In ICIP, pages 4030– 4034, 2019. 2</li>
<li>[44] Bin Zhao, Fei-Fei Li, and Eric Xing. Online detection of unusual events in videos via dynamic sparse coding. In CVPR , pages 3313–3320, 2011. 2 , 3</li>
<li>[45] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset. In ICCV, pages 8668–8678, 2019. 6 , 12</li>
<li>[46] Zhedong Zheng and Yi Yang. Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation. International Journal of Computer Vision (IJCV), 2021. 3 , 4 , 5</li>
<li>[47] Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H Li, and Ge Li. Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection. In CVPR, pages 1237–1246, 2019. 1 , 3</li>
<li>[48] Yi Zhu and Shawn D. Newsam. Motion-aware feature for improved video anomaly detection. In BMVC, page 270, 2019. 2</li>
<li>[49] Yuansheng Zhu, Wentao Bao, and Qi Yu. Towards open set video anomaly detection. In ECCV, pages 395–412, 2022. 7 , 14</li>
</ul>

<h2 class="relative group">Cross-Domain Learning for Video Anomaly Detection with Limited Supervision
    <div id="cross-domain-learning-for-video-anomaly-detection-with-limited-supervision-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#cross-domain-learning-for-video-anomaly-detection-with-limited-supervision-1" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">Supplementary Material
    <div id="supplementary-material" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#supplementary-material" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">6. Revisiting Multiple Instance Learning
    <div id="6-revisiting-multiple-instance-learning" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#6-revisiting-multiple-instance-learning" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Since acquiring frame-level labels requires significant time and effort, following Sultani et al. [32], we use Multiple Instance Learning (MIL) to train the classifiers using weakly-supervised video-level labels. By dividing a video (bag) into multiple temporal non-overlapping segments (instances) and encouraging anomalous video segments to have higher anomaly scores as compared to the normal segments, they formulate anomaly detection as a regression problem.</p>
<p>The multiple instance ranking objective function is given by:</p>
<!-- formula-not-decoded -->
<p>where D a l = {(X, Y ) ∈ Dl : Y = 1} and D n l = {(X, Y ) ∈ Dl : Y = 0} are the set of abnormal and normal videos, respectively and max is taken over all video segments in a bag.</p>
<p>Instead of ranking every segment of the positive and negative bags, ranking is enforced on one segment from each bag, having the highest anomaly score. The overall loss function, Lrank, for a pair of abnormal and normal videos, is given by:</p>
<!-- formula-not-decoded -->
<p>where L Ts is the temporal smoothness constraint, and LSp is the sparsity constraint.</p>

<h2 class="relative group">7. Datasets
    <div id="7-datasets" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#7-datasets" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>UCF-Crime [32]: This is a large-scale VAD dataset having a total duration of 128 hours. It contains long and untrimmed real-world surveillance videos across 13 realistic anomaly categories that are specifically chosen due to their significant impact on public safety. The dataset comprises 1610 weakly-labeled training videos and 290 test videos annotated at the frame level.</p>
<p>XD-Violence (XDV) [38]: This is a large-scale and multiscene audio-visual dataset for violence detection, having a total duration of 217 hours. Its long and untrimmed videos are collected from movies, games, and in-the-wild scenarios, with anomalies spread over 6 categories. It comprises 3954 weakly-labeled training videos and 800 test videos annotated at the frame level.</p>
<p>HACS [45]: This is a large-scale dataset for human action recognition, sourced from YouTube. It features 200 action classes across 140K segments on 50K videos. Due to its diverse range of actions, larger size, and longer video durations compared to other video datasets such as UCF-101, Kinetics, and ActivityNet, we use a subset of 11K videos from HACS Segments as external, unlabeled data.</p>

<h2 class="relative group">8. Implementation Details
    <div id="8-implementation-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#8-implementation-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To ensure consistency and gradient stability, while training on D l ∪ D u , each mini-batch consists of an equal number of samples from Dl and D u . Since the computation of L rank necessitates pairs of abnormal and normal videos, each labeled sample within the mini-batch comprises a pair of anomalous and normal videos. All the experiments were conducted on an NVIDIA RTX A5000 24 GB GPU. For the experiments using UCF-Crime as the weakly-labeled data, we set the batch size to 64, and for the experiments using XD-Violence as the weakly-labeled data, we set the batch size to 32. In all our experiments except the open-set, we set n s to 64, τ to 1.25, λ1 to 5e-3, λ2 to 1e-3, λ3 to 1e-3. We set λ 4 to 2000 for UCF+HACS and UCF+XDV, 1250 for XDV+HACS, and 700 for XDV+UCF. For all our experiments, we use the Adam optimizer with a weight decay of 1e-3. For the fully connected layers, we use a learning rate of 5e-4 when UCF-Crime is used as the weakly-labeled dataset and a learning rate of 1e-4 when XDV is used as the weakly-labeled dataset. For the transformer encoder layers, we use a learning rate of 3e-5 when UCF-Crime is used as the weakly-labeled dataset and a learning rate of 5e-5 when XDV is used as the weakly-labeled dataset. In all our experiments, we explicitly encode positional information in the segments using sinusoidal positional encodings [34]. We train on the weakly-labeled source dataset for 200 epochs, followed by training on the union of weakly-labeled and external datasets for 40 CDL steps, each CDL step comprising 4 epochs. Due to the finer granularity and semantic richness inherent in CLIP features, we choose to use CLIP features during inference.</p>

<h2 class="relative group">9. Comparison with Unsupervised Baselines in Open-Set Settings
    <div id="9-comparison-with-unsupervised-baselines-in-open-set-settings" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#9-comparison-with-unsupervised-baselines-in-open-set-settings" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Table 6 depicts that the proposed method outperforms all the baselines in open-set settings on the UCF-Crime dataset by a large margin. As expected, all the weakly-supervised methods outperform the unsupervised methods, even when a small subset of the data is used for weakly-supervised training. This highlights the necessity of incorporating</p>
<p>Figure 5. A comparison between the original annotations (UCF) and the proposed annotations (UCF-R). The green region represents frames labeled as anomalous by both the original and proposed annotations. The red region indicates frames labeled as anomalous by the proposed annotations but not by the original annotations. The unshaded (white) region denotes normal frames. For instance, in the first row, while the original annotations just label frames depicting arson (a person setting the Christmas tree on fire) as anomalous, UCF-R also labels the frames depicting the fire and smoke following arson as anomalous.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000004_822305dc1de653feb3eeff51b52a66ac034b3539e2d6918941cb39dc854ddae3.png"
    ></figure>
<p>weak labels during training. Since a direct comparison of the proposed weakly-supervised framework with unsupervised methods is not fair, we did not include unsupervised baselines in Table 4.</p>

<h2 class="relative group">10. Comparison of the Original and Proposed Annotations for UCF-Crime Dataset
    <div id="10-comparison-of-the-original-and-proposed-annotations-for-ucf-crime-dataset" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#10-comparison-of-the-original-and-proposed-annotations-for-ucf-crime-dataset" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure 5 illustrates a subset of instances from the UCFCrime&rsquo;s test set where the original annotations do not label frames as anomalous, despite their actual anomalous nature. We also provide a comparison of the proposed and original annotations superimposed on the videos at this link:</p>
<p>Table 6. Comparison with prior works in open-set setting on UCF-Crime dataset; c denotes the number of anomalous classes included for weakly-supervised training. The values represent AUC (%).</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>c</th>
          <th>0</th>
          <th>1</th>
          <th>3</th>
          <th>6</th>
          <th>9</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>p</td>
          <td>Conv-AE [14]</td>
          <td>50.60</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td>p</td>
          <td>Sohrab et al. [31]</td>
          <td>58.50</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td>p</td>
          <td>Lu et al. [22]</td>
          <td>65.51</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td>p</td>
          <td>BODS [36]</td>
          <td>68.26</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td>p</td>
          <td>GODS [36]</td>
          <td>70.46</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
      </tr>
      <tr>
          <td></td>
          <td>Wu et al. [38] (offline</td>
          <td>-</td>
          <td>73.22</td>
          <td>75.15</td>
          <td>78.46</td>
          <td>79.96</td>
      </tr>
      <tr>
          <td></td>
          <td>Wu et al. [38] (online)</td>
          <td>-</td>
          <td>73.78</td>
          <td>74.64</td>
          <td>77.84</td>
          <td>79.11</td>
      </tr>
      <tr>
          <td></td>
          <td>RTFM [33]</td>
          <td>-</td>
          <td>75.91</td>
          <td>76.98</td>
          <td>77.68</td>
          <td>79.55</td>
      </tr>
      <tr>
          <td></td>
          <td>Zhu et al. [49]</td>
          <td>-</td>
          <td>76.73</td>
          <td>77.78</td>
          <td>78.82</td>
          <td>80.14</td>
      </tr>
      <tr>
          <td></td>
          <td>Ours (w/o CDL)</td>
          <td>-</td>
          <td>75.17</td>
          <td>81.51</td>
          <td>82.97</td>
          <td>83.02</td>
      </tr>
      <tr>
          <td></td>
          <td>Ours</td>
          <td>-</td>
          <td>77.45</td>
          <td>82.57</td>
          <td>83.44</td>
          <td>83.37</td>
      </tr>
  </tbody>
</table>

<h2 class="relative group"><a
  href="https://rb.gy/4vkr1r"
    target="_blank"
  >https://rb.gy/4vkr1r</a> .
    <div id="heading" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#heading" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">11. Limitations
    <div id="11-limitations" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#11-limitations" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Similar to some recent weakly-supervised VAD works [11 , 20 , 42], the training process of the proposed CDL framework involves two stages. Consequently, the training does not operate in an end-to-end manner. This incurs additional complexity and challenges for training the model in real-world applications. However, since the generalization obtained using this multi-stage training is significant, the complex training setup of the multi-stage framework is reasonable. Nonetheless, developing end-to-end training frameworks would be an important direction for future research. This can facilitate the advancement of anomaly detection approaches for real-world applications, particularly the ones with limited training budgets.</p>
<p>Additionally, the cross-domain performance in case of drastic distribution shifts between the source and target domains may be hindered. For instance, a model primarily trained on videos from stationary surveillance cameras may not effectively work on videos with rapidly evolving scenes from car dashcams. This is mainly because the uncertaintybased reweighing approach in our framework aims to select samples from the external set that are similar to the source domain. In case of drastic shifts between the two domains, finding informative samples from the target domain would not be trivial.</p>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/cross domain learning for VAD with limited supervision.md"
          data-oid-likes="likes_papers/cross domain learning for VAD with limited supervision.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/delving-into-clip-latent-space-for-video-anomaly-recognition/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/clip-assisted/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
