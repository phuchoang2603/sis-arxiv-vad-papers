<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/typicality-and-context-uniqueness-for/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/typicality-and-context-uniqueness-for/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/typicality-and-context-uniqueness-for\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "7867"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>7867 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">37 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection
    <div id="action-hints-semantic-typicality-and-context-uniqueness-for-generalizable-skeleton-based-video-anomaly-detection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#action-hints-semantic-typicality-and-context-uniqueness-for-generalizable-skeleton-based-video-anomaly-detection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Canhui Tang, Sanping Zhou, Member, IEEE, Haoyue Shi, Le Wang, Senior Member, IEEE</p>
<p>Abstract—Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing anomalies without target domain training data, which is a crucial task due to various practical concerns, e.g., data privacy or new surveillance deployments. Skeleton-based approach has inherent generalizable advantages in achieving ZS-VAD as it eliminates domain disparities both in background and human appearance. However, existing methods only learn low-level skeleton representation and rely on the domain-limited normality boundary, which cannot generalize well to new scenes with different normal and abnormal behavior patterns. In this paper, we propose a novel zero-shot video anomaly detection framework, unlocking the potential of skeleton data via action typicality and uniqueness learning. Firstly, we introduce a language-guided semantic typicality modeling module that projects skeleton snippets into action semantic space and distills LLM&rsquo;s knowledge of typical normal and abnormal behaviors during training. Secondly, we propose a test-time context uniqueness analysis module to finely analyze the spatio-temporal differences between skeleton snippets and then derive sceneadaptive boundaries. Without using any training samples from the target domain, our method achieves state-of-the-art results against skeleton-based methods on four large-scale VAD datasets: ShanghaiTech, UBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.</p>
<p>Index Terms—Video Anomaly Detection, Skeleton-based, Zeroshot, Action Semantic Typicality, Context Uniqueness.</p>

<h2 class="relative group">I. INTRODUCTION
    <div id="i-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#i-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video Anomaly Detection (VAD) aims to temporally locate abnormal events, which has wide applications in the context of video surveillance and public safety [1], [2]. Current mainstream paradigms include one-class [3]–[7] and weakly supervised methods [2], [8], [9], which require abundant samples from the target video domain for training. However, in surveillance scenarios involving privacy or newly installed monitoring devices, training samples from the target domain are usually not available. Therefore, designing a Zero-Shot Video Anomaly Detection (ZS-VAD) method that can generalize to diverse target domains becomes necessary. Despite the recent extensive attention given to zero-shot image anomaly detection [10]–[14], the zero-shot setting in the complex surveillance video domain remains under-explored [15].</p>
<p>The challenges of ZS-VAD come from significant variations in visual appearance and human activities across different</p>
<p>This work was supported in part by National Science and Technology Major Project under Grant 2023ZD0121300, National Natural Science Foundation of China under Grants 62088102, U24A20325 and 12326608, and Fundamental Research Funds for the Central Universities under Grant XTR042021005. (Corresponding author: Sanping Zhou, E-mail: <a
  href="mailto:spzhou@mail.xjtu.edu.cn">spzhou@mail.xjtu.edu.cn</a>.)</p>
<p>Canhui Tang, Sanping Zhou, Haoyue Shi, and Le Wang are all with the National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, and Institute of Artificial Intelligence and Robotics, Xi&rsquo;an Jiaotong University, Shaanxi 710049, China.</p>
<p>Fig. 1. An illustration of skeleton-based VAD paradigm comparison. Previous approaches suffer from two main issues: (1) low-level representations and (2) domain-limited normal boundary. Our method enhances generalizability via action semantic typicality learning and context uniqueness analysis.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_05aa420b70ad712266ece39019dd6e5508122089297d5b2e8d58fcb7113713d8.png"
    ></figure>
<p>video domains. While frame/object-based methods [3], [4], [16] have been prominent in video anomaly detection, their performance will degrade when adapting to new scenes due to visual feature distribution shifts. In another view, skeletonbased methods [6], [17]–[19] utilize mature pose detection systems [20], [21] to obtain skeleton data, learn to encode features via self-supervision tasks [18], [19], and then calculate the anomaly score. They are effective for identifying human behavior anomalies, which are popular in the VAD task due to their superior efficiency and performance. skeletonbased methods also have inherent generalizable advantages in achieving ZS-VAD as it eliminates domain disparities both in background and human appearance.</p>
<p>However, as shown in Fig. 1, existing skeleton-based VAD methods still suffer from several limitations: (1) Low-level skeleton representations. They learn normal distribution of skeleton patterns using self-supervised tasks, such as skeleton prediction [19], reconstruction [18], or coordinate-based normalizing flows [6]. Without semantic supervision signals, such methods fail to capture higher-level action patterns, making them unable to distinguish novel anomaly patterns similar to normal patterns and sensitive to noise. (2) Domain-limited normality boundary. They blindly rely on training-datadefined normality boundaries, leading to the misclassification of unseen normal events as anomalies. Both limitations hinder their generalization to unseen scenes with varying normal and</p>
<p>abnormal patterns. This leads to a question: &ldquo;Can we further unlock the potential of skeleton in ZS-VAD with generalizable representation learning and prior injection? &quot;</p>
<p>To address this question, we reflect on how human observers judge normal and abnormal behavior in a new scenario. As shown in Fig. 1, we first identify the types of individual actions in the video and consider whether they are normal or abnormal based on our experiential knowledge of normality and abnormality, which is referred to as typicality. For instance, a pedestrian walking would be considered normal, while a fight or scuffle would be deemed abnormal. Secondly, for atypical normal or abnormal scenarios, we integrate the behaviors of all individuals in the video to observe if any individual&rsquo;s behavior significantly differs from others, as anomalies are usually rare and unique, referred to as context uniqueness.</p>
<p>Based on these complementary priors, we propose a novel skeleton-based zero-shot video anomaly detection framework, which captures both typical anomalies guided by language prior and unique anomalies in spatio-temporal contexts. First, we introduce a language-guided typicality modeling module to achieve high-level semantic understanding beyond previous low-level representations. Specifically, it projects skeleton snippets into language-aligned action semantic space and distills LLM&rsquo;s knowledge of typical normal and abnormal behaviors during training. Secondly, to derive scene-adaptive boundaries, we propose a context uniqueness analysis module at test time. It finely analyzes the spatio-temporal differences between skeleton snippets to get an adaptive understanding of target scene activities. Without using any training samples from the target domain, we achieve state-of-the-art results on four large-scale VAD datasets: ShanghaiTech [1], UBnormal [22], NWPU [23], UCF-Crime [2], featuring over 100 unseen surveillance scenes. Our contributions are as follows:</p>
<ul>
<li>We propose a skeleton-based video anomaly detection framework that learns action typicality and uniqueness, enabling generalization across diverse target scenes.</li>
<li>We propose a language-guided typicality modeling module that projects skeleton snippets into a generalizable semantic space and distills LLM&rsquo;s knowledge of typical normal and abnormal behaviors during training.</li>
<li>We propose a test-time uniqueness analysis module to finely analyze the spatio-temporal differences between skeleton snippets and derive scene-adaptive boundaries between normal and abnormal behavior.</li>
</ul>
<p>The rest of this paper is organized as follows. We review the related work in Section II. Section III describes the technical details of our proposed method. Section IV presents the experiment details and results. Finally, we summarize the paper in Section V .</p>

<h2 class="relative group">II. RELATED WORK
    <div id="ii-related-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#ii-related-work" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video anomaly detection. Most previous video anomaly detection studies can be grouped into frame-based [1], [2], [4], object-centric [3], [16], [24], and skeleton-based methods [6], [19], [25]. In this work, we focus on the skeleton-based methods, which detect anomalies in human activity based on preprocessed skeleton/pose data. Morais et al. [17] propose an anomaly detection method that uses an RNN network to learn the representation of pose snippets, with prediction errors serving as anomaly scores. GEPC [25] utilizes autoencoders to learn pose graph embeddings, generates soft assignments through clustering, and uses a Dirichlet process mixture to determine anomaly scores. To model normal diversity, MoCoDAD [19] leverages diffusion probabilistic models to generate multimodal future human poses. FG-Diff [26] guides the diffusion model with observed high-frequency information and prioritizes the reconstruction of low-frequency components, enabling more accurate and robust anomaly detection. STGNF [6] proposes a simple yet effective method by establishing normalized flow [27] from normal pose snippets to obtain normal boundaries. DA-Flow [28] proposes a lightweight dual attention module for capturing cross-dimension interaction relationships in spatio-temporal skeletal data. However, these methods rely on training with normal data from the target domain, while overlooking the semantic understanding of human behavior, which makes it difficult to ensure performance in scenarios where the target data is unavailable.</p>
<p>Zero-shot anomaly detection. Thanks to the development of vision-language models, zero-shot anomaly detection has received a lot of attention [10]–[12], [14], [29]–[31], especially in the field of image anomaly detection [32]. The pioneering work is WinCLIP [11], which utilizes CLIP [33]&rsquo;s image-text matching capability to distinguish between unseen normal and abnormal anomalies. Building on that, AnomalyCLIP [14] proposes to learn object-agnostic text prompts that capture generic normal and abnormal patterns in an image. AdaCLIP [13] introduces two types of learnable prompts to enhance CLIP&rsquo;s generalization ability for anomaly detection. Despite the success in the image domain, only a few works [15], [34] have ventured into zero-shot video anomaly detection with underwhelming performance. Although recently [35] proposes to leverage large visual language models for zero-shot video anomaly detection, it requires multi-stage reasoning and the collaboration of multiple large models, making it less userfriendly. We aim to develop a lightweight, user-friendly, and easily deployable zero-shot anomaly detector starting from skeleton data. Our work shares some similarities with a recent study [36]. However, we emphasize that our approach differs significantly from [36] in the following ways: 1) Different tasks: It addresses abnormal action recognition, involving no more than two individuals in a short video, while ours requires temporally localizing abnormal events in real surveillance videos. 2) Novel perspective: We combine the action typicality and uniqueness priors to address zero-shot anomaly detection challenges in video surveillance scenes.</p>

<h2 class="relative group">III. METHOD
    <div id="iii-method" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#iii-method" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">A. Overview
    <div id="a-overview" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-overview" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The objective of ZS-VAD is to train one model that can generalize to diverse target domains. Formally, let V train be a training set from source video domain and {W test 1 , W
2 test W
2 , &hellip;, W test N } be multiple test sets from target video domain. The test videos are annotated at the frame level with labels l i ∈ {0 , 1}, and the VAD model is required to</p>
<p>Fig. 2. Overview of our approach for skeleton-based zero-shot video anomaly detection. I. Language-guided typicality modeling in the training phase. It projects skeleton snippets into the action semantic space, collects typicality knowledge from LLM, and then effectively learns the typical distribution of normal and abnormal behavior. (Only the black dashed boxes are used during inference.) II. Test-time uniqueness analysis in the inference phase. It finely analyzes the spatio-temporal differences between skeleton snippets and derives scene-adaptive boundaries between normal and abnormal behavior.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_2789375e553d7329cdca69b919b0eaed3c6b8d7aac7b80ca8919326e9d056a52.png"
    ></figure>
<p>predict each frame&rsquo;s anomaly score. In this work, we focus on the skeleton-based paradigm, as it is computation-friendly and can benefits ZS-VAD by reducing the domain gap in both background and appearance.</p>
<p>Fig. 2 overviews our proposed approach. Our model tackles the ZS-VAD problem from the perspective of action typicality and uniqueness learning. Firstly, to obtain a high-level semantic understanding, we propose a Language-Guided Typicality Modeling module that projects skeleton snippets into action semantic space and distills LLM&rsquo;s knowledge of typical normal and abnormal behaviors during training. Secondly, to get scene-adaptive decision boundaries, we propose a Test-Time Uniqueness Analysis module that finely analyzes the spatiotemporal differences between skeleton snippets. During inference on unseen VAD datasets, our model integrates typicality scores and uniqueness scores of human behavior to provide a holistic understanding of anomalies.</p>

<h2 class="relative group">B. Language-guided Typical Modeling
    <div id="b-language-guided-typical-modeling" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-language-guided-typical-modeling" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Unlike previous works that learn low-level skeleton representations via self-supervised tasks [6], [18], [19], this module aims to obtain a high-level semantic understanding of human behavior. It learns language-aligned action features and scenegeneric distributions of typical distribution with distillation of LLM&rsquo;s knowledge during training. Specifically, this module consists of skeleton-text alignment, typicality knowledge selection, and typicality distribution learning. During inference, it can predict typicality anomaly scores with only a lightweight skeleton encoder and a normalizing flow module.</p>
<p>Skeleton-text alignment. For achieving a generalizable semantic understanding of human behavior, we first propose to align the skeleton snippets with the corresponding semantic labels. For such skeleton-text pairs, we utilize external action recognition datasets (e.g., Kinect [37] as the training set instead of specific VAD datasets (e.g., ShanghaiTech). The raw skeleton data of an action video is typically formally represented as Xi ∈ R C×J×L×M , where C is the coordinate number, J is the joint number, L is the sequence length, and M is the pre-defined maximum number of persons, respectively. In addition, each video is annotated with a text label gi representing the action class, which can also be transformed into a one-hot class vector yi .</p>
<p>Compared to action recognition tasks [38]–[40] that only predict video-level categories, the VAD task is more finegrained, focusing on frame-level anomaly scores. Therefore, we decompose the original sequences into multiple short skeleton snippets Ai ∈ R C×J×T using a sliding window, and discard snippets that are composed of zeros, where T is the length of a snippet. For the snippets from the same action video, they share the same labels and undergo a normalization operation like STG-NF [6] to make different snippets independent. Inspired by the recent multimodal alignment works [41], [42], we then perform a skeleton-text alignment pretraining procedure to learn the discriminative representation. The procedure is built with a skeleton encoder E s and a text encoder E t , for generating skeleton features F s and text features F t , respectively. Additionally, the skeleton encoder also predicts a probability vector yˆ ˆ i using a fully-connected layer. The training loss consists of a KL divergence loss and a crossentropy classification loss following GAP [42]. This skeletontext alignment procedure effectively guides the projection of skeleton snippets into language-aligned action semantic space beyond previous VAD works [6], [18], [19] that learns lowlevel skeleton representation.</p>
<p>Typicality knowledge selection. In most video surveillance scenarios, some behaviors are generally considered normal or abnormal, which constitute a scene-generic set. Therefore, training a typicality-aware capability is one of the promising</p>
<p>TABLE I THE GENERATED TYPICALITY LABELS .</p>
<table>
  <thead>
      <tr>
          <th>Type</th>
          <th>Typicality action list</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Normal</td>
          <td>Normal</td>
      </tr>
      <tr>
          <td>Abnormal</td>
          <td>Abnormal</td>
      </tr>
  </tbody>
</table>
<p>ways to achieve ZS-VAD. Thanks to the cutting-edge advancements of Large Language Models (LLMs), we propose to distill their prior knowledge about generic normality and abnormality during training. Based on the pre-trained skeletontext representation, we aim to use a LLM as our knowledge engine to collect typical normal and abnormal data from the massive skeleton snippets. In detail, we give the large model a prompt P: &ldquo;In most video surveillance scenarios, what are generally considered as normal actions and abnormal actions among these actions (Please identify the 20 most typical normal actions and 20 most typical abnormal actions, ranked in order of decreasing typicality). The action list is &lt;T &gt;&rdquo;, where T refers to the set of all action class labels in the prepared action recognition dataset [37].</p>
<p>The large language model will respond with a list of typical normal action classes T
n T
n and a list of typical abnormal action classes T a , which can be formalized as:</p>
<!-- formula-not-decoded -->
<p>where T
n T
n and T a are the subsets of T , and OLLM denotes the offline LLM used for initial typicality label generation. Note that the LLM is only needed to be used once during training for auxiliary data selection, while inference is not.</p>
<p>After knowing the action categories of typicality, we first collect the data of these selected categories and then proceed to select the high-quality snippets from them. This is because 1) Some snippets contain noise, such as errors in pose detection and tracking. 2) In an abnormal action sequence, not all the snippets are abnormal. Therefore, we use the skeleton-text similarity score to select the high-quality skeleton snippets, which is formulated as:</p>
<!-- formula-not-decoded -->
<p>where M x refers to the selected snippets index, gi denotes the text label of snippet i, and β denotes the selection ratio. The superscript x represents n or a, indicating normal and abnormal, respectively. Using the index M x , we obtain the corresponding skeleton data A ˜ n and A ˜ a , as well as skeleton features F ˜ s n and F ˜ s a .</p>
<p>Typicality distribution learning. As shown in Fig. 2, after obtaining the data, we proceed to model the feature distribution of typical behavior. Normalizing Flow (NF) [27] provides a robust framework for modeling feature distributions, transforming this distribution through a series of invertible and differentiable operations. Consider a random variable X ∈ R D with target distribution pX(x), and a random variable Z follows a spherical multivariate Gaussian distribution. A bijective map f : X ↔ Z is then introduced, which is composed of a sequence of transformations: f1 ◦ f2 ◦ &hellip; ◦ fK. According to the variable substitution formula, the log-likelihood of X can be expressed as:</p>
<!-- formula-not-decoded -->
<p>Using such a transformation, the feature distribution of typicality behavior is effectively modeled. Specifically, the bijective maps for the normal features and abnormal features are f : X n ↔ Zn Zn and f : X a ↔ Z a , respectively. Here, the log-likelihood of Zn Zn and Z a are as follows:</p>
<!-- formula-not-decoded -->
<p>where Con is a constant, and u n and u z are the centers of the Gaussian distributions (|u n − u z | ≫ 0), respectively. During training, the normalizing flow is optimized to increase the loglikelihood of the skeleton features F s with the following loss:</p>
<!-- formula-not-decoded -->
<p>During inference, the testing skeleton snippet F s i will be sent to the trained normalizing flow, outputting the typicality anomaly score as follows:</p>
<!-- formula-not-decoded -->
<p>where the normal skeletons will exhibit low S t i , while the anomalies will exhibit higher S t i . Our approach differs significantly from STG-NF [6]. It takes low-level skeleton coordinates as inputs and only learns implicit spatio-temporal features, which struggle to generalize to new datasets without the normality reference of training data from the target dataset. Differently, we use action semantics as a generalizable representation for normalizing flow input and leverage experiential typicality labels to learn domain-general boundaries between normal and abnormal behavior.</p>

<h2 class="relative group">C. Test-time Uniqueness Analysis
    <div id="c-test-time-uniqueness-analysis" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-test-time-uniqueness-analysis" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The goal of this component is to serve as a complementary perspective of typicality, deriving scene-adaptive boundaries by considering the context of the target scene. To this end, we propose a context uniqueness analysis module during the inference of the unseen VAD dataset.</p>
<p>Unlike action recognition datasets, surveillance videos contain richer contextual information, featuring longer temporal spans, larger numbers of people, and more diverse behavioral patterns. For such a video, H skeleton sequences {X1 , &hellip;, X H } are extracted, where each sequence comprises L i -frame poses, represented as Xi = {P1 , &hellip;, P L i }. Here, P t ∈ R J×2 comprises J keypoints, each defined by a pair of coordinate values. Targeted at frame-level anomaly scoring, the sequences are segmented into shorter skeleton snippets, denoted as A i ∈ R C×J×T , each of which is then individually scored based on its contextual information.</p>
<p>Spatio-temporal context. As shown in Fig. 2, to gain a fine-grained context understanding of the scene, we construct</p>
<p>two types of spatio-temporal context graphs: a cross-person graph G c and a self-inspection graph G s . The first graph is constructed by retrieving the feature nearest neighbors among the surrounding skeleton snippets, while the second one is constructed by retrieving the feature nearest neighbors from different time segments of the current person. In this way, we can filter out some unrelated activities and focus solely on behaviors related to the current individual. Given a skeleton snippet Ai with feature F s i , the cross-person graph is defined as G c i = {V
i c V
i
, E
i c E
i }, where V
i c V
i = {Ai , Nc Nc (Ai)} denotes the node set and E i c = {(i, j)| j ∈ Nc Nc } denotes the edge set. Besides, during the preprocessing of skeleton snippets, Aiis associated with a human trajectory index pi and timestamp ti . The neighborhood Nc Nc is formulated as:</p>
<!-- formula-not-decoded -->
<p>where d(·) represents the Euclidean distance, and D k c refers to the k-th smallest value for the cross-person distances. The second graph, which depicts self-inspection, is defined as G s i = {V
i s V
i
, E
i s E
i }, where V
i s V
i = {Ai , Ns Ns (Ai)} denotes the node set and E i s = {(i, j)| j ∈ Ns Ns ) denotes the edge set. Then, the neighborhood Ns Ns is formulated as:</p>
<!-- formula-not-decoded -->
<p>where D k s refers to the k-th smallest value for the selfinspection distances. α is a threshold that masks out a period of time before and after the current time window, as the individual&rsquo;s state tends to remain stable during adjacent periods.</p>
<p>Uniqueness scores. Since abnormal activities are rare, anomalies in real-world surveillance videos often differ from other activities in both spatial and temporal context, which is referred to as uniqueness. Based on the pre-trained discriminative skeleton features, uniqueness can be represented as the feature distances between a query node and other nodes in the built graph. Specifically, the uniqueness score S u for individual i is obtained by taking the larger one of the crossperson and self-inspection distances, formulated as follows:</p>
<!-- formula-not-decoded -->
<p>Holistic anomaly scoring. By integrating the complementary typicality S t i scores and the uniqueness scores S u i , our model can capture both typical anomalies in language prior and unique anomalies in spatio-temporal contexts. This helps gain a comprehensive understanding of anomalies in new scenes, where the holistic anomaly score of individual i is defined as:</p>
<!-- formula-not-decoded -->
<p>Finally, the frame-level anomaly scores are obtained by taking the highest score among all individuals within each frame. If any individual is considered anomalous, the entire frame is classified as anomalous. For frames where no individuals are detected, it is classified as a normal frame. In this condition, the anomaly score is assigned the minimum value among all scores in that video, following the approach in [6].</p>
<p>TABLE II THE DETAILS OF OUR ZERO -SHOT VIDEO ANOMALY DETECTION BENCHMARKS. EACH SNIPPET CONTAINS 16 FRAMES OF SKELETON DATA WITH A 1-FRAME INTERVAL , WHILE THE SNIPPETS OF UCF-CRIME* ARE SAMPLED WITH A 16-FRAME INTERVAL AS ITS VIDEOS ARE TOO LONG .</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Resolution</th>
          <th>Test Video  Num.</th>
          <th>Scenes  Num.</th>
          <th>Snippet Num.</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ShanghaiTech [1]</td>
          <td>480×856</td>
          <td>107</td>
          <td>13</td>
          <td>156,571</td>
      </tr>
      <tr>
          <td>UBnormal [22]</td>
          <td>720×1280</td>
          <td>211</td>
          <td>29</td>
          <td>315,416</td>
      </tr>
      <tr>
          <td>NWPU [23]</td>
          <td>multiple</td>
          <td>242</td>
          <td>43</td>
          <td>723,490</td>
      </tr>
      <tr>
          <td>UCF-Crime [2]</td>
          <td>240×320</td>
          <td>290</td>
          <td>&gt;50</td>
          <td>152,231*</td>
      </tr>
  </tbody>
</table>

<h2 class="relative group">IV. EXPERIMENTS
    <div id="iv-experiments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#iv-experiments" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">A. Dataset and Implementation Details
    <div id="a-dataset-and-implementation-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-dataset-and-implementation-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Dataset. The training of our model is conducted on the Kinect-400-skeleton dataset [37], [38], while the ZS-VAD capability of our model is evaluated on four large-scale VAD datasets: ShanghaiTech [1], UBnormal [22], NWPU [23] and UCF-Crime [2]. Note that we only use the test set of these four VAD datasets.</p>
<p>For the training of our model, we use the external Kinect400-skeleton [38] dataset. It is not intended for VAD tasks but for action recognition, which is gathered from YouTube videos covering 400 action classes. We utilize the preprocessed skeleton data obtained from ST-GCN [38] for training. For evaluation, we take four VAD-relevant datasets. Compared to some early VAD benchmarks [43], [44] that involve single scenes staged and captured at one location, the four datasets we evaluated are more extensive, encompassing a wider variety of scenes. Consequently, these four datasets are better suited for testing the model&rsquo;s zero-shot capabilities and assessing its cross-scenario performance. The details are summarized in Table II and the following descriptions. (1) ShanghaiTech. It is a widely-used benchmark for one-class video anomaly detection, which consists of 330 training videos and 107 test videos from 13 different scenes. (2) UBnormal. It is a synthetic dataset with virtual objects and real-world environments. It consists of 186 training videos and 211 test videos from 29 different scenes. (3) NWPU. It is a newly published dataset that contains some scene-dependent anomaly types. It comprises 305 training videos and 242 testing videos from 43 scenes. (4) UCF-Crime. It is a large-scale dataset with 1900 long untrimmed surveillance videos. The 290 testing videos are used for our evaluation.</p>
<p>Implementation Details. For a fair comparison, we directly use the skeleton data of ShanghaiTech and UBnormal from STG-NF [6]. For NWPU and UCF-Crime, as they do not have open-source skeleton data, we resort to utilizing AlphaPose [21] for data extraction. We use a segment window T = 16 and a stride of 1 to divide each sequence into snippets. Specifically, we use a stride of 16 for UCF-Crime because its videos are too long. For the backbone, we use multi-scale CTR-GCN [46] (2.1M) as the skeleton encoder and use a 4-layer feature normalizing flow [47] (2.9M) to model the normality probability. During training, we use the &ldquo;ViT-B/32&rdquo; CLIP [33] as the text encoder, and GPT-3.5-Turbo as our knowledge engine. During inference, these two models are</p>
<p>TABLE III ZERO -SHOT VIDEO ANOMALY DETECTION PERFORMANCE ON THE FOUR LARGE -SCALE DATASETS, SHANGHAITECH, UBNORMAL, NWPU, AND UCF-CRIME , WHERE THE SUBSCRIPT DENOTES THE NUMBER OF SCENES .</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Training  VAD</th>
          <th>SHT13</th>
          <th>Testing Set UB29 NWPU43</th>
          <th>Testing Set UB29 NWPU43</th>
          <th>UCFC&gt;50</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>LVLM Imagebind [45]  LAVAD [35]</td>
          <td>✘</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>55.8</td>
      </tr>
      <tr>
          <td>LAVAD [35]</td>
          <td>✘</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>80.3</td>
      </tr>
      <tr>
          <td>Frame/Object HF2-VAD [4]</td>
          <td>SHT</td>
          <td>76.2</td>
          <td>59.5</td>
          <td>58.3</td>
          <td>52.9</td>
      </tr>
      <tr>
          <td>Jigsaw-VAD [3]</td>
          <td>SHT</td>
          <td>84.3</td>
          <td>58.6</td>
          <td>61.1</td>
          <td>53.3</td>
      </tr>
      <tr>
          <td>Skeleton MocoDAD [19]</td>
          <td>SHT</td>
          <td>77.6</td>
          <td>67.0</td>
          <td>56.4</td>
          <td>51.8</td>
      </tr>
      <tr>
          <td>MocoDAD [19]</td>
          <td>UB</td>
          <td>76.0</td>
          <td>68.4</td>
          <td>56.6</td>
          <td>52.0</td>
      </tr>
      <tr>
          <td>STG-NF [6]</td>
          <td>SHT</td>
          <td>85.9</td>
          <td>68.8</td>
          <td>57.6</td>
          <td>51.6</td>
      </tr>
      <tr>
          <td>STG-NF [6]</td>
          <td>UB</td>
          <td>83.0</td>
          <td>71.8</td>
          <td>57.9</td>
          <td>51.9</td>
      </tr>
      <tr>
          <td>Ours</td>
          <td>✘</td>
          <td>84.1</td>
          <td>74.5</td>
          <td>57.9  62.1</td>
          <td>51.9 627</td>
      </tr>
  </tbody>
</table>
<p>removed. For the hyperparameters, the batch size is set to 1024, and the Adam optimizer is used with a learning rate of 0.0005. Additionally, β n , β a , k, and α are set to 90%, 10%, 16, and 4, respectively. For the evaluation metrics, we follow common practice [1], [2], [6] by using the micro-average frame-level AUC as the evaluation metric, which involves concatenating all frames and calculating the score.</p>

<h2 class="relative group">B. Main Results
    <div id="b-main-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-main-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We conduct a comprehensive comparison of the performance of ZS-VAD, comparing the frame-based/objectbased [3], [4], skeleton-based [6], [19], and LVLM-based methods [35], [45].</p>
<p>Comparison with frame/object-based methods. We use their open-source checkpoints trained on the ShanghaiTech to evaluate the zero-shot performance on the remaining three VAD datasets. As shown in Table III, their generalization capabilities on new scene datasets are relatively poor due to the influence of human appearance and background variations.</p>
<p>Comparison with skeleton-based methods. We use their open-source checkpoints trained on the ShanghaiTech or UBnormal to evaluate on the remaining three VAD datasets. The performance of prevalent skeleton-based methods is still underwhelming due to a lack of understanding of complex normal and abnormal behaviors without target training data. Compared with our baseline STG-NF, our proposed method improves the frame-level AUC-ROC by 1.1% on ShanghaiTech, 5.7% on UBnormal, 4.2% on NWPU, and 10.8% on UCF-Crime. We also compare their performance in the fullshot setting, where target domain data is used for training. Table IV shows that our zero-shot approach can achieve comparable or even superior results to SoTA full-shot performance. To evaluate our method under the popular full-shot setting, we train our normalizing flow only on VAD normal data to model the normal distribution and test it on the same domain. The results outperform state-of-the-art (SOTA) full-shot methods.</p>
<p>Comparison with LVLM-based methods. With the advancements in Large Vision-Language Models (LVLMs) [49]–</p>
<p>TABLE IV OUR ZERO -SHOT PERFORMANCE VS. SOTA FULL -SHOT PERFORMANCE . WE ALSO PROVIDE A VERSION NAMED OURS -FULL TO EVALUATE OUR METHOD UNDER THE POPULAR FULL -SHOT SETTING .</p>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Method</th>
          <th>Training  VAD</th>
          <th>Testing HT UB</th>
          <th>Testing HT UB</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Setting</td>
          <td>Method</td>
          <td>VAD</td>
          <td>SHT</td>
          <td>UB</td>
      </tr>
      <tr>
          <td>zero-shot</td>
          <td>Ours</td>
          <td>✘</td>
          <td>84.1</td>
          <td>74.5</td>
      </tr>
      <tr>
          <td>full-shot</td>
          <td>HF2-VAD [4]</td>
          <td>SHT / UB</td>
          <td>76.2</td>
          <td>-</td>
      </tr>
      <tr>
          <td>full-shot</td>
          <td>Jigsaw-VAD [3]</td>
          <td>SHT / UB</td>
          <td>84.3</td>
          <td>-</td>
      </tr>
      <tr>
          <td>full-shot</td>
          <td>g SSMTL++ [48]</td>
          <td>SHT / UB</td>
          <td>83.8</td>
          <td>62.1</td>
      </tr>
      <tr>
          <td>full-shot</td>
          <td>GEPC [25]</td>
          <td>SHT / UB</td>
          <td>76.1</td>
          <td>53.4</td>
      </tr>
      <tr>
          <td>full-shot</td>
          <td>MocoDAD [19]</td>
          <td>SHT / UB</td>
          <td>77.6</td>
          <td>68.4</td>
      </tr>
      <tr>
          <td>full-shot</td>
          <td>STG-NF [6]</td>
          <td>SHT / UB</td>
          <td>85.9</td>
          <td>71.8</td>
      </tr>
      <tr>
          <td>full-shot</td>
          <td>FG-Diff [26]</td>
          <td>- / UB</td>
          <td>-</td>
          <td>68.9</td>
      </tr>
      <tr>
          <td>full-shot</td>
          <td>Ours-full</td>
          <td>SHT / UB</td>
          <td>86.0</td>
          <td>78.2</td>
      </tr>
  </tbody>
</table>
<p>TABLE V ABLATION EXPERIMENTS OF THE TYPICALITY MODELING MODULE .</p>
<table>
  <thead>
      <tr>
          <th>Experiments</th>
          <th>SHT</th>
          <th>UB</th>
          <th>NWPU</th>
          <th>UCFC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>(a) ours w/o aligning</td>
          <td>83.2</td>
          <td>69.4</td>
          <td>60.4</td>
          <td>59.5</td>
      </tr>
      <tr>
          <td>(b) ours w/o selection</td>
          <td>-</td>
          <td>64.1</td>
          <td>61.9</td>
          <td>56.7</td>
      </tr>
      <tr>
          <td>(c) ours w/o NF</td>
          <td>83.4</td>
          <td>72.2</td>
          <td>62.6</td>
          <td>60.5</td>
      </tr>
      <tr>
          <td>(d) prompt score</td>
          <td>81.3</td>
          <td>64.4</td>
          <td>61.5</td>
          <td>61</td>
      </tr>
      <tr>
          <td>(e) ours</td>
          <td>84.1</td>
          <td>74.5</td>
          <td>62.1</td>
          <td>62.7</td>
      </tr>
  </tbody>
</table>
<p>[51], LAVAD [35] proposes a zero-shot video anomaly detection (ZS-VAD) framework. However, it relies on multistage reasoning and the coordination of multiple large models with over 13 billion (B) parameters, posing challenges for widespread deployment. In contrast, we develop a lightweight zero-shot anomaly detector with a mere 5.0 million (M) parameters, just one in two thousand of LAVAD&rsquo;s parameters.</p>

<h2 class="relative group">C. Ablation Study
    <div id="c-ablation-study" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-ablation-study" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Ablation of typicality module. We conduct ablation experiments on the typicality modeling module with the following settings: (a) removing the aligning stage, training an STGNF [6] network with our typicality data; (b) removing the collection phase, training with VAD source data (SHT); (c) removing the normalizing flow and calculating typicality scores using k-nearest neighbors distance techniques. As shown in Table V, the model shows poor performance without the aligning stage, as it fails to learn generalizable and discriminative semantic representations. Moreover, performance deteriorates without the selection of typicality action knowledge, as the model can only learn a limited normality boundary from the VAD source data. Furthermore, without the normalizing flow (NF), the model also loses flexibility in modeling the distribution of typical behaviors.</p>
<p>Ablation of uniqueness module. We ablate the uniqueness scores and the holistic scores in this part. As demonstrated in Table VI, when only using the cross-person distance, the model can identify contextual anomalies with acceptable performance. When combined with the self-inspection score, the model can spot changes in motion states, aiding in detecting a wider range of anomalies. The reason for the suboptimal</p>
<p>Fig. 3. Example results of our method that succeed in capturing typical anomalies. STG-NF [6] fails to detect the &ldquo;jumping in the street&rdquo; event, while ours performs well through action typicality learning. Each individual (blue skeleton) has a predicted anomaly score (red font), where the frame-level score (orange line) is defined as the maximum among all individuals in that frame.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_842f8992b0eb5aac5642dcbd2b9cdd7a24ce69159d19a1abc2992dfef77c3ca3.png"
    ></figure>
<p>Fig. 4. Example results of our method that succeed in capturing unique anomalies. STG-NF misclassifies unseen normal events during periods where &ldquo;riding&rdquo; occurs as anomalies, whereas our method correctly identifies them as normal by recognizing their contextual similarity. Moreover, STG-NF fails to detect the &ldquo;photographing in restricted areas&rdquo; anomaly, while our approach successfully identifies it by recognizing a sudden change in the person&rsquo;s movement trajectory.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_8d44a97d857d072ebca6187e14119fefb805e0d871752d878d09f7b511381a11.png"
    ></figure>
<p>TABLE VI ABLATION STUDY OF THE UNIQUENESS ANALYSIS MODULE AND HOLISTIC ANOMALY SCORING .</p>
<table>
  <thead>
      <tr>
          <th>Typ.</th>
          <th>Cross</th>
          <th>Self</th>
          <th>SHT</th>
          <th>UB</th>
          <th>NWPU</th>
          <th>UCFC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td>81.9</td>
          <td>73.2</td>
          <td>62.1</td>
          <td>59.6</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td>81.9</td>
          <td>62.9</td>
          <td>60.7</td>
          <td>59.9</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td>67.8</td>
          <td>60.1</td>
          <td>61</td>
          <td>62.6</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td></td>
          <td>82</td>
          <td>64.5</td>
          <td>61.7</td>
          <td>61</td>
      </tr>
      <tr>
          <td>✓</td>
          <td>✓</td>
          <td>✓</td>
          <td>84.1</td>
          <td>74.5</td>
          <td>62.1</td>
          <td>62.7</td>
      </tr>
  </tbody>
</table>
<p>TABLE VII COMPARISON WITH PROMPT -BASED METHODS .</p>
<table>
  <thead>
      <tr>
          <th>Experiments</th>
          <th>SHT</th>
          <th>UB</th>
          <th>NWPU</th>
          <th>UCFC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>(a) normal prompts</td>
          <td>81.3</td>
          <td>64.4</td>
          <td>61.5</td>
          <td>61</td>
      </tr>
      <tr>
          <td>(b) abnormal prompts</td>
          <td>80.7</td>
          <td>63.6</td>
          <td>61</td>
          <td>60.2</td>
      </tr>
      <tr>
          <td>(c) ensemble prompts</td>
          <td>80.7</td>
          <td>63.9</td>
          <td>61.1</td>
          <td>61.3</td>
      </tr>
      <tr>
          <td>(d) ours</td>
          <td>84.1</td>
          <td>74.5</td>
          <td>62.1</td>
          <td>62.7</td>
      </tr>
  </tbody>
</table>
<p>performance of uniqueness score on UBnormal is that UBnormal is a synthetic dataset where some videos contain only one person with relatively short movement durations, which does not align well with real surveillance video scenarios. By integrating both the typicality and uniqueness modules, our approach can achieve optimal performance.</p>
<p>Comparison with prompt-based methods. Since promptbased techniques have been popular in other zero-shot tasks [11], [36], we conduct experiments to compare our typicality module with theirs. To this end, we design typical normal prompts, typical abnormal prompts, and the ensemble prompts, then use the skeleton-prompt similarity as the anomaly score. In detail, we use a normal prompt list: [&ldquo;usual&rdquo;, &ldquo;normal&rdquo;, &ldquo;daily&rdquo;, &ldquo;stable&rdquo;, &ldquo;safe&rdquo;], and an abnormal prompt list [&ldquo;danger&rdquo;, &ldquo;violence&rdquo;, &ldquo;suddenness&rdquo;, &ldquo;unusual&rdquo;, &ldquo;instability&rdquo;]. The prompts are encoded into text features and compute similarity with the skeleton features, together with our uniqueness analysis. As shown in Table V (d) and Table VII, the results are suboptimal. Unlike various forms of text seen in CLIP image-text alignment, the current skeletontext alignment scheme has only encountered text of action class names, thus the alignment capability for prompt text is weak. Our method, on the other hand, distills LLM&rsquo;s</p>
<p>TABLE VIII ABLATION STUDIES OF THE BACKBONE NETWORKS. THE PERFORMANCE IS EVALUATED ON UBNORMAL / NWPU / UCF-CRIME DATASETS .</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Backbone</th>
          <th>Params.</th>
          <th>Performance</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Jigsaw [3]</td>
          <td>3D-Conv</td>
          <td>1.5M</td>
          <td>58.6 / 61.1 / 53.3</td>
      </tr>
      <tr>
          <td>HF2-VAD [4]</td>
          <td>MemAE+C-VAE</td>
          <td>36.4M</td>
          <td>9.5 / 58.3 / 52.9</td>
      </tr>
      <tr>
          <td>STG-NF [4]</td>
          <td>STG-NF</td>
          <td>0.7K</td>
          <td>68.8 / 57.6 / 51.</td>
      </tr>
      <tr>
          <td>[]  STG- STG-CN  CTR-GCN</td>
          <td>STG-NF  STG-CN + NF  TR-GCN + NF</td>
          <td>0.7K  4.1M  50M</td>
          <td>0 / 60.1 / 58.5</td>
      </tr>
      <tr>
          <td>STG-CN + NF</td>
          <td>TG-CN + NF</td>
          <td>73.1 / 61.3 / 61</td>
          <td>0 / 60.1 / 58.5</td>
      </tr>
      <tr>
          <td>CTR-GCN + NF</td>
          <td>5.0M</td>
          <td>74.5 / 62.1 / 62.7</td>
          <td>0 / 60.1 / 58.5</td>
      </tr>
  </tbody>
</table>
<p>knowledge and learns typicality distribution, avoiding directly using the skeleton-prompt similarity as anomaly scores.</p>
<p>Ablation of backbone. In this part, we ablate our backbone. For a fair comparison with our baseline STG-NF [6], we attempt to use STG-NF as the backbone. However, STGNF takes XY -coordinates as input, with only 2 dimensions, making it extremely lightweight yet difficult to learn highlevel features. We then use STG-NF to learn the typicality skeleton coordinate inputs to obtain the typicality score. (Note that the results in Table 3 (a) of the main paper also consider the uniqueness score, resulting in the higher performance). As shown in Table VIII, using STG-NF as our backbone still demonstrates better performance compared to vanilla STGNF, highlighting the effectiveness of our typicality training. In addition, when we switch our backbone from CTR-GCN [46] to STG-CN [38], the model becomes more lightweight and the performance remains good.</p>
<p>Hyper-parameter ablations. We ablate the nearest neighborhood (NN) number k and the masking threshold α in the uniqueness analysis module. As shown in Table IX, our method is robust for these two hyper-parameters. Choosing an appropriate k can filter out some unrelated activities and focus solely on behaviors related to the current skeleton snippets. In addition, taking the average of the k neighbors helps suppress noise, which also makes our model insensitive to α .</p>
<p>We also ablate the hyperparameters in the typicality knowledge selection step. As shown in Table X, the optimal values of β n and β a are 0.9 (90%) and 0.1 (10%), respectively. A smaller β a can enhance performance by filtering out noisy data and normal snippets within the anomalous sequences.</p>

<h2 class="relative group">D. Visualization Results
    <div id="d-visualization-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-visualization-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Fig. 3 presents the qualitative localization results of typical anomalies. STG-NF fails to detect the &ldquo;jumping&rdquo; anomaly due to its low-level skeleton representation, which is unable to distinguish between indiscernible anomaly patterns that are similar to normal patterns. Disturbed by the skeletal noise of frame 350, it erroneously identifies the anomaly&rsquo;s position. In contrast, our model maps the skeleton snippets to a highlevel space with generalizable and discriminative semantic information, allowing it to identify the anomaly based on our trained decision boundary. In surveillance scenarios lacking training samples, our model can still be effectively utilized to detect certain typical abnormal behaviors.</p>
<p>Fig. 4 shows the qualitative localization results of unique anomalies. Existing skeleton-based methods rely on the source</p>
<p>TABLE IX ABLATION RESULTS OF TWO MAIN HYPER -PARAMETER .</p>
<table>
  <thead>
      <tr>
          <th>(k, α)</th>
          <th>SHT</th>
          <th>UB</th>
          <th>(k, α)</th>
          <th>SHT</th>
          <th>UB</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>(1, 1)</td>
          <td>83.9</td>
          <td>74.2</td>
          <td>(16, 1)</td>
          <td>84.2</td>
          <td>74.2</td>
      </tr>
      <tr>
          <td>(4, 1)</td>
          <td>84.1</td>
          <td>74.3</td>
          <td>(16, 2)</td>
          <td>84.2</td>
          <td>74.4</td>
      </tr>
      <tr>
          <td>(16, 1)</td>
          <td>84.1</td>
          <td>74.5</td>
          <td>(16, 3)</td>
          <td>84</td>
          <td>74.5</td>
      </tr>
      <tr>
          <td>(64, 1)</td>
          <td>83.7</td>
          <td>74.7</td>
          <td>(16, 4)</td>
          <td>84.1</td>
          <td>74.5</td>
      </tr>
  </tbody>
</table>
<p>TABLE X HYPER -PARAMETER SENSITIVITY OF THE SELECTION RATIO .</p>
<table>
  <thead>
      <tr>
          <th>(β n , βa)</th>
          <th>SHT</th>
          <th>UB</th>
          <th>(β n , βa)</th>
          <th>SHT</th>
          <th>UB</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>(0.1, 0.1)</td>
          <td>82.8</td>
          <td>68.3</td>
          <td>(0.9, 0.1)</td>
          <td>84.1</td>
          <td>74.5</td>
      </tr>
      <tr>
          <td>(0.3, 0.1)</td>
          <td>83</td>
          <td>70.9</td>
          <td>(0.9, 0.3)</td>
          <td>84.1</td>
          <td>73.3</td>
      </tr>
      <tr>
          <td>(0.5, 0.1)</td>
          <td>83.4</td>
          <td>74.1</td>
          <td>(0.9, 0.5)</td>
          <td>84.1</td>
          <td>72.3</td>
      </tr>
      <tr>
          <td>(0.7, 0.1)</td>
          <td>83.9</td>
          <td>74.3</td>
          <td>(0.9, 0.7)</td>
          <td>83.5</td>
          <td>72</td>
      </tr>
      <tr>
          <td>(0.9, 0.1)</td>
          <td>84.1</td>
          <td>74.5</td>
          <td>(0.9, 0.9)</td>
          <td>82.5</td>
          <td>71.8</td>
      </tr>
  </tbody>
</table>
<p>normal data for training. When the source domain does not include some novel behavior that appears in the target domain, these behavior will be classified as anomalies. Consequently, STG-NF erroneously localizes the anomaly during time periods when &ldquo;riding&rdquo; is present. In contrast, our model can analyze the spatio-temporal differences and establish sceneadaptive decision boundaries. Since &ldquo;riding&rdquo; occurs multiple times in the video, its uniqueness score is low. On the other hand, &ldquo;photographing at restricted areas&rdquo; exhibits significant differences from the surrounding people&rsquo;s behavior and appears as a sudden change in the person&rsquo;s movement trajectory, resulting in a corresponding increase in its anomaly score.</p>

<h2 class="relative group">V. CONCLUSION
    <div id="v-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#v-conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this paper, we identify the advantages of the skeletonbased approach in ZS-VAD, and introduce a novel framework that can generalize to various target scenes with typicality and uniqueness learning. First, we propose a language-guided typicality modeling module that effectively learns the typical distribution of normal and abnormal behavior. Secondly, we propose a test-time uniqueness analysis module to derive scene-adaptive boundaries. Experiments demonstrate the effectiveness of our model. Limitations and future work: Our work aims to exploit the full potential of skeleton data in the ZS-VAD task, while the complex relationships between behaviors and scenes will be left for future work.</p>

<h2 class="relative group">REFERENCES
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>
<p>[1] W. Liu, W. Luo, D. Lian, and S. Gao, &ldquo;Future frame prediction for anomaly detection–a new baseline,&rdquo; in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 6536– 6545.</p>
</li>
<li>
<p>[2] W. Sultani, C. Chen, and M. Shah, &ldquo;Real-world anomaly detection in surveillance videos,&rdquo; in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 6479–6488.</p>
</li>
<li>
<p>[3] G. Wang, Y. Wang, J. Qin, D. Zhang, X. Bao, and D. Huang, &ldquo;Video anomaly detection by solving decoupled spatio-temporal jigsaw puzzles,&rdquo; in European Conference on Computer Vision. Springer, 2022, pp. 494–511.</p>
</li>
<li>
<p>[4] Z. Liu, Y. Nie, C. Long, Q. Zhang, and G. Li, &ldquo;A hybrid video anomaly detection framework via memory-augmented flow reconstruction and flow-guided frame prediction,&rdquo; in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 13 588–13 597.</p>
</li>
<li>
<p>[5] C. Tang, S. Zhou, Y. Li, Y. Dong, and L. Wang, &ldquo;Advancing pre-trained teacher: towards robust feature discrepancy for anomaly detection,&rdquo; arXiv preprint arXiv:2405.02068, 2024.</p>
</li>
<li>
<p>[6] O. Hirschorn and S. Avidan, &ldquo;Normalizing flows for human pose anomaly detection,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 13 545–13 554.</p>
</li>
<li>
<p>[7] L. Wang, J. Tian, S. Zhou, H. Shi, and G. Hua, &ldquo;Memory-augmented appearance-motion network for video anomaly detection,&rdquo; Pattern Recognition, vol. 138, p. 109335, 2023.</p>
</li>
<li>
<p>[8] M. Cho, M. Kim, S. Hwang, C. Park, K. Lee, and S. Lee, &ldquo;Look around for anomalies: weakly-supervised anomaly detection via context-motion relational learning,&rdquo; in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 12 137–12 146.</p>
</li>
<li>
<p>[9] H. Shi, L. Wang, S. Zhou, G. Hua, and W. Tang, &ldquo;Abnormal ratios guided multi-phase self-training for weakly-supervised video anomaly detection,&rdquo; IEEE Transactions on Multimedia, 2023.</p>
</li>
<li>
<p>[10] Y. Liu, S. Li, Y. Zheng, Q. Chen, C. Zhang, and S. Pan, &ldquo;Arc: A generalist graph anomaly detector with in-context learning,&rdquo; arXiv preprint arXiv:2405.16771, 2024.</p>
</li>
<li>
<p>[11] J. Jeong, Y. Zou, T. Kim, D. Zhang, A. Ravichandran, and O. Dabeer, &ldquo;Winclip: Zero-/few-shot anomaly classification and segmentation,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 19 606–19 616.</p>
</li>
<li>
<p>[12] Z. Gu, B. Zhu, G. Zhu, Y. Chen, H. Li, M. Tang, and J. Wang, &ldquo;Filo: Zero-shot anomaly detection by fine-grained description and highquality localization,&rdquo; arXiv preprint arXiv:2404.13671, 2024.</p>
</li>
<li>
<p>[13] Y. Cao, J. Zhang, L. Frittoli, Y. Cheng, W. Shen, and G. Boracchi, &ldquo;Adaclip: Adapting clip with hybrid learnable prompts for zero-shot anomaly detection,&rdquo; arXiv preprint arXiv:2407.15795, 2024.</p>
</li>
<li>
<p>[14] Q. Zhou, G. Pang, Y. Tian, S. He, and J. Chen, &ldquo;AnomalyCLIP: Objectagnostic prompt learning for zero-shot anomaly detection,&rdquo; in The Twelfth International Conference on Learning Representations, 2024.</p>
</li>
<li>
<p>[15] A. Aich, K.-C. Peng, and A. K. Roy-Chowdhury, &ldquo;Cross-domain video anomaly detection without target domain adaptation,&rdquo; in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , 2023, pp. 2579–2591.</p>
</li>
<li>
<p>[16] J. Micorek, H. Possegger, D. Narnhofer, H. Bischof, and M. Kozinski, &ldquo;Mulde: Multiscale log-density estimation via denoising score matching for video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18 868– 18 877.</p>
</li>
<li>
<p>[17] R. Morais, V. Le, T. Tran, B. Saha, M. Mansour, and S. Venkatesh, &ldquo;Learning regularity in skeleton trajectories for anomaly detection in videos,&rdquo; in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 11 996–12 004.</p>
</li>
<li>
<p>[18] S. Yu, Z. Zhao, H. Fang, A. Deng, H. Su, D. Wang, W. Gan, C. Lu, and W. Wu, &ldquo;Regularity learning via explicit distribution modeling for skeletal video anomaly detection,&rdquo; IEEE Transactions on Circuits and Systems for Video Technology, 2023.</p>
</li>
<li>
<p>[19] A. Flaborea, L. Collorone, G. M. D. Di Melendugno, S. D&rsquo;Arrigo, B. Prenkaj, and F. Galasso, &ldquo;Multimodal motion conditioned diffusion model for skeleton-based video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 10 318–10 329.</p>
</li>
<li>
<p>[20] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh, &ldquo;Openpose: Realtime multi-person 2d pose estimation using part affinity fields,&rdquo; IEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 1, pp. 172–186, 2019.</p>
</li>
<li>
<p>[21] H.-S. Fang, J. Li, H. Tang, C. Xu, H. Zhu, Y. Xiu, Y.-L. Li, and C. Lu, &ldquo;Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time,&rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 6, pp. 7157–7173, 2022.</p>
</li>
<li>
<p>[22] A. Acsintoae, A. Florescu, M.-I. Georgescu, T. Mare, P. Sumedrea, R. T. Ionescu, F. S. Khan, and M. Shah, &ldquo;Ubnormal: New benchmark for supervised open-set video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 20 143–20 153.</p>
</li>
<li>
<p>[23] C. Cao, Y. Lu, P. Wang, and Y. Zhang, &ldquo;A new comprehensive benchmark for semi-supervised video anomaly detection and anticipation,&rdquo; in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 20 392–20 401.</p>
</li>
<li>
<p>[24] S. Sun and X. Gong, &ldquo;Hierarchical semantic contrast for scene-aware video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 22 846–22 856.</p>
</li>
<li>
<p>[25] A. Markovitz, G. Sharir, I. Friedman, L. Zelnik-Manor, and S. Avidan, &ldquo;Graph embedded pose clustering for anomaly detection,&rdquo; in Proceed-</p>
</li>
</ul>
<p>ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 539–10 547.</p>
<ul>
<li>[26] X. Tan, H. Wang, X. Geng, and L. Wang, &ldquo;Frequency-guided diffusion model with perturbation training for skeleton-based video anomaly detection,&rdquo; arXiv preprint arXiv:2412.03044, 2024.</li>
<li>[27] D. P. Kingma and P. Dhariwal, &ldquo;Glow: Generative flow with invertible 1x1 convolutions,&rdquo; Advances in neural information processing systems , vol. 31, 2018.</li>
<li>[28] R. Wu, Y. Chen, J. Xiao, B. Li, J. Fan, F. Dufaux, C. Zhu, and Y. Liu, &ldquo;Da-flow: Dual attention normalizing flow for skeleton-based video anomaly detection,&rdquo; arXiv preprint arXiv:2406.02976, 2024.</li>
<li>[29] A. Li, C. Qiu, M. Kloft, P. Smyth, M. Rudolph, and S. Mandt, &ldquo;Zeroshot anomaly detection via batch normalization,&rdquo; Advances in Neural Information Processing Systems, vol. 36, 2024.</li>
<li>[30] T. Aota, L. T. T. Tong, and T. Okatani, &ldquo;Zero-shot versus manyshot: Unsupervised texture anomaly detection,&rdquo; in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , 2023, pp. 5564–5572.</li>
<li>[31] X. Chen, Y. Han, and J. Zhang, &ldquo;A zero-/fewshot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&amp;2: 1st place on zero-shot ad and 4th place on few-shot ad,&rdquo; arXiv preprint arXiv:2305.17382, vol. 2, no. 4, 2023.</li>
<li>[32] A. Miyai, J. Yang, J. Zhang, Y. Ming, Y. Lin, Q. Yu, G. Irie, S. Joty, Y. Li, H. Li et al., &ldquo;Generalized out-of-distribution detection and beyond in vision language model era: A survey,&rdquo; arXiv preprint arXiv:2407.21794, 2024.</li>
<li>[33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., &ldquo;Learning transferable visual models from natural language supervision,&rdquo; in International conference on machine learning. PMLR, 2021, pp. 8748–8763.</li>
<li>[34] D. Guo, Y. Fu, and S. Li, &ldquo;Ada-vad: Domain adaptable video anomaly detection,&rdquo; in Proceedings of the 2024 SIAM International Conference on Data Mining (SDM). SIAM, 2024, pp. 634–642.</li>
<li>[35] L. Zanella, W. Menapace, M. Mancini, Y. Wang, and E. Ricci, &ldquo;Harnessing large language models for training-free video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18 527–18 536.</li>
<li>[36] F. Sato, R. Hachiuma, and T. Sekii, &ldquo;Prompt-guided zero-shot anomaly action recognition using pretrained deep skeleton features,&rdquo; in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 6471–6480.</li>
<li>[37] J. Carreira and A. Zisserman, &ldquo;Quo vadis, action recognition? a new model and the kinetics dataset,&rdquo; in proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 6299–6308.</li>
<li>[38] S. Yan, Y. Xiong, and D. Lin, &ldquo;Spatial temporal graph convolutional networks for skeleton-based action recognition,&rdquo; in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, 2018.</li>
<li>[39] X. Liu, S. Zhou, L. Wang, and G. Hua, &ldquo;Parallel attention interaction network for few-shot skeleton-based action recognition,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 1379–1388.</li>
<li>[40] Y. Wang, S. Zhou, K. Xia, and L. Wang, &ldquo;Learning discriminative spatiotemporal representations for semi-supervised action recognition,&rdquo; arXiv preprint arXiv:2404.16416, 2024.</li>
<li>[41] M. Wang, J. Xing, and Y. Liu, &ldquo;Actionclip: A new paradigm for video action recognition,&rdquo; arXiv preprint arXiv:2109.08472, 2021.</li>
<li>[42] W. Xiang, C. Li, Y. Zhou, B. Wang, and L. Zhang, &ldquo;Generative action description prompts for skeleton-based action recognition,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 10 276–10 285.</li>
<li>[43] C. Lu, J. Shi, and J. Jia, &ldquo;Abnormal event detection at 150 fps in matlab,&rdquo; in Proceedings of the IEEE international conference on computer vision , 2013, pp. 2720–2727.</li>
<li>[44] W. Li, V. Mahadevan, and N. Vasconcelos, &ldquo;Anomaly detection and localization in crowded scenes,&rdquo; IEEE transactions on pattern analysis and machine intelligence, vol. 36, no. 1, pp. 18–32, 2013.</li>
<li>[45] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra, &ldquo;Imagebind: One embedding space to bind them all,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 15 180–15 190.</li>
<li>[46] Y. Chen, Z. Zhang, C. Yuan, B. Li, Y. Deng, and W. Hu, &ldquo;Channelwise topology refinement graph convolution for skeleton-based action recognition,&rdquo; in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 13 359–13 368.</li>
<li>[47] X. Yao, R. Li, J. Zhang, J. Sun, and C. Zhang, &ldquo;Explicit boundary guided semi-push-pull contrastive learning for supervised anomaly detection,&rdquo;</li>
</ul>
<p>in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 24 490–24 499.</p>
<ul>
<li>[48] A. Barbalau, R. T. Ionescu, M.-I. Georgescu, J. Dueholm, B. Ramachandra, K. Nasrollahi, F. S. Khan, T. B. Moeslund, and M. Shah, &ldquo;Ssmtl++: Revisiting self-supervised multi-task learning for video anomaly detection,&rdquo; Computer Vision and Image Understanding, vol. 229, p. 103656, 2023.</li>
<li>[49] H. Liu, C. Li, Y. Li, and Y. J. Lee, &ldquo;Improved baselines with visual instruction tuning,&rdquo; 2023.</li>
<li>[50] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang et al., &ldquo;Qwen2. 5-vl technical report,&rdquo; arXiv preprint arXiv:2502.13923, 2025.</li>
<li>[51] C. Tang, Z. Han, H. Sun, S. Zhou, X. Zhang, X. Wei, Y. Yuan, J. Xu, and H. Sun, &ldquo;Tspo: Temporal sampling policy optimization for longform video language understanding,&rdquo; arXiv preprint arXiv:2508.04369 , 2025.</li>
</ul>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/Typicality and Context Uniqueness for.md"
          data-oid-likes="likes_papers/Typicality and Context Uniqueness for.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/vad-r1-towards-video-anomaly-reasoning-via/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/transanomaly_video_anomaly_detection_using_video_vision_transformer/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
