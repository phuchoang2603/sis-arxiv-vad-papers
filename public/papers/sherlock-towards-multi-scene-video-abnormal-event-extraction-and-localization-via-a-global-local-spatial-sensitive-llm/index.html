<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/sherlock-towards-multi-scene-video-abnormal-event-extraction-and-localization-via-a-global-local-spatial-sensitive-llm/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/sherlock-towards-multi-scene-video-abnormal-event-extraction-and-localization-via-a-global-local-spatial-sensitive-llm/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/sherlock-towards-multi-scene-video-abnormal-event-extraction-and-localization-via-a-global-local-spatial-sensitive-llm\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "8779"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>8779 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">42 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          <figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_b447d430dc6db6df8f0bc8f4a03d7608fdfde18bf917f625a60e4b6ea6ff94d1.png"
    ></figure>

<h2 class="relative group">Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM
    <div id="sherlock-towards-multi-scene-video-abnormal-event-extraction-and-localization-via-a-global-local-spatial-sensitive-llm" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#sherlock-towards-multi-scene-video-abnormal-event-extraction-and-localization-via-a-global-local-spatial-sensitive-llm" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">Junxiao Ma
    <div id="junxiao-ma" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#junxiao-ma" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p><a
  href="mailto:jxma0711@stu.suda.edu.cn">jxma0711@stu.suda.edu.cn</a> School of Computer Science and Technology, Soochow University Suzhou, China</p>

<h2 class="relative group">Jingjing Wang âˆ—
    <div id="jingjing-wang-" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#jingjing-wang-" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p><a
  href="mailto:djingwang@suda.edu.cn">djingwang@suda.edu.cn</a> School of Computer Science and Technology, Soochow University Suzhou, China</p>

<h2 class="relative group">Peiying Yu
    <div id="peiying-yu" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#peiying-yu" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p><a
  href="mailto:20244227007@stu.suda.edu.cn">20244227007@stu.suda.edu.cn</a> School of Computer Science and Technology, Soochow University Suzhou, China</p>

<h2 class="relative group">Jiamin Luo
    <div id="jiamin-luo" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#jiamin-luo" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p><a
  href="mailto:20204027003@stu.suda.edu.cn">20204027003@stu.suda.edu.cn</a> School of Computer Science and Technology, Soochow University Suzhou, China</p>

<h2 class="relative group">Guodong Zhou
    <div id="guodong-zhou" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#guodong-zhou" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p><a
  href="mailto:gdzhou@suda.edu.cn">gdzhou@suda.edu.cn</a> School of Computer Science and Technology, Soochow University Suzhou, China</p>
<p>Figure 1: (a) and (b) illustrate two surveillance video examples for our M-VAE task and Sherlock model in two scenes (Street and Residence). Sherlock precisely generates the abnormal event quadruples and their corresponding timestamps. (c) presents a circular ratio diagram illustrating different spatial information. From (c), we observe that the global spatial information and the local spatial information (i.e., action, object relation, and background) in our M-VAE dataset are imbalanced.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_ec20db868475b2bdefc7e7edfce9a2383f2016673cc43541f6469e38c2101773.png"
    ></figure>

<h2 class="relative group">Abstract
    <div id="abstract" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#abstract" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Prior studies on Video Anomaly Detection (VAD) mainly focus on detecting whether each video frame is abnormal or not in the video, which largely ignore the structured video semantic information (i.e., what, when, and where does the abnormal event happen). With this in mind, we propose a new chat-paradigm Multi-scene Video Abnormal Event Extraction and Localization (M-VAE) task, aiming to extract the abnormal event quadruples (i.e., subject, event type, object, scene) and localize such event. Further, this paper believes that this new task faces two key challenges, i.e., globallocal spatial modeling and global-local spatial balancing. To this end, this paper proposes a Global-local Spatial-sensitive Large Language</p>
<p>âˆ— Corresponding Author: Jingjing Wang.</p>
<p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from <a
  href="mailto:permissions@acm.org">permissions@acm.org</a>.</p>
<p>WWW â€™25, April 28â€“May 2, 2025, Sydney, NSW, Australia.</p>
<p>Â© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1274-6/25/04</p>
<p><a
  href="https://doi.org/10.1145/3696410.3714617"
    target="_blank"
  >https://doi.org/10.1145/3696410.3714617</a></p>
<p>Model (LLM) named Sherlock, i.e., acting like Sherlock Holmes to track down the criminal events, for this M-VAE task. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module and a Spatial Imbalance Regulator (SIR) to address the two challenges respectively. Extensive experiments on our M-VAE instruction dataset show the significant advantages of Sherlock over several advanced Video-LLMs. This justifies the importance of global-local spatial information for the M-VAE task and the effectiveness of Sherlock in capturing such information.</p>

<h2 class="relative group">CCS Concepts
    <div id="ccs-concepts" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#ccs-concepts" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>Computing methodologies â†’ Artificial intelligence .</li>
</ul>

<h2 class="relative group">Keywords
    <div id="keywords" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#keywords" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Multi-scene Video, Video Abnormal Event, Spatial-sensitive LLM</p>

<h2 class="relative group">ACM Reference Format:
    <div id="acm-reference-format" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#acm-reference-format" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Junxiao Ma, Jingjing Wang, Jiamin Luo, Peiying Yu, and Guodong Zhou. 2025. Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM . In Proceedings of the ACM Web Conference 2025 (WWW &lsquo;25), April 28â€“May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 10 pages. <a
  href="https://doi.org/10.1145/"
    target="_blank"
  >https://doi.org/10.1145/</a> 3696410.3714617</p>

<h2 class="relative group">1 Introduction
    <div id="1-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video Understanding is a foundational task in artificial intelligence, which focuses on analyzing and interpreting the content of videos to enable various applications, including video classification, activity recognition, and scene understanding [40 , 58 , 59]. As a critical branch of video understanding, Video Anomaly Detection (VAD) [20], which aims to automatically detect abnormal videos, has garnered significant research attention due to its wide range of applications in criminal activity detection and disaster response [56]. Prior studies on VAD mainly focus on detecting whether each video frame is abnormal or not in the video [20 , 29 , 41 , 56]. However, these studies overlook targeting at determining the underlying video semantic structure, i.e., &ldquo;what is the abnormal type, where they have occurred, which people or things are involved&rdquo; with a given video.</p>
<p>Motivated by these, this paper proposes a novel Multi-scene Video Abnormal Event Extraction and Localization (M-VAE) task, aiming at localizing abnormal events (i.e., starting and ending times of the anomaly) and extracting event quadruples (i.e. [subject of the event, event type, object of the event, scene of the event]) through a chat paradigm. Take an example of Street scene in Figure 1 (a), within 23s to 25s, a man bends down and pries the lock, then drives away from the street and the abnormal event quadruple is [people, steal, car, street]. Different scene (i.e., Residence scene) is also shown in Figure 1 (b). Within 15s to 17s, a man vandalizes a sculpture at one&rsquo;s residence and the quadruple is [people, Vandalism, Sculpture, Residence]. This structured processing for abnormal videos can significantly improve the practicality and efficiency of video anomaly localization systems. In fields such as real-time abnormal event monitoring that require high reliability and precision monitoring, using such structured processing can quickly search and screen for the required abnormal elements, which provides more convenient and intuitive evidence for further processing. Therefore, it is worthwhile to address this new task. Nevertheless, we believe that this new task faces two key challenges.</p>
<p>For one thing, it is challenging to model the global-local spatial information (named global-local spatial modeling challenge). Existing video understanding models [34 , 38 , 54] mainly focus on modeling general global information. However, local spatial information in our M-VAE task is often crucial compared to general global information, which are highly discriminative and essential for precise identification. Taking Figure 1 (a) as an example, the local spatial information, such as action (bend down), object relations (&lt;man, near, car&gt;), and background (street), can help better identify abnormal events. However, those local spatial information (e.g., actions, object relations, backgrounds) have different heterogeneous representations (i.e., different model structures and encoders). Therefore, a single, fixed-capacity transformer-based model, often makes it difficult to capture those critical local spatial information in videos. Recently, the Mixture of Expert (MoE) [18 , 23] paradigm has demonstrated scalability in multi-modal heterogeneous representation fusion tasks [18 , 23 , 24]. Inspired by this, a well-behaved model for our task should adopt the MoE paradigm to not only consider global spatial information but also emphasize the importance of local spatial information.</p>
<p>For another, a straightforward approach is to employ a basic Mixture of Expert (MoE) mechanism [18 , 23 , 24] to treat global spatial information (i.e., general representations of videos) and local spatial information (e.g., actions) as the global expert and local experts for integrating those information. However, the data imbalance issue among local spatial information may lead to the basic MoE experts being biased towards the more frequently occurring spatial information in the dataset. The statistics in Figure 1 (c) can illustrate this imbalance. Certain frequently appearing local information (i.e., action at 45%), can lead to higher weight for the corresponding expert. However, in Figure 1 (a), the object relations information, with the smallest proportion (25%), but is the most discriminative for extracting and localizing Theft events. More seriously, global spatial information is the most frequent and our preliminary experiments in Figure 7 (a) reveal global expert is often more thoroughly trained and often have the highest weights. Therefore, a better-behaved MoE expert fusion mechanism should mitigate this data imbalance (named global-local spatial balancing challenge), ensuring all experts are sufficiently trained to highlight their importance.</p>
<p>To tackle above challenges, we propose a Global-local Spatialsensitive LLM named Sherlock, i.e., acting like Sherlock Holmes to track down criminal events, for M-VAE. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module to address the global-local spatial modeling challenge, which includes four spatial experts to extract spatial information and an expert gate to weigh global and local spatial information. Furthermore, this model designs a Spatial Imbalance Regulator (SIR) to address the global-local spatial balancing challenge, which includes a Gated Spatial Balancing Loss (GSB) to further balance global and local experts. Particularly, we construct a M-VAE instruction dataset to better evaluate the effectiveness of our model. Detailed experiments show Sherlock can effectively extract and localize abnormal events and surpass advanced Video-LLMs in multiple evaluation metrics.</p>

<h2 class="relative group">2 Related Work
    <div id="2-related-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-related-work" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Â· Video Anomaly Detection. Video Understanding is a rapidly evolving research field which encompasses several tasks, including video grounding [40 , 58 , 59], spatial-temporal detection [13] and so on. As an important branch of video understanding, previous studies on Video Anomaly Detection (VAD) can be categorized into unsupervised, weakly-supervised, and fully-supervised categories. Unsupervised approaches focus on leveraging reconstruction techniques to identify anomalies [15 , 20 , 62 , 64]. Weaklysupervised methods have shown promising results in identifying abnormal frames [11 , 36 , 57 , 60 , 72]. Fully-supervised methods are scarce due to the expensive frame-level annotations required [7 , 10 , 12 , 19 , 53 , 55 , 70]. Different from the above studies, our Sherlock model aims to target at determining the underlying video semantic structure, providing a structured quadruple that goes beyond previous methods, facilitating the rapid detection and early warning of abnormal events in real-time.</p>
<p>Â· Event Extraction (EE) focuses on extracting structured information from given types of information. Traditional EE methods mainly extract from text documents [21 , 25 , 35 , 37 , 52]. Recently, many studies [2 , 44 , 66 â€“ 68] generate similar event structures from visual image data. Different from all the above studies, we are the first to focus on extracting the abnormal event from videos and constructing a quadruple dataset, incorporating information from</p>

<h2 class="relative group">Sherlock
    <div id="sherlock" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#sherlock" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure 2: The overall framework of Sherlock. It consists of a Global-local Spatial-enhanced MoE (GSM) Module and a Spatial Imbalance Regulator (SIR). The SIR exerts a direct influence on the output weights of the expert gate.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_6528684845bd2663adf657f18a804845d07ec192f44a81c594599bba717f0022.png"
    ></figure>
<p>multiple spatial information, enriching the task of event extraction, and making it more practical for real-world applications.</p>
<p>Â· Video-oriented Large Language Models. The rise of ChatGPT [49] has stimulated the prosperity of Video Large Language Models which can be categorized into four major types: firstly, Video Chat [34] and Video LLaMA [69], which utilize BLIP-2 [33] and Q-Former to map visual representations onto Vicuna; secondly, models like Video ChatGPT [47], Otter [31], Valley [46], mPLUGOwl [65], and Chat-UniVi [26], which leverage CLIP [51] to encode visual features; thirdly, PandaGPT [54], which adopts ImageBind [14] as its core architecture for video understanding; and fourthly, VideoLLaVA [38], which aligns image and video features into a linguistic feature space using LanguageBind [73]. Recently, a few studies [27 , 63] consider incorporating spatial information in models. Besides, some studies [18 , 23 , 24] introduce the concept of MoE into LLMs, but they only focus on efficiency, without considering the balance between different information. Different from all the above studies, we design a new Sherlock model, to address our M-VAE task, which includes a Global-local Spatial-enhanced MoE module and a Spatial Imbalance Regulator to address the challenges of global-local modeling and balancing.</p>

<h2 class="relative group">3 Our Sherlock Model
    <div id="3-our-sherlock-model" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-our-sherlock-model" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this paper, we propose a Sherlock model to address the MVAE task. Figure 2 illustrates the framework of Sherlock, which is composed of two core components (i.e., the Global-local Spatialenhanced MoE (GSM) module (sec 3.1) for the global-local spatial modeling challenge and the Spatial Imbalance Regulator (SIR) (sec 3.2) for the global and local spatial balancing challenge). Subsequently, we present our training strategies to enhance the ability of understanding spatial information (sec 3.3).</p>
<p>Backbone. We choose Video-LLaVA [38] and its visual encoder LanguageBind [73] as the core framework. Video-LLaVA, which is optimized with a mixed dataset of images and videos, demonstrates leading performance across most image and video benchmarks. We employ Video-LLaVA as the backbone to explore the potential of Video-LLMs in extracting and localizing abnormal events.</p>
<p>Task Formulation. Given a video ğ‘‰ for ğ‘€ frames, each frame is labeled with 1 or 0, where 1 and 0 represent whether this frame conveys an abnormal event. The goal of M-VAE is to interactively generate the quadruple (ğ‘ ğ‘¢ğ‘ , ğ‘¡ğ‘¦ğ‘ğ‘’ , ğ‘œğ‘ ğ‘— , ğ‘ ğ‘ğ‘’ ) for each event along with the corresponding timestamp ğ‘ ğ‘¡ğ‘ and ğ‘’ğ‘›ğ‘‘, where ğ‘ ğ‘¢ğ‘ , ğ‘¡ğ‘¦ğ‘ğ‘’ , ğ‘œğ‘ ğ‘— , ğ‘ ğ‘ğ‘’ , ğ‘ ğ‘¡ğ‘ and ğ‘’ğ‘›ğ‘‘ are the subject, event type, object, scene, start time and end time of the abnormal event. As shown in Figure 1 (a), a man steals a car at street from 23s to 25s. Therefore, the output of our M-VAE task is {23s , 25s, (people , steal, l, car, r, street)}.</p>

<h2 class="relative group">3.1 Global-local Spatial-enhanced MoE Module
    <div id="31-global-local-spatial-enhanced-moe-module" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#31-global-local-spatial-enhanced-moe-module" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>As shown in Figure 2, we design a Global-local Spatial-enhanced MoE (GSM) Module for the global-local spatial modeling challenge. Inspired by Mixture-of-Experts (MoE) [24], we design three Local Spatial Experts (i.e., Local Action Expert, Local Object Relation</p>
<p>Expert and Local Background Expert) and a Global Spatial Expert to extract spatial information, detailed as follows.</p>
<p>Local Spatial Experts contain three local spatial experts (i.e., action, object relation, and background), detailed as follows.</p>
<p>Â· Local Action Expert (Action Expert, AE). We leverage HigherHRNet [6], a well-adopted bottom-up human pose estimation network to extract local spatial action information. HigherHRNet can generate local spatial action tokens T ğ’‚ = {ğ’• ğ’‚ 1
, &hellip;, ğ’• ğ’‚ ğ’Š
, &hellip;, ğ’• ğ’‚ ğ’ }, and each token consists of 17 human joint nodes for each individual in every frame of a video sequence. Here, ğ‘– denotes the ğ‘–-th frame. Next, we apply Action Graph Attention to integrate T ğ’‚ with the video tokens T ğ’— = {ğ’• ğ’— 1
, &hellip;, ğ’• ğ’— ğ’Š
, &hellip;, ğ’• ğ’— ğ’ } generated by the Video Encoder in Video-LLMs. We start by calculating the attention weights ğ›¼ ğ‘˜ ğ‘— for each node ğ‘’ğ‘˜in ğ’• ğ’‚ ğ’Š relative to its neighboring node ğ‘’ğ‘—:</p>
<!-- formula-not-decoded -->
<p>where â„ğ‘˜ and â„ğ‘—is the features of ğ‘’ğ‘˜ and ğ‘’ğ‘— respectively. Wa denote the learnable weight matrix, and ğ‘‘ is the feature dimension. Then we aggregate the feature Ë† â„ğ‘˜ of node ğ‘’ğ‘˜: Ë† â„ğ‘˜ =
Ã
ğ‘— âˆˆN ( ğ‘’ğ‘˜ ) ğ›¼ ğ‘˜ ğ‘— Â· â„ğ‘— , where N (ğ‘’ğ‘˜) is the neighboring nodes of ğ‘’ğ‘˜. Finally the feature of ğ‘’ğ‘˜is calculated by â„ â€² ğ‘˜ = ReLU(Wk[ Ë† â„ğ‘˜, â„ğ‘˜]), where W a donates the weight matrix and [ Ë† â„ğ‘˜, â„ğ‘˜] is the concatenation of Ë† â„ğ‘˜ and â„ğ‘˜ .</p>
<p>After graph attention operation, we enhance T ğ’‚ using the attention mechanism with query Q ğ’— , key K ğ’‚ , and value V ğ’‚ calculation to obtain final action tokens: T â€² ğ’‚ = softmax Q âŠ¤ ğ’— Â· K ğ’‚ 
Â· V ğ’‚.</p>
<ul>
<li>Local Object Relation Expert (Object Relation Expert, ORE). We leverage RelTR [9], a well-studied one-stage object relation graph generation method to extract local spatial object relation information. RelTR can generate an object relation token ğ’• ğ’ ğ’Š = (ğ‘…ğ‘–</li>
</ul>
<!-- formula-not-decoded -->
<p>where ğœ is the activation function on the graph. A Ëœ is the adjacency matrix of the object-relation graph, derived from ğ¸ğ‘–, and D Ëœ is its degree matrix, with D Ëœ ğ‘–ğ‘– = Ã
ğ‘– A Ëœ ğ‘–ğ‘— . W (â„“) is a trainable weight matrix.</p>
<p>Â· Local Background Expert (Background Expert, BE). We leverage SAM2 [28], an advanced model for visual segmentation, to extract local spatial background information from videos. SAM2 can generate a background image for each frame of video. Then we leverage InternVit [5] to encode local spatial background information which is a large vision encoder extending the parameters of vision transformer (VIT) [4] to 6B, formally represented as:</p>
<!-- formula-not-decoded -->
<p>where ğ‘£ğ‘–is the ğ‘–-th frame of video ğ‘‰ . This process results in the local spatial background tokens Tğ’ƒ = {ğ’• ğ’ƒ 1
, &hellip;, ğ’• ğ’ƒ ğ’Š
, &hellip;, ğ’• ğ’ƒ ğ’ } for the entire video sequence, with ğ‘› representing the total number of frames.</p>
<p>Global Spatial Expert has a comprehensive understanding of the training data. Collaborate with local spatial experts to bring specialization and generalization capabilities to M-VAE tasks.</p>
<p>Â· Global Spatial Expert (Global Expert, GE). The weight assigned to the global spatial expert complements that of the local spatial experts. Consequently, the local spatial experts acquire specialized skills for specific tasks, whereas the global spatial expert develops a comprehensive understanding of the entire training corpus. The collaboration between these two types of experts provides both specialization and generalization for our M-VAE task. In this way, we leverage LanguageBind [73] in Video-LLaVA [38], which inherits the ViT-L/14 structure from CLIP and is equipped with powerful and universal visual encoding capabilities to extract global spatial information for our task. We subsequently leverage a pre-trained FFN layer by [38] to align the dimension with other spatial information, formally represented as:</p>
<!-- formula-not-decoded -->
<p>where ğ‘£ğ‘–is the ğ‘–-th frame of video ğ‘‰ . This process yields the full set of global tokens T ğ’ˆ = {ğ’• ğ’ˆ 1
, &hellip;, ğ’• ğ’ˆ ğ’Š
, &hellip;, ğ’• ğ’ˆ ğ’ } for the entire video sequence, with ğ‘› representing the total number of frames.</p>
<p>After designing four experts, we ensure that the four Spatial Experts can dynamically adjust the weights of the four heterogeneous types of spatial information inspired by Mixture-of-Experts (MoE) [18]. As shown in Figure 2, unlike methods that embed several FFNs within LLMs, our GSM put four experts outside the LLMs to adjust weights for global and local spatial information. Based on this, we introduce a dynamic Expert Gate (EG) [50], which controls the contribution of each expert by calculating gating weights as a soft gate. Finally, the output O of GSM, based on four spatial experts and EG, is formally represented as:</p>
<!-- formula-not-decoded -->
<!-- formula-not-decoded -->
<p>where LayerNorm (Â·) indicates layer normalization [1]. ğ‘”ğ‘–(the ğ‘–-th entry in ğ’ˆ) represents the weight of the ğ‘–-th expert. Si represents the outputs of the ğ‘–-th Spatial expert. ğ‘ is the total number of spatial expert, and W ğ‘” being the trainable weight matrix.</p>

<h2 class="relative group">3.2 Spatial Imbalance Regulator
    <div id="32-spatial-imbalance-regulator" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#32-spatial-imbalance-regulator" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>After modeling the spatial information, we design a Spatial Imbalance Regulator (SIR) including a Gated Spatial Balancing Loss (GSB) for the global-local spatial balancing challenge, detailed as follows.</p>
<p>Table 1: The statistics of the number of events and the duration in seconds (s) of events for each scene.</p>
<table>
  <thead>
      <tr>
          <th>Spli</th>
          <th>School</th>
          <th>Shop</th>
          <th>Underwate</th>
          <th>Street</th>
          <th>Road</th>
          <th>Boat</th>
          <th>Wild</th>
          <th>Fore</th>
          <th>Residenc</th>
          <th>Bank</th>
          <th>Commercia</th>
          <th>Factor</th>
          <th>Lawn</th>
          <th>Othe</th>
          <th>Total</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Train</td>
          <td>55 (2136s)</td>
          <td>107 (4130s)</td>
          <td>78 (3022s)</td>
          <td>113 (7076s)</td>
          <td>114 (5586s)</td>
          <td>115 (5203s)</td>
          <td>111 (4681s</td>
          <td>102 (3918s</td>
          <td>117 (4914s)</td>
          <td>89 (3380s</td>
          <td>105 (5011s)</td>
          <td>82 (3173s)</td>
          <td>104 (5943s)</td>
          <td>56 (1497</td>
          <td>48 (59670s</td>
      </tr>
      <tr>
          <td>Inference</td>
          <td>ence 13 (534s)</td>
          <td>26 (1032s</td>
          <td>19 (755s)</td>
          <td>28 (1769s)</td>
          <td>28 (1396s)</td>
          <td>29 (1300s)</td>
          <td>27 (1170s)</td>
          <td>25 (979s</td>
          <td>29 (1228</td>
          <td>22 (845s</td>
          <td>26 (1252s</td>
          <td>20 (793s</td>
          <td>26 (1485s</td>
          <td>14 (374</td>
          <td>332 (14912s</td>
      </tr>
  </tbody>
</table>
<p>Figure 3: The word cloud distribution of quadruple elements in the M-VAE dataset, which reveals the spatial imbalance. (e.g., The proportion of people is the highest)</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_b0b99d17361767d89c07a78d15f1d12ff3feea9ed78921e8b20e2c86fb8ee1a9.png"
    ></figure>
<p>Gated Spatial Balancing (GSB) Loss. Previous researches employ a basic Mixture of Experts (MoE) [18 , 23] to model global and local spatial information. When faced with an imbalance between these two types of information, the weights assigned to experts tend to be biased toward those that appear more frequently. As shown in Figure 1 (c), there are the most spatial elements (e.g., People) related to local spatial action information in event quadruple. This implies that performance will deteriorate when faced with real-world data that is not processed by an action expert (e.g., object relations). More seriously, as shown in Figure 1 (c), global information holds significant weight in all data, which will lead to excessive training of global experts and weaken the abilities of local experts with lower weights. This imbalance phenomenon will greatly affect the performance of our model. Based on this, we should keep the weights of all spatial experts not too different and achieve the optimal state of relative balance where every expert is fully trained. Inspired by MoELoRA [42], we propose a Gated Spatial Balancing (GSB) Loss to balance spatial weights, as follows:</p>
<!-- formula-not-decoded -->
<p>where ğ‘local is the number of local expert. ğ‘”global is the weight of global expert. The first term of Eq.(7) is balancing between local experts, and the second term is balancing between local and global experts. The weights of four experts have already balanced when the loss is optimized to a minimum. This regulation achieves a better balance among all experts, reducing the impact of data imbalance, which effectively addresses the global-local balancing challenge. Finally, the overall loss of Sherlock can be represented as:</p>
<!-- formula-not-decoded -->
<p>where ğ›¼ is the hyper-parameter that controls the strength of Lgate , and LD is the next-token prediction loss of Video-LLMs.</p>

<h2 class="relative group">3.3 Training Strategies for Sherlock
    <div id="33-training-strategies-for-sherlock" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#33-training-strategies-for-sherlock" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In order to enhance the ability of understanding spatial information, we design a two-stage training process. Stage 1 is to enhance the ability of understanding spatial information and Stage 2 is to address the M-VAE task, detailed as follows.</p>
<p>Stage 1. Pre-Tuning for spatial understanding. As shown in Figure 2, we first pre-tune Video-LLaVA using four high-quality</p>
<p>Stage 1: The dataset of pre-tuning for spatial understanding</p>
<p>Ref</p>
<ul>
<li></li>
</ul>
<p>L4</p>
<p>HumanML3D</p>
<p>20K Frames</p>
<p>RSI</p>
<ul>
<li></li>
</ul>
<p>CB</p>
<p>20K Frames</p>
<p>20K Frames</p>
<p>Stage 2: Our constructed dataset for M-VAE task</p>
<p>Train</p>
<p>80163s (640K Frames)</p>
<p>Inference</p>
<p>20053s (160K Frames)</p>
<p>Figure 4: Data composition for training and inference.</p>
<p>datasets. We aim for Video-LLaVA to have a good spatial understanding ability. Specifically, we selected four high-quality datasets: HumanML3D [16], Ref-L4 [3], RSI-CB [32], and COCO-Caption [39], as described in sec 4.1. For each pre-tuning dataset, we enable this dataset to understand corresponding spatial information.</p>
<p>Stage 2. Instruction Tuning for M-VAE task. We aim to enable the model to localize abnormal events and extract quadruples through the chat paradigm. We construct an instruction tuning dataset described in sec 4.1 and instruct the pre-tuned Video-LLaVA to Extract quadruples and localize abnormal events. The quadruple includes subject, event type, object, and scene in abnormal events. The instruction will undergo text embedding to obtain the textual tokens T ğ’• . Finally, the input of the LLM is &ldquo;O from Eq.(5) + Tğ’•&rdquo;.</p>

<h2 class="relative group">4 Experimental Settings
    <div id="4-experimental-settings" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#4-experimental-settings" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">4.1 Instruction Data Construction
    <div id="41-instruction-data-construction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#41-instruction-data-construction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The training pipeline of Sherlock contains two stages. As shown in Figure 4, for each stage, we construct the corresponding instruction dataset for better tuning.</p>
<p>For Stage 1. We construct a special understanding dataset based on Ref-L4 [3], HumanML3D [16], RSI-CB [32] and COCO [39]. Specifically, we manually design an instruction for each type of spatial information, for instance: Instruction: &ldquo;Judge the action of the characters in the image. Describe the image region &lt;objs&gt; in the image. Judge the background of the image. Describe the image&rdquo;. As HumanML3D has 25K videos with an average duration of 1 second, and we take 8 frames per second. For the data balance, we randomly select 20K images or frames from each dataset.</p>
<p>For Stage 2. We construct an M-VAE instruction dataset based on CUVA [10], which primarily consists of surveillance videos, with an average duration of 80 seconds per video. As this dataset includes five detailed video Q-A tasks (i.e., timestamp, classification, reason, result, and description tasks), it is highly beneficial for constructing our M-VAE dataset. 1) For abnormal event quadruples, constructing quadruples involves two steps. First, we collect answers from the reason, result, and description tasks in CUVA for each video. Subsequently, we construct initial quadruples through ChatGPT [49] based on the answers to these tasks, with the instruction: &ldquo;Please extract the subject, object, and scene of the event based on the responses below&rdquo;. Second, we create multiple candidate sets for subjects, objects, and scenes in quadruple. Specifically, for subjects and objects elements, we manually construct a set of around 40 for subjects and objects and filter elements based on this set. For event types elements, we adopt the 11 categories (i.e., Fighting, Animals, Water, Vandalism, Accidents, Robbery, Theft, Pedestrian, Fire, Violations, and Forbidden) from CUVA as the event types. For</p>
<p>COCO</p>
<p>20K Frames</p>
<p>Table 2: Comparison of several Video-LLMs and Sherlock on our instruction dataset. The â†“ beside FNRs indicates the lower the metric, the better the performance. AE, ORE, BE, GE, and EG represent four Spatial Experts and Expert Gate respectively. Sub, Type, Obj, and Sce represent Subject, Event type, Object, and Scene respectively. For each task, Blue and Green donate the first and second place respectively.</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Extraction Even</th>
          <th>Event Location An</th>
          <th>Event Location An</th>
          <th>Event Location An</th>
          <th>Event Location An</th>
          <th>Anomaly Cls.</th>
          <th>Anomaly Cls.</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Models</td>
          <td>Single (F1)</td>
          <td>Single (F1)</td>
          <td>Single (F1)</td>
          <td>Single (F1)</td>
          <td>Pair (F1)</td>
          <td>Pair (F1)</td>
          <td>Pair (F1)</td>
          <td>Pair (F1)</td>
          <td>Quadruple</td>
          <td>Quadruple</td>
          <td>Quadruple</td>
          <td>Average</td>
          <td></td>
          <td>mAP@tIoU</td>
          <td>mAP@tIoU</td>
          <td>Average</td>
          <td>FNRs F2</td>
          <td>FNRs F2</td>
      </tr>
      <tr>
          <td></td>
          <td>Subject</td>
          <td>Type</td>
          <td>Object</td>
          <td>Scene</td>
          <td>Sub-Type</td>
          <td>Obj-Typ</td>
          <td>Sub-Sce</td>
          <td>Obj-Sce</td>
          <td>F1</td>
          <td>T5-based</td>
          <td>GPT-based</td>
          <td>Average</td>
          <td>0.1</td>
          <td>0.2</td>
          <td>0.3</td>
          <td>Average</td>
          <td>FNRs</td>
          <td>F2</td>
      </tr>
      <tr>
          <td>Video Chat</td>
          <td>73.14</td>
          <td>71.35</td>
          <td>64.28</td>
          <td>71.76</td>
          <td>70.12</td>
          <td>58.69</td>
          <td>71.55</td>
          <td>61.18</td>
          <td>40.95</td>
          <td>61.68</td>
          <td>53.94</td>
          <td>62.6</td>
          <td>77.28</td>
          <td>74.93</td>
          <td>66.26</td>
          <td>72.82</td>
          <td>38.79</td>
          <td>65.88</td>
      </tr>
      <tr>
          <td>Video ChatGPT</td>
          <td>61.87</td>
          <td>59.51</td>
          <td>54.82</td>
          <td>46.39</td>
          <td>54.23</td>
          <td>49.68</td>
          <td>43.26</td>
          <td>41.38</td>
          <td>39.63</td>
          <td>57.36</td>
          <td>50.38</td>
          <td>49.86</td>
          <td>74.65</td>
          <td>70.91</td>
          <td>67.03</td>
          <td>70.86</td>
          <td>41.47</td>
          <td>61.35</td>
      </tr>
      <tr>
          <td>Valley</td>
          <td>64.64</td>
          <td>62.27</td>
          <td>58.94</td>
          <td>52.26</td>
          <td>58.36</td>
          <td>51.64</td>
          <td>49.68</td>
          <td>46.42</td>
          <td>42.38</td>
          <td>63.34</td>
          <td>56.67</td>
          <td>54.23</td>
          <td>69.34</td>
          <td>62.26</td>
          <td>57.66</td>
          <td>63.08</td>
          <td>43.49</td>
          <td>59.42</td>
      </tr>
      <tr>
          <td>Panda GPT</td>
          <td>73.09</td>
          <td>75.45</td>
          <td>68.42</td>
          <td>61.93</td>
          <td>71.96</td>
          <td>59.92</td>
          <td>59.79</td>
          <td>59.45</td>
          <td>41.17</td>
          <td>54.36</td>
          <td>48.55</td>
          <td>60.37</td>
          <td>76.64</td>
          <td>62.69</td>
          <td>57.21</td>
          <td>65.51</td>
          <td>35.62</td>
          <td>69.16</td>
      </tr>
      <tr>
          <td>mPLUG-Owl</td>
          <td>52.86</td>
          <td>37.54</td>
          <td>40.24</td>
          <td>37.68</td>
          <td>31.97</td>
          <td>28.89</td>
          <td>33.9</td>
          <td>27.87</td>
          <td>22.12</td>
          <td>30.68</td>
          <td>32.41</td>
          <td>34.1</td>
          <td>61.42</td>
          <td>53.21</td>
          <td>46.46</td>
          <td>53.69</td>
          <td>56.98</td>
          <td>51.66</td>
      </tr>
      <tr>
          <td>Chat-UniVi</td>
          <td>59.71</td>
          <td>57.26</td>
          <td>55.28</td>
          <td>44.23</td>
          <td>52.43</td>
          <td>50.62</td>
          <td>41.24</td>
          <td>40.96</td>
          <td>37.68</td>
          <td>55.34</td>
          <td>48.84</td>
          <td>43.59</td>
          <td>65.89</td>
          <td>58.62</td>
          <td>40.02</td>
          <td>54.84</td>
          <td>52.52</td>
          <td>53.78</td>
      </tr>
      <tr>
          <td>Video-LLaVA</td>
          <td>77.85</td>
          <td>73.68</td>
          <td>65.67</td>
          <td>75.91</td>
          <td>69.32</td>
          <td>59.21</td>
          <td>73.25</td>
          <td>62.24</td>
          <td>41.32</td>
          <td>52.94</td>
          <td>56.74</td>
          <td>64.37</td>
          <td>78.31</td>
          <td>74.79</td>
          <td>64.92</td>
          <td>72.67</td>
          <td>41.34</td>
          <td>64.96</td>
      </tr>
      <tr>
          <td>Sherlock</td>
          <td>87.97</td>
          <td>82.12</td>
          <td>74.99</td>
          <td>92.15</td>
          <td>77.06</td>
          <td>66.28</td>
          <td>85.16</td>
          <td>73.17</td>
          <td>57.57</td>
          <td>75.46</td>
          <td>67.52</td>
          <td>75.22</td>
          <td>94.03</td>
          <td>82.59</td>
          <td>76.12</td>
          <td>84.24</td>
          <td>17.24</td>
          <td>83.59</td>
      </tr>
      <tr>
          <td>w/o AE</td>
          <td>83.15</td>
          <td>77.64</td>
          <td>71.28</td>
          <td>90.16</td>
          <td>72.36</td>
          <td>63.47</td>
          <td>80.52</td>
          <td>70.39</td>
          <td>52.48</td>
          <td>60.61</td>
          <td>62.02</td>
          <td>71.18</td>
          <td>92.24</td>
          <td>81.21</td>
          <td>75.38</td>
          <td>82.94</td>
          <td>21.82</td>
          <td>80.45</td>
      </tr>
      <tr>
          <td>w/o ORE</td>
          <td>83.96</td>
          <td>78.25</td>
          <td>72.37</td>
          <td>90.01</td>
          <td>74.24</td>
          <td>64.46</td>
          <td>81.56</td>
          <td>70.97</td>
          <td>54.35</td>
          <td>72.28</td>
          <td>65.08</td>
          <td>72.5</td>
          <td>91.13</td>
          <td>82.08</td>
          <td>74.62</td>
          <td>82.61</td>
          <td>22.97</td>
          <td>78.83</td>
      </tr>
      <tr>
          <td>w/o BE</td>
          <td>81.16</td>
          <td>74.65</td>
          <td>67.88</td>
          <td>88.07</td>
          <td>69.29</td>
          <td>61.12</td>
          <td>77.64</td>
          <td>66.64</td>
          <td>48.63</td>
          <td>53.04</td>
          <td>55.94</td>
          <td>67.71</td>
          <td>88.62</td>
          <td>79.09</td>
          <td>72.24</td>
          <td>79.98</td>
          <td>25.36</td>
          <td>73.51</td>
      </tr>
      <tr>
          <td>w/o GE</td>
          <td>79.2</td>
          <td>74.09</td>
          <td>66.71</td>
          <td>84.11</td>
          <td>70.38</td>
          <td>60.77</td>
          <td>75.44</td>
          <td>66.28</td>
          <td>46.34</td>
          <td>63.97</td>
          <td>57.06</td>
          <td>66.75</td>
          <td>86.18</td>
          <td>78.37</td>
          <td>69.28</td>
          <td>77.94</td>
          <td>28.97</td>
          <td>71.28</td>
      </tr>
      <tr>
          <td>w/o EG</td>
          <td>78.83</td>
          <td>73.96</td>
          <td>65.02</td>
          <td>83.15</td>
          <td>70.15</td>
          <td>60.26</td>
          <td>74.15</td>
          <td>63.37</td>
          <td>43.64</td>
          <td>59.14</td>
          <td>51.82</td>
          <td>64.86</td>
          <td>81.31</td>
          <td>77.68</td>
          <td>67.88</td>
          <td>75.62</td>
          <td>32.58</td>
          <td>67.07</td>
      </tr>
      <tr>
          <td>w/o SIR</td>
          <td>84.47</td>
          <td>80.14</td>
          <td>71.94</td>
          <td>92.34</td>
          <td>75.58</td>
          <td>64.84</td>
          <td>83.21</td>
          <td>70.06</td>
          <td>55.73</td>
          <td>72.87</td>
          <td>65.18</td>
          <td>73.3</td>
          <td>83.41</td>
          <td>78.49</td>
          <td>68.37</td>
          <td>76.75</td>
          <td>30.64</td>
          <td>70.97</td>
      </tr>
      <tr>
          <td>w/o pre-tuning</td>
          <td>78.24</td>
          <td>74.44</td>
          <td>64.22</td>
          <td>82.21</td>
          <td>68.55</td>
          <td>57.74</td>
          <td>72.62</td>
          <td>62.91</td>
          <td>42.51</td>
          <td>57.22</td>
          <td>50.54</td>
          <td>63.74</td>
          <td>79.58</td>
          <td>75.32</td>
          <td>65.07</td>
          <td>73.32</td>
          <td>34.87</td>
          <td>66.64</td>
      </tr>
  </tbody>
</table>
<p>scenes elements, we assign two annotators to classify scenes for each abnormal event. If they cannot reach an agreement, an expert will make the final decision to ensure annotation quality. The Kappa consistency check value of the annotation is 0.87. 2) For localization task, we use the timestamp in the CUVA as labels for localization. Furthermore, we adhere to the split of CUVA for training and inference videos and take 8 frames per second, resulting in 800K frames from 1k videos and each video contains 1.68 abnormal event on average. The statistics of the number of events and the duration in seconds (s) of events for each scene are shown in Table 1. Finally, we obtain our M-VAE instruction dataset. Our instruction for the M-VAE task is: &ldquo;Generate a quadruple and localize an abnormal event in the video. The quadruple includes subject, event type, object, and scene in abnormal events.&rdquo;. Figure 1 (c) and Figure 3 show the top 20 quadruple elements, revealing the spatial imbalance.</p>

<h2 class="relative group">4.2 Baselines
    <div id="42-baselines" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#42-baselines" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this paper, we select several advanced Video-LLMs as baselines which are introduced as follows. VideoChat [47] employs Q-Former [33] to map visual representations to Vicuna [8]. VideoChatGPT [47] integrates LLMs with CLIP [51] for video representations. Valley [46] employs a temporal modeling module to bridge visual and textual modes. PandaGPT [54] utilizes ImageBind [14] to demonstrate cross-modal capabilities. mPLUG-Owl [65] introduces a visual abstractor module to align different modes. ChatUniVi [26] merges visual tokens with semantic meanings. VideoLLaVA [38] conducts joint training on images and videos. To ensure a fair comparison, we re-implement these models using their released codes in our experiments, with all LLMs sized at 7B.</p>

<h2 class="relative group">4.3 Evaluation Metrics
    <div id="43-evaluation-metrics" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#43-evaluation-metrics" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>M-VAE focuses on extracting event quadruples and locating abnormal events from videos, requiring evaluation metrics in three aspects (i.e., extract event quadruples, locate abnormal events, and classify abnormal events). For the extraction performance, we measure our model through three perspectives. 1) Single: performance of generating each individual element. 2) Pair: performance of generating the element pair, i.e., Subject-Type pair, Object-Type pair, Subject-Scene pair, Object-Scene pair. 3) Quadruple Generation: performance of generating the complete event quadruple. Following the prior works [30], the performance is evaluated with Macro-F1. Furthermore, we use T5-based and GPT-based metrics based on Video-bench [48] especially for LLM. For localization performance, we use the mAP@tIoU metric [71], calculated by mean Average Precision (mAP) at different IoU thresholds from 0.1 to 0.3 with 0.1 intervals. For classification performance, we refer to the traditional anomaly classification task [17 , 45 , 61] for anomaly classification metric, which mainly determines whether each video frame is abnormal or not in the video. We prefer Recall over Precision and report F2 [71] as another classification metric. Furthermore, our model focuses on accurately distinguishing abnormal events. As shown in Figure 1, it&rsquo;s better to mark all timestamps as abnormal than to miss any. So we prioritize false negative rates (FNRs): FNRs = num of false-negative frame num of positive frame , which is the rate of mislabeling an abnormal event frame as normal. In addition, ğ‘¡-test is used to evaluate the significance of the performance.</p>

<h2 class="relative group">4.4 Implementation Details
    <div id="44-implementation-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#44-implementation-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In our experiments, we utilize open-source codes to obtain experimental results of all the baselines in Table 2. The hyper-parameters of these baselines remain the same setting reported by their public papers. For both Stage 1 and 2, we use a batch size of 16 and train for 1 epoch with the AdamW [43] optimizer and a cosine learning rate decay schedule with a warm-up period. The initial learning rate is 2e-5. The hyper-parameter ğ›¼ in L is set to 0.4. We tune the Video-LLaVA model using LoRA [22]. The LoRA matrix dimension, dropout rate, and dropout rate are 16, 64, and 0.05 respectively. Experiments are run on a single NVIDIA A100 GPU with 40GB memory. Stage 1 training takes about 16 hours, Stage 2 takes 60 hours, and inference takes about 8 hours.</p>

<h2 class="relative group">5 Results and Discussions
    <div id="5-results-and-discussions" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#5-results-and-discussions" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">5.1 Experimental Results
    <div id="51-experimental-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#51-experimental-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Table 2 and Table!4 shows the performance comparison of different models on our M-VAE task, and we can see that: For extraction performance, our Sherlock model outperforms all baselines, with</p>
<p>Table 3: Comparison of several advanced Video-LLMs and Sherlock on the 14 scenes of the M-VAE dataset with FNRs.</p>
<table>
  <thead>
      <tr>
          <th>Models</th>
          <th>School</th>
          <th>Shop</th>
          <th>Underwater</th>
          <th>Street</th>
          <th>Road</th>
          <th>Boat</th>
          <th>Wild</th>
          <th>Forest</th>
          <th>Residence</th>
          <th>Bank</th>
          <th>Commercial</th>
          <th>Factory</th>
          <th>Lawn</th>
          <th>Other</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Video Chat</td>
          <td>39.57</td>
          <td>39.47</td>
          <td>37.3</td>
          <td>36.81</td>
          <td>27.41</td>
          <td>35.32</td>
          <td>33.27</td>
          <td>33.36</td>
          <td>35.95</td>
          <td>40.59</td>
          <td>38.97</td>
          <td>45.52</td>
          <td>35.26</td>
          <td>49.04</td>
      </tr>
      <tr>
          <td>Video Chatgpt</td>
          <td>45.91</td>
          <td>41.98</td>
          <td>39.36</td>
          <td>41.41</td>
          <td>30.11</td>
          <td>38.19</td>
          <td>36.32</td>
          <td>37.73</td>
          <td>37.54</td>
          <td>44.5</td>
          <td>42.96</td>
          <td>40.78</td>
          <td>36.28</td>
          <td>52.33</td>
      </tr>
      <tr>
          <td>Valley</td>
          <td>46.68</td>
          <td>43.76</td>
          <td>41.37</td>
          <td>44.24</td>
          <td>35.66</td>
          <td>42.15</td>
          <td>46.78</td>
          <td>39.25</td>
          <td>42.15</td>
          <td>48.35</td>
          <td>48.31</td>
          <td>47.21</td>
          <td>37.11</td>
          <td>53.09</td>
      </tr>
      <tr>
          <td>Pandagpt</td>
          <td>34.56</td>
          <td>35.65</td>
          <td>34.47</td>
          <td>36.48</td>
          <td>24.42</td>
          <td>35.85</td>
          <td>31.78</td>
          <td>32.37</td>
          <td>34.18</td>
          <td>38.55</td>
          <td>37.89</td>
          <td>41.46</td>
          <td>31.17</td>
          <td>44.24</td>
      </tr>
      <tr>
          <td>mPLUG-Owl</td>
          <td>54.13</td>
          <td>54.41</td>
          <td>53.21</td>
          <td>47.34</td>
          <td>36.51</td>
          <td>45.02</td>
          <td>58.37</td>
          <td>46.31</td>
          <td>45.63</td>
          <td>57.94</td>
          <td>56.88</td>
          <td>53.14</td>
          <td>54.74</td>
          <td>59.56</td>
      </tr>
      <tr>
          <td>Chatunivi</td>
          <td>52.51</td>
          <td>48.82</td>
          <td>47.52</td>
          <td>48.68</td>
          <td>35.53</td>
          <td>44.41</td>
          <td>59.88</td>
          <td>45.96</td>
          <td>44.34</td>
          <td>54.92</td>
          <td>55.66</td>
          <td>51.12</td>
          <td>52.22</td>
          <td>55.48</td>
      </tr>
      <tr>
          <td>Video-llava</td>
          <td>45.27</td>
          <td>37.43</td>
          <td>34.63</td>
          <td>38.84</td>
          <td>27.76</td>
          <td>32.54</td>
          <td>26.41</td>
          <td>30.29</td>
          <td>31.45</td>
          <td>21.19</td>
          <td>29.84</td>
          <td>20.08</td>
          <td>30.72</td>
          <td>28.31</td>
      </tr>
      <tr>
          <td>Sherlock</td>
          <td>16.35</td>
          <td>21.91</td>
          <td>15.16</td>
          <td>24.24</td>
          <td>14.63</td>
          <td>20.96</td>
          <td>17.29</td>
          <td>18.48</td>
          <td>20.43</td>
          <td>11.21</td>
          <td>23.43</td>
          <td>8.96</td>
          <td>21.44</td>
          <td>13.6</td>
      </tr>
  </tbody>
</table>
<p>Figure 5: Convergence analysis of other baselines, Sherlock, and its variant without specific components.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000004_dcb2d26fc97eeb7918293e941c2c53f2616bdc0a1320b106a665894419d36ed7.png"
    ></figure>
<p>AE</p>
<p>ORE</p>
<p>BE</p>
<p>GE</p>
<p>Figure 6: The visualization of balanced spatial expert weights calculated in Eq.(5). The length of the bar in different colors represents the weights for the corresponding expert. ğ¶1 to ğ¶11 is different Event types in quadruples.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000005_d727f0f3f658c63a32e8e7e4ea4a0df73a4031a38cf181c444bda1a51279118f.png"
    ></figure>
<p>an average improvement of 10.85 (ğ‘-value &lt; 0.05) over the second performance. Specifically, our Sherlock model surpasses the second performance by an average of 9.9 (ğ‘-value &lt; 0.05), 8.59 (ğ‘-value &lt; 0.05), and 9.52 (ğ‘-value &lt; 0.05) in average Single, Pair, and Quadruple metrics, justifying the effectiveness of Sherlock on extraction task. For localization performance, our Sherlock model exceeds the second performance by 11.42 (ğ‘-value &lt; 0.01) in average mAP@tIoU metric, justifying the effectiveness of Sherlock on localization task. Furthermore, for classification performance, in FNRs and F2 metric, Sherlock surpasses the second performance in 18.38 (ğ‘value &lt; 0.01) and 14.43 (ğ‘-value &lt; 0.01). This implies the importance of our global and local information and justifies the effectiveness of our Sherlock model on our task.</p>

<h2 class="relative group">5.2 Contributions of Each Key Component
    <div id="52-contributions-of-each-key-component" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#52-contributions-of-each-key-component" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In order to further investigate the contributions of different modules of Sherlock, we conduct an ablation study on our Sherlock model. As shown in Table 2, w/o AE, w/o ORE, w/o BE, w/o GE, w/o EG,</p>
<p>Figure 7: (a) is the visual comparison of our SIR and (b) is the comparison of the average inference time for a one-minute video between Sherlock and other Video-LLMs.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000006_27032078b6e7a2ca244dfd498caf5054bb5b9b5f796881779b333cbfddc312fd.png"
    ></figure>
<p>Table 4: Comparison of localization and anomaly classification task with several well-performing non-LLM models.</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Anomaly Location</th>
          <th>Anomaly Location</th>
          <th>Anomaly Location</th>
          <th>Anomaly Location</th>
          <th>Anomaly Cls.</th>
          <th>Anomaly Cls.</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Models</td>
          <td></td>
          <td>mAP@tIoU</td>
          <td>mAP@tIoU</td>
          <td>Average</td>
          <td>FNRs</td>
          <td>F2</td>
      </tr>
      <tr>
          <td></td>
          <td>0.1</td>
          <td>0.2</td>
          <td>0.3</td>
          <td>Averag</td>
          <td>FNRs</td>
          <td>F2</td>
      </tr>
      <tr>
          <td>BiConvLSTM[19]</td>
          <td>52.74</td>
          <td>37.31</td>
          <td>31.12</td>
          <td>40.39</td>
          <td>68.05</td>
          <td>44.48</td>
      </tr>
      <tr>
          <td>SPIL[55]</td>
          <td>53.28</td>
          <td>38.89</td>
          <td>32.91</td>
          <td>41.69</td>
          <td>67.84</td>
          <td>46.87</td>
      </tr>
      <tr>
          <td>FlowGatedNet[7]</td>
          <td>53.64</td>
          <td>39.64</td>
          <td>33.18</td>
          <td>42.15</td>
          <td>67.24</td>
          <td>46.55</td>
      </tr>
      <tr>
          <td>X3D[53]</td>
          <td>54.52</td>
          <td>40.05</td>
          <td>34.96</td>
          <td>43.17</td>
          <td>65.08</td>
          <td>48.65</td>
      </tr>
      <tr>
          <td>HSCD[12]</td>
          <td>56.14</td>
          <td>42.87</td>
          <td>35.28</td>
          <td>44.76</td>
          <td>60.36</td>
          <td>52.28</td>
      </tr>
      <tr>
          <td>Sherlock</td>
          <td>94.03</td>
          <td>82.59</td>
          <td>76.12</td>
          <td>84.24</td>
          <td>17.24</td>
          <td>83.59</td>
      </tr>
  </tbody>
</table>
<p>and w/o pre-tuning represent without four Spatial Experts, Expert Gate, and pre-tuning stage in sec 3.2 respectively.</p>
<p>Effectiveness Study of Global and Local Spatial Expert . From Table 2, we can see that: The performance of w/o AE , w/o ORE , w/o BE and w/o GE degrades in all metrics, with an average decrease of 7.54 (ğ‘-value &lt; 0.01), 7.57 (ğ‘-value &lt; 0.01), 4.37 (ğ‘-value &lt; 0.01), and 5.68 (ğ‘-value &lt; 0.01) in FNRs, F2, average map@tIoU, and average event extraction metrics. This confirms the importance of global and local information in extracting and localizing abnormal events, and Sherlock can better model those information well.</p>
<p>Effectiveness Study of Spatial Imbalance Regulator. From Table 2, we can see that: 1) Compared with Sherlock , w/o EG shows poorer performance in all metrics, with a decrease of FNRs, F2, average map@tIoU, and average extraction performance by 15.34 (ğ‘-value &lt; 0.01), 16.52 (ğ‘-value &lt; 0.01), 8.62 (ğ‘-value &lt; 0.05) and 10.36 (ğ‘-value &lt; 0.01), respectively. This demonstrates the effectiveness of GSM in global-local spatial modeling and encourages us to consider handling heterogeneity issues between spatial information in the manner of MoE. 2) From Table 2, we can see that compared to performance of w/o SIR, the performance of w/o MG is poorer, with FNRs, F2, average map@tIoU, and average event extraction metrics decreasing by 1.94 (ğ‘-value &lt; 0.05), 3.9 (ğ‘-value</p>
<p>Figure 8: Two Visualized samples to compare Sherlock with other Video-LLMs.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000007_ec02f402c1ff199b633b92dfff4c43a4ddfd5bd5a8fffe7cafede7a21a390135.png"
    ></figure>
<p>&lt; 0.05), 1.13 (ğ‘-value &lt; 0.05) and 4.84 (ğ‘-value &lt; 0.05), respectively. This further demonstrates the effectiveness of Lgate in global-local spatial balancing and encourages us to consider using SIR to better balance spatial information. 3) In addition, we record the weights of four spatial experts after training in Figure 6 and Figure 7 (a). We can see that the weights of all experts have been relatively balanced, and each expert has demonstrated outstanding professional abilities when facing different types of abnormal videos.</p>
<p>Effectiveness Study of Pre-tuning. From Table 2, we can see that w/o pre-tuning, the performance is inferior to Sherlock . FNRs, F2, average map@tIoU, and average event extraction metrics have decreased by 17.63 (ğ‘-value &lt; 0.01), 16.95 (ğ‘-value &lt; 0.01), 10.92 (ğ‘-value &lt; 0.01) and 11.48 (ğ‘-value &lt; 0.01), respectively. This further justifies the effectiveness of pre-tuning, as well as encourages us to use more high-quality datasets to enhance the spatial understanding ability of Video-LLMs before instruction-tuning.</p>

<h2 class="relative group">5.3 Convergence Analysis and Practical Assessment for Sherlock
    <div id="53-convergence-analysis-and-practical-assessment-for-sherlock" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#53-convergence-analysis-and-practical-assessment-for-sherlock" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In order to analyze the convergence of Sherlock, we record the loss of baseline Video-LLMs, Sherlock, and its variant without specific components over various training steps. The results are shown in Figure 5 and we can see that: 1) Sherlock demonstrates the fastest convergence compared to other Video-LLMs. At the convergence point, the loss of Sherlock is 1.05, while Video-LLaVA is 2.06. This underscores the high efficiency of Sherlock over other advanced Video-LLMs. 2) Sherlock demonstrates the fastest convergence compared to its variant without specific components in Figure 5. This justifies that the spatial information along with GSM and SIR can accelerate the convergence process, which further encourages us to consider the spatial information in the M-VAE task.</p>
<p>To assess practicality, we analyze the FNRs of Sherlock for each scene. As shown in Table 3, we can observe that in every scene, Sherlock outperforms other Video-LLMs. This indicates that the possibility of misclassifying abnormal events as normal events is minimized, thereby demonstrating the importance of global and local spatial modeling of Sherlock. We also analyze the average inference time in seconds for a one-minute video. As shown in Figure 7 (b), Sherlock does not perform much differently from the other models in terms of inference time. This is reasonable, as some studies confirm that the MoE architecture can improve efficiency</p>
<p>[11, 28]. This suggests that introducing more information along with a MoE module for the M-VAE task does not increase the inference time and Sherlock can maintain good inference efficiency.</p>

<h2 class="relative group">5.4 Qualitative Analysis for Sherlock
    <div id="54-qualitative-analysis-for-sherlock" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#54-qualitative-analysis-for-sherlock" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>As shown in Figure 8, we visualize and compare Sherlock with other Video-LLMs. We randomly select two samples from our dataset and ask these models to Analyze the following video and localize the timestamp and extract the quadruple of the abnormal events. From the figure, we can see that: 1) Accurately localizing abnormal events and extracting correct quadruples is a huge challenge. For instance, example 2 captures a segment from 9s to 15s, where identifying the collision of the truck at road is challenging, 2) Compared with other advanced Video-LLMs, Sherlock shows excellent performance in localizing abnormal events. In example 1, Sherlock outperforms other models in terms of accuracy. In example 2, it outperforms PandaGPT in terms of accuracy and can generate a correct quadruple. This further demonstrates the effectiveness of Sherlock in precisely extracting and localizing abnormal events.</p>

<h2 class="relative group">6 Conclusion
    <div id="6-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#6-conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this paper, we firstly propose a new M-VAE task and a constructed instruction dataset, making a significant contribution to future research on abnormal events. Secondly, we propose a Globallocal Spatial-sensitive LLM named Sherlock to assist in localizing and extracting abnormal event quadruples. This model includes a Global-local Spatial-enhanced MoE module and Spatial Imbalance Regular to model and balance spatial information. In the end, our experimental results demonstrate the outstanding performance of Sherlock. In future work, we hope to consider the relationships between events and enrich our tasks with event inference to improve the performance of extraction. In addition, we also hope to improve the interpretability of our model by providing explanations for each abnormal event.</p>

<h2 class="relative group">Acknowledgments
    <div id="acknowledgments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#acknowledgments" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We thank our anonymous reviewers for their helpful comments. This work was supported by three NSFC grants, i.e., No.62006166, No.62376178 and No.62076175. This work was also supported by a Project Funded by the Priority Academic Program Development of Jiangsu Higher Education Institutions (PAPD).</p>

<h2 class="relative group">References
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>
<p>[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016).</p>
</li>
<li>
<p>[2] Antoine Bosselut, Jianfu Chen, David Scott Warren, Hannaneh Hajishirzi, and Yejin Choi. 2016. Learning Prototypical Event Structure from Photo Albums. In Proceedings of ACL 2016 .</p>
</li>
<li>
<p>[3] Jierun Chen, Fangyun Wei, Jinjing Zhao, Sizhe Song, Bohuai Wu, Zhuoxuan Peng, S.-H. Gary Chan, and Hongyang Zhang. 2024. Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models. CoRR abs/2406.16866 (2024).</p>
</li>
<li>
<p>[4] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. 2022. When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations. In Proceedings of ICLR 2022. 2.</p>
</li>
<li>
<p>[5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. 2023. InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. CoRR abs/2312.14238 (2023).</p>
</li>
<li>
<p>[6] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S. Huang, and Lei Zhang. 2020. HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation. In Proceedings of CVPR 2020. 5385â€“5394.</p>
</li>
<li>
<p>[7] Ming Cheng, Kunjing Cai, and Ming Li. 2020. RWF-2000: An Open Large Scale Video Database for Violence Detection. In Proceedings of ICPR 2020. 4183â€“4190.</p>
</li>
<li>
<p>[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. <a
  href="https://lmsys.org/blog/2023-03-30-vicuna/"
    target="_blank"
  >https://lmsys.org/blog/2023-03-30-vicuna/</a></p>
</li>
<li>
<p>[9] Yuren Cong, Michael Ying Yang, and Bodo Rosenhahn. 2023. RelTR: Relation Transformer for Scene Graph Generation. IEEE Trans. Pattern Anal. Mach. Intell. 45, 9 (2023), 11169â€“11183.</p>
</li>
<li>
<p>[10] Hang Du, Sicheng Zhang, Binzhu Xie, Guoshun Nan, Jiayang Zhang, Junrui Xu, Hangyu Liu, Sicong Leng, Jiangming Liu, Hehe Fan, Dajiu Huang, Jing Feng, Linli Chen, Can Zhang, Xuhuan Li, Hao Zhang, Jianhang Chen, Qimei Cui, and Xiaofeng Tao. 2024. Uncovering What, Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly. CoRR abs/2405.00181 (2024).</p>
</li>
<li>
<p>[11] Jia-Chang Feng, Fa-Ting Hong, and Wei-Shi Zheng. 2021. MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection. In Proceedings of CVPR 2021. 14009â€“14018.</p>
</li>
<li>
<p>[12] Guillermo Garcia-Cobo and Juan C. SanMiguel. 2023. Human skeletons and change detection for efficient violence detection in surveillance videos. Comput. Vis. Image Underst. 233 (2023).</p>
</li>
<li>
<p>[13] Rohit Girdhar, JoÃ£o Carreira, Carl Doersch, and Andrew Zisserman. 2019. Video Action Transformer Network. In Proceedings of CVPR 2019. 244â€“253.</p>
</li>
<li>
<p>[14] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. ImageBind One Embedding Space to Bind Them All. In Proceedings of CVPR 2023. 15180â€“15190.</p>
</li>
<li>
<p>[15] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. 2019. Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection. In Proceedings of ICCV 2019. 1705â€“1714.</p>
</li>
<li>
<p>[16] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. 2022. Generating Diverse and Natural 3D Human Motions From Text. In Proceedings of CVPR 2022. 5142â€“5151.</p>
</li>
<li>
<p>[17] Huiwen Guo, Xinyu Wu, Nannan Li, Ruiqing Fu, Guoyuan Liang, and Wei Feng. 2013. Anomaly detection and localization in crowded scenes using short-term trajectories. In Proceedings of ROBIO 2013. 245â€“249.</p>
</li>
<li>
<p>[18] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. 2023. OneLLM: One Framework to Align All Modalities with Language. CoRR abs/2312.03700 (2023).</p>
</li>
<li>
<p>[19] Krishnagopal Sanjukta Davis Larry Hanson Alex, PNVR Koutilya. 2019. Bidirectional Convolutional LSTM for the Detection of Violence in Videos. In Proceedings of ECCV 2018. 280â€“295.</p>
</li>
<li>
<p>[20] Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K. Roy-Chowdhury, and Larry S. Davis. 2016. Learning Temporal Regularity in Video Sequences. In Proceedings of CVPR 2016. 733â€“742.</p>
</li>
<li>
<p>[21] Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao, Guodong Zhou, and Qiaoming Zhu. 2011. Using Cross-Entity Inference to Improve Event Extraction. In Proceedings of ACL 2011. 1127â€“1136.</p>
</li>
<li>
<p>[22] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In Proceedings of ICLR 2022. 2.</p>
</li>
<li>
<p>[23] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. Adaptive Mixtures of Local Experts. Neural Comput. 3, 1 (1991), 79â€“87.</p>
</li>
<li>
<p>[24] Gagan Jain, Nidhi Hegde, Aditya Kusupati, Arsha Nagrani, Shyamal Buch, Prateek Jain, Anurag Arnab, and Sujoy Paul. 2024. Mixture of Nested Experts: Adaptive Processing of Visual Tokens. CoRR abs/2407.19985 (2024).</p>
</li>
<li>
<p>[25] Heng Ji and Ralph Grishman. 2008. Refining Event Extraction through CrossDocument Inference. In Proceedings of ACL 2008. 254â€“262.</p>
</li>
<li>
<p>[26] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. 2023. Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding. CoRR abs/2311.08046 (2023).</p>
</li>
<li>
<p>[27] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. 2024. Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation. In Proceedings of CVPR 2024. 9492â€“9502.</p>
</li>
<li>
<p>[28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, ChloÃ© Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr DollÃ¡r, and Ross B. Girshick. 2023. Segment Anything. In Proceedings of ICCV 2023. 3992â€“4003.</p>
</li>
<li>
<p>[29] Federico Landi, Cees G. M. Snoek, and Rita Cucchiara. 2019. Anomaly Locality in Video Surveillance. CoRR abs/1901.10364 (2019).</p>
</li>
<li>
<p>[30] Bobo Li, Hao Fei, Fei Li, Yuhan Wu, Jinsong Zhang, Shengqiong Wu, Jingye Li, Yijiang Liu, Lizi Liao, Tat-Seng Chua, and Donghong Ji. 2023. DiaASQ: A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis. In Proceedings of ACL 2023. 13449â€“13467.</p>
</li>
<li>
<p>[31] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023. Otter: A Multi-Modal Model with In-Context Instruction Tuning. CoRR abs/2305.03726 (2023).</p>
</li>
<li>
<p>[32] Haifeng Li, Xin Dou, Chao Tao, Zhixiang Wu, Jie Chen, Jian Peng, Min Deng, and Ling Zhao. 2020. RSI-CB: A Large-Scale Remote Sensing Image Classification Benchmark Using Crowdsourced Data. Sensors 20 (2020).</p>
</li>
<li>
<p>[33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In Proceedings of ICML 2023. 19730â€“19742.</p>
</li>
<li>
<p>[34] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023. VideoChat: Chat-Centric Video Understanding. CoRR abs/2305.06355 (2023).</p>
</li>
<li>
<p>[35] Qi Li, Heng Ji, and Liang Huang. 2013. Joint Event Extraction via Structured Prediction with Global Features. In Proceedings of ACL 2013. 73â€“82.</p>
</li>
<li>
<p>[36] Shuo Li, Fang Liu, and Licheng Jiao. 2022. Self-Training Multi-Sequence Learning with Transformer for Weakly Supervised Video Anomaly Detection. In Proceedings of AAAI 2022. 1395â€“1403.</p>
</li>
<li>
<p>[37] Shasha Liao and Ralph Grishman. 2010. Using Document Level Cross-Event Inference to Improve Event Extraction. In Proceedings of ACL 2010. 789â€“797.</p>
</li>
<li>
<p>[38] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. 2023. Video-LLaVA: Learning United Visual Representation by Alignment Before Projection. CoRR abs/2311.10122 (2023).</p>
</li>
<li>
<p>[39] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In Proceedings of ECCV 2014. 740â€“755.</p>
</li>
<li>
<p>[40] Zihang Lin, Chaolei Tan, Jian-Fang Hu, Zhi Jin, Tiancai Ye, and Wei-Shi Zheng. 2023. Collaborative Static and Dynamic Vision-Language Streams for SpatioTemporal Video Grounding. In Proceedings of CVPR 2023. 23100â€“23109.</p>
</li>
<li>
<p>[41] Kun Liu and Huadong Ma. 2019. Exploring Background-bias for Anomaly Detection in Surveillance Videos. In Proceedings of MM 2019. 1490â€“1499.</p>
</li>
<li>
<p>[42] Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. 2023. MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications. CoRR abs/2310.18339 (2023).</p>
</li>
<li>
<p>[43] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In Proceedings of ICLR 2019. 9.</p>
</li>
<li>
<p>[44] Cewu Lu, Ranjay Krishna, Michael S. Bernstein, and Li Fei-Fei. 2016. Visual Relationship Detection with Language Priors. In Proceedings of ECCV 2016. 852â€“ 869.</p>
</li>
<li>
<p>[45] Cewu Lu, Jianping Shi, and Jiaya Jia. 2013. Abnormal Event Detection at 150 FPS in MATLAB. In Proceedings of ICCV 2013. 2720â€“2727.</p>
</li>
<li>
<p>[46] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. 2023. Valley: Video Assistant with Large Language model Enhanced abilitY. CoRR abs/2306.07207 (2023).</p>
</li>
<li>
<p>[47] Muhammad Maaz, Hanoona Abdul Rasheed, Salman H. Khan, and Fahad Shahbaz Khan. 2023. Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models. CoRR abs/2306.05424 (2023).</p>
</li>
<li>
<p>[48] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. 2023. Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models. CoRR abs/2311.16103 (2023).</p>
</li>
<li>
<p>[49] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023).</p>
</li>
<li>
<p>[50] Joan Puigcerver, Carlos Riquelme Ruiz, Basil Mustafa, and Neil Houlsby. 2024. From Sparse to Soft Mixtures of Experts. In Proceedings of ICLR 2024 .</p>
</li>
<li>
<p>[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al . 2021. Learning transferable visual models from natural language supervision. In Proceedings of ICML 2021. 8748â€“8763.</p>
</li>
<li>
<p>[52] Zhiyi Song, Ann Bies, Stephanie M. Strassel, Tom Riese, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Ryant, and Xiaoyi Ma. 2015. From Light to Rich ERE: Annotation of Entities, Relations, and Events. In Proceedings of EVENTS 2015. 89â€“98.</p>
</li>
<li>
<p>[53] Jiayi Su, Paris Her, Erik Clemens, Edwin E. Yaz, Susan C. Schneider, and Henry Medeiros. 2022. Violence Detection using 3D Convolutional Neural Networks. In Proceedings of AVSS 2022. 1â€“8.</p>
</li>
<li>
<p>[54] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. 2023. PandaGPT: One Model To Instruction-Follow Them All. CoRR abs/2305.16355 (2023).</p>
</li>
<li>
<p>[55] Yukun Su, Guosheng Lin, Jin-Hui Zhu, and Qingyao Wu. 2020. Human Interaction Learning on 3D Skeleton Point Clouds for Video Violence Recognition. In Proceedings of ECCV 2020. 74â€“90.</p>
</li>
<li>
<p>[56] Waqas Sultani, Chen Chen, and Mubarak Shah. 2018. Real-World Anomaly Detection in Surveillance Videos. In Proceedings of CVPR 2018. 6479â€“6488.</p>
</li>
<li>
<p>[57] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W. Verjans, and Gustavo Carneiro. 2021. Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning. In Proceedings of ICCV 2021 . 4955â€“4966.</p>
</li>
<li>
<p>[58] Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. 2024. Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding. CoRR abs/2401.00901 (2024).</p>
</li>
<li>
<p>[59] Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, and Ping Luo. 2023. UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces. arXiv preprint arXiv:2312.15715 (2023).</p>
</li>
<li>
<p>[60] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. 2020. Not only Look, But Also Listen: Learning Multimodal Violence Detection Under Weak Supervision. In Proceedings of ECCV 2020. 322â€“339.</p>
</li>
<li>
<p>[61] Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng Wang, and Yanning Zhang. 2024. VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection. In Proceedings of AAAI 2023 . 6074â€“6082.</p>
</li>
<li>
<p>[62] Dan Xu, Yan Yan, Elisa Ricci, and Nicu Sebe. 2017. Detecting anomalous events in videos by learning deep representations of appearance and motion. Comput. Vis. Image Underst. 156 (2017), 117â€“127.</p>
</li>
<li>
<p>[63] Zhixuan Xu, Chongkai Gao, Zixuan Liu, Gang Yang, Chenrui Tie, Haozhuo Zheng, Haoyu Zhou, Weikun Peng, Debang Wang, Tianyi Chen, Zhouliang Yu, and Lin Shao. 2024. ManiFoundation Model for General-Purpose Robotic Manipulation</p>
</li>
</ul>
<p>of Contact Synthesis with Arbitrary Objects and Robots. CoRR (2024).</p>
<ul>
<li>[64] Zhiwei Yang, Jing Liu, Zhaoyang Wu, Peng Wu, and Xiaotao Liu. 2023. Video Event Restoration Based on Keyframes for Video Anomaly Detection. In Proceedings of CVPR 2023. 14592â€“14601.</li>
<li>[65] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. 2023. mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. CoRR abs/2304.14178 (2023).</li>
<li>[66] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Trans. Assoc. Comput. Linguistics 2 (2014), 67â€“78.</li>
<li>[67] Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-Seng Chua. 2017. Visual Translation Embedding Network for Visual Relation Detection. In Proceedings of CVPR 2017. 3107â€“3115.</li>
<li>[68] Hanwang Zhang, Zawlin Kyaw, Jinyang Yu, and Shih-Fu Chang. 2017. PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN. In Proceedings of CVPR 2017. 4243â€“4251.</li>
<li>[69] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding. In Proceedings of EMNLP 2023. 543â€“553.</li>
<li>[70] Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Chuchu Han, Xiaonan Huang, Changxin Gao, Yuehuan Wang, and Nong Sang. 2024. Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM. CoRR abs/2406.12235 (2024).</li>
<li>[71] Zhicheng Zhang and Jufeng Yang. 2022. Temporal Sentiment Localization: Listen and Look in Untrimmed Videos. In Proceedings of MM 2022. 199â€“208.</li>
<li>[72] Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H. Li, and Ge Li. 2019. Graph Convolutional Label Noise Cleaner: Train a Plug-And-Play Action Classifier for Anomaly Detection. In Proceedings of CVPR 2019. 1237â€“1246.</li>
<li>[73] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Hongfa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Caiwan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. 2024. LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. In Proceedings of ICLR 2024 .</li>
</ul>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/Sherlock Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM.md"
          data-oid-likes="likes_papers/Sherlock Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/simplifying-traffic-anomaly-detection-with-video-foundation-models/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/plovad_prompting_vision-language_models_for_open_vocabulary_video_anomaly_detection/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
