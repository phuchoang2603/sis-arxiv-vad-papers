<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/toward-video-anomaly-retrieval-from-video/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/toward-video-anomaly-retrieval-from-video/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/toward-video-anomaly-retrieval-from-video\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "10197"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>10197 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">48 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">Towards Video Anomaly Retrieval from Video Anomaly Detection: New Benchmarks and Model
    <div id="towards-video-anomaly-retrieval-from-video-anomaly-detection-new-benchmarks-and-model" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#towards-video-anomaly-retrieval-from-video-anomaly-detection-new-benchmarks-and-model" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Peng Wu, Jing Liu Senior Member, IEEE, Xiangteng He, Yuxin Peng Senior Member, IEEE , Peng Wang, and Yanning Zhang Senior Member, IEEE</p>
<p>Abstract—Video anomaly detection (VAD) has been paid increasing attention due to its potential applications, its current dominant tasks focus on online detecting anomalies, which can be roughly interpreted as the binary or multiple event classification. However, such a setup that builds relationships between complicated anomalous events and single labels, e.g., &ldquo;vandalism&rdquo;, is superficial, since single labels are deficient to characterize anomalous events. In reality, users tend to search a specific video rather than a series of approximate videos. Therefore, retrieving anomalous events using detailed descriptions is practical and positive but few researches focus on this. In this context, we propose a novel task called Video Anomaly Retrieval (VAR), which aims to pragmatically retrieve relevant anomalous videos by cross-modalities, e.g., language descriptions and synchronous audios. Unlike the current video retrieval where videos are assumed to be temporally well-trimmed with short duration, VAR is devised to retrieve long untrimmed videos which may be partially relevant to the given query. To achieve this, we present two large-scale VAR benchmarks and design a model called Anomaly-Led Alignment Network (ALAN) for VAR. In ALAN, we propose an anomaly-led sampling to focus on key segments in long untrimmed videos. Then, we introduce an efficient pretext task to enhance semantic associations between video-text finegrained representations. Besides, we leverage two complementary alignments to further match cross-modal contents. Experimental results on two benchmarks reveal the challenges of VAR task and also demonstrate the advantages of our tailored method. Captions are publicly released at <a
  href="https://github.com/Roc-Ng/VAR"
    target="_blank"
  >https://github.com/Roc-Ng/VAR</a>.</p>
<p>Index Terms—video anomaly retrieval, video anomaly detection, cross-modal retrieval</p>

<h2 class="relative group">I. INTRODUCTION
    <div id="i-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#i-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>V IDEO anomaly detection (VAD) plays a critical role in video content analysis, and has become a hot topic being studied due to its potential applications, e.g., danger earlywarning. VAD, by definition, aims to identify the location of anomaly occurrence, which can be regarded as the framelevel event classification. VAD can be broadly divided into two categories, i.e., semi-supervised [1]–[5] and weakly supervised [6]–[10]. The former typically recognizes anomalies</p>
<p>Peng Wu, Peng Wang, and Yanning Zhang are with the National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, China. E-mail: <a
  href="mailto:xdwupeng@gmail.com">xdwupeng@gmail.com</a>; peng.wang, <a
  href="mailto:ynzhang@nwpu.edu.cn">ynzhang@nwpu.edu.cn</a>. Jing Liu is with the Guangzhou Institute of Technology, Xidian University, China. E-mail: <a
  href="mailto:neouma@163.com">neouma@163.com</a>. Xiangteng He and Yuxin Peng are with the Wangxuan Institute of Computer Technology, Peking University, China. E-mail: hexiangteng, <a
  href="mailto:pengyuxin@pku.edu.cn">pengyuxin@pku.edu.cn</a>. This work is supported by the National Natural Science Foundation of China (No. 62306240, U23B2013, U19B2037, 62272013, 61925201, 62132001), China Postdoctoral Science Foundation (No. 2023TQ0272), and the Fundamental Research Funds for the Central Universities (No. D5000220431). (Corresponding author: Yanning Zhang.)</p>
<p>Manuscript received April 19, 2021; revised August 16, 2021.</p>
<p>Fig. 1. VAD vs. VAR. Single labels may be unable to describe sequential anomalous events in VAD, but text captions or synchronous audios can sufficiently depict events in VAR.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_74c2cccd8aae7c04ae69d587bf00568414a23b6590f5c0af395ace1ca5cdfb7f.png"
    ></figure>
<p>through self-supervised learning or one-class learning. The latter, thanks to massive normal and abnormal videos with video-level labels, achieves better detection accuracy.</p>
<p>Impressive progress has been witnessed for VAD, however, an event in videos generally captures an interaction between actions and entities that evolves over time, simply utilizing single labels in VAD may be insufficient to explain the sequential events depicted. Besides, compared with VAD, offline video search thus far is still more commonly used in realworld applications. Imagining the case when searching for related videos, we prefer to use comprehensive descriptions to accurately search, e.g., &ldquo;At night, two topless men smashed the door of the store.&rdquo;, rather than use a single coarse word, e.g, &ldquo;vandalism&rdquo;, to get a large collection of rough results.</p>
<p>Based on VAD, we propose a new task called Video Anomaly Retrieval (VAR) and present two large-scale benchmark datasets, UCFCrime-AR and XDViolence-AR, to further facilitate the research of video anomaly analysis. The goal of VAR is to retrieve relevant untrimmed videos given crossmodal queries, e.g., text captions and synchronous audios, and vice versa. Unlike VAD, VAR depicts anomalies from multiple viewpoints and sufficiently characterizes sequential events. We illustrate the advantage of video anomaly retrieval in Figure 1. VAR task has high value to real-world applications, especially for smart ground and car surveillance. Generally speaking, for surveillance, the recorded video will be stored in the hard disk or memory card as a series of segments with a certain time length. After an abnormal event occurs, we need to search the corresponding video segment that contains the queried abnormal event through the descriptions, such as a white car crashed into the rear of a van, a group of people breaking into a house at night, etc.</p>
<p>Our VAR is considerably different from traditional video retrieval (VR) [11]–[13]. In traditional video retrieval, videos</p>
<p>Query: A girl playing guitar and singing a song</p>
<p>.</p>
<p>Query: An adult brown horse stand in the barn and his father horse jumps a barrier and his mother</p>
<p>Fig. 2. Comparison of VAR with video retrieval and video moment retrieval.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_78c2271e70fb37554c7b83edba4dbacb7b615b86faa4fd44b076535d9de6be9b.png"
    ></figure>
<p>are assumed to be temporally pre-trimmed with short duration, and thus the whole video is supposed to be completely relevant to the paired query. In reality, videos are usually not welltrimmed and may only exist partial fragments to fully meet the query. In VAR, the main goal is to retrieve long and untrimmed videos. Such a setup more meets realistic requirements, and also evokes the new challenge. Concretely, the length of relevant fragments is variable in videos w.r.t. given paired queries. For normal videos, the relevant fragment is generally the whole video; For abnormal videos, the relevant fragment may occupy only a fraction or a lion&rsquo;s share of the entire video since the length of anomalous events is inconstant in videos. Besides, our VAR task also differs from video moment retrieval (VMR) [14]–[17] since the latter is to retrieve moments rather than untrimmed videos. Because both abnormal videos and normal videos (no anomaly) need to be retrieved in VAR, video moment retrieval methods are hard to tackle this task. Traditional video retrieval and video moment retrieval methods cannot solve this new challenge well, detailed results are listed in Tables II and III. The differences between video retrieval, video moment retrieval and video anomaly retrieval are shown in Figure 2.</p>
<p>To overcome the above challenge, we propose ALAN, an Anomaly-Led Alignment Network for video anomaly retrieval. In ALAN, video, audio, and text encoders are intended to encode raw data into high-level representations, and crossmodal alignment is introduced to match cross-modal representations from different perspectives. Since videos are long untrimmed and anomalous events have complex variations in the scenario and length, we expect that, in the encoding phase, the retrieval system maintains holistic views, meanwhile, focuses on key anomalous segments, so that cross-modal representations can be well aligned in the joint embedding space. Therefore, vanilla fixed-frame sampling, e.g., uniform sampling and random sampling, is not flexible to focus on specific anomalous segments. Inspired by dynamic neural networks [18]–[21], we propose an anomaly-led sampling, which simply resorts to frame-level anomaly priors generated by an ad-hoc anomaly detector and does not require intensive pairwise interactions between cross modality, to select key segments with large anomaly identification degree. We then</p>
<p>Query: two teams playing volleyball</p>
<p>.</p>
<p>couple these two win-win sampling mechanisms for videos and audios, where anomaly-led sampling focuses on anomalous segments, and fixed-frame sampling pays attention to the entirety as well as normal videos. Furthermore, to establish associations between video-text fine-grained representations as well as maintain high retrieval efficiency, we also propose a pretext task, i.e., video prompt based masked phrase modeling (VPMPM), serving the model training. Particularly, a new module termed Prompting Decoder takes both frame-level video representations and contextual text representations as input and predicts the masked noun phrases or verb phrases by the cross-modal attention, where video representations serve as fixed prompts [22], [23]. In this paper, video frames are regarded as the fine granularity as frames usually reflect more detailed content of videos, meanwhile noun phrases and verb phrases in texts, e.g., &ldquo;a black car&rdquo; and &ldquo;left quickly&rdquo;, are regarded as the fine granularity, which reflect the local spatial contents and temporal dynamics in the video, respectively. Notably, compared with nouns and verbs, noun phrases and verb phrases contain more contents, and can also particularly illustrate the subtle differences. Finally, such a proxy training objective optimizes the encoder parameters and further promotes the semantic associations between local video frames and text phrases by cross-modal interactions.</p>
<p>To summarize, our contributions are three-fold:</p>
<ul>
<li>We introduce a new task named video anomaly retrieval to bridge the gap between the literature and real-world applications in terms of video anomaly analysis. To our knowledge, this is the first work moves towards VAR from VAD;</li>
<li>We present two large-scale benchmarks, i.e., UCFCrimeAR and XDViolence-AR, based on public VAD datasets. The former is applied to video-text VAR, the latter is to video-audio VAR;</li>
<li>We propose a model called ALAN, aiming at challenges in VAR, where anomaly-led sampling, video prompt based masked phrase modeling, and cross-modal alignment are introduced for the attention of anomalous segments, enhancement of fine-grained associations, and multi-perspective match, respectively.</li>
</ul>

<h2 class="relative group">II. RELATED WORK
    <div id="ii-related-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#ii-related-work" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">A. Video Anomaly Detection
    <div id="a-video-anomaly-detection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-video-anomaly-detection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Aided by the success of deep learning, VAD has made much good progress in recent years, which is usually classified into two groups, semi-supervised anomaly detection and weakly supervised anomaly detection. In semi-supervised anomaly detection, only normal event samples are available for model training. Recent researchers mainly adopt deep auto-encoders [2], [4], [24]–[30] for self-supervised learning, e.g., reconstruction, prediction, jigsaw, etc. Weakly supervised anomaly detection can be regarded as the problem of binary classification, to obtain frame-level predictions given coarse labels, and multiple instance learning [6], [8], [9], [31]–[34] is widely used to train models. Unlike VAD that utilizes single labels to distinguish whether each frame is anomalous or not, our proposed VAR uses elaborate text descriptions or synchronous audios to depict the sequential events.</p>

<h2 class="relative group">B. Cross-Modal Retrieval
    <div id="b-cross-modal-retrieval" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-cross-modal-retrieval" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We mainly introduce cross-modal retrieval [35]–[38] built on videos, texts, and audios. There are some works [39]– [44] focus on audio-text and audio-video retrieval. Specifically, Tian et al. [45] propose an audio-to-video/video-toaudio cross-modality localization/retrieval task [46], i.e, given a sound segment, locate the corresponding visual sound source temporally within a video, and vice versa. Then Wu et al. [47] introduce a novel dual attention matching method for this task. Recently, Lin et al. [48] propose a latent audio-visual hybrid adapter that adapts pre-trained vision transformers to audiovisual tasks, this method focuses on audio-video event localization task rather than cross-modal retrieval. In addition, textvideo retrieval is a key role in cross-modal retrieval. Generally, text-video retrieval can be divided into two categories, i.e., dual-encoder and joint-encoder. Dual-encoder based methods usually train two individual encoders to learn video and text representations and then align these representations in joint embedding spaces. Among them, some works [13], [49]–[51] focus on learning single global representations, but they lack the consideration of fine-grained information. Thereby, several works devote efforts to aligning fine-grained information [52]– [58]. Joint-encoder based methods [59]–[62] typically feed the video and text into a joint encoder to capture their cross-modal interactions. In comparison to dual-encoder based methods, joint-encoder based methods explicitly learn finegrained associations and achieve more impressive results, but sacrifice the retrieval efficiency since every text-video pair needs to be fed into the encoder at the inference time.</p>
<p>Different from the above video retrieval, we consider a more realistic scenario, where most videos contain anomalous events, and a more realistic demand, where videos are long untrimmed and partially relevant to the cross-modal query [63]. Such a new task poses extra challenges as well as multi-field research points. In addition, our ALAN also differs from video moment retrieval methods [64]–[67] in that it does not require complex cross-modal interactions.</p>

<h2 class="relative group">III. BENCHMARK
    <div id="iii-benchmark" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#iii-benchmark" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Manually collecting a large-scale video benchmark is laborintensive and time-consuming, it is also subjective since video understanding can often be an ill-defined task with low annotator consistency [68]. Therefore, we start with two acknowledged datasets in VAD community, i.e., UCF-Crime [6] and XD-Violence [7], and construct our benchmarks for VAR. We adopt these two datasets as the base since they thus far are the two most comprehensive VAD datasets in terms of length and scene, where the total duration of them are 128 and 217 hours, respectively. Besides, they are also collected from a variety of scenarios. For example, UCF-Crime covers 13 realworld anomalies as well as normal activities, and XD-Violence captures 6 anomalies and normal activities from movies and YouTube. In addition, both of them contain half normal videos and half abnormal videos, therefore, retrieval systems retrieve both abnormal and normal videos from the video gallery given related cross-modal queries in VAR. Large and diverse video databases allow us to construct more practicable benchmarks for VAR.</p>

<h2 class="relative group">A. UCFCrime-AR
    <div id="a-ucfcrime-ar" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-ucfcrime-ar" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>UCF-Crime dataset consists of 1900 untrimmed videos with 950 abnormal videos and 950 normal videos. Notably, for anomaly videos in the training set, the start timestamp and duration of anomalous activities are unavailable. For normal videos, they are totally anomaly-free. We directly use the total videos as the video search base. To achieve cross-modal retrieval, we require pairwise text descriptions.</p>
<p>We invite 8 experienced annotators who are proficient in Chinese and English to annotate these videos. The annotators watch the entire video and make the corresponding captions in both Chinese and English. Specifically, annotators are required to focus on anomalous events when describing anomaly videos. Due to the subtle differences in videos for the same anomaly category, we need to obtain quality sentence annotations to distinguish fine differences and avoid being into a one-to-many dilemma [69] which often appears in the current video retrieval. To be specific, there are at most two annotators to describe videos in the same category. For two similar videos in the same category, describe their differences in detail as much as possible. Take the scene of a fighting between two people as an example, e.g., &ldquo;At a party, the yellow-haired man suddenly attacked a man opposite him.&rdquo;, &ldquo;A young man suddenly beat another man with glasses in the elevator.&rdquo; The above two annotations clearly describe the difference between two similar videos. Finally, we double-check each sentence description to guarantee the quality.</p>
<p>Following the partition of UCF-Crime, UCFCrime-AR includes 1610 training videos and 290 test videos. Each video is annotated with captions in both English and Chinese. In this work, we only use captions in English.</p>

<h2 class="relative group">B. XDViolence-AR
    <div id="b-xdviolence-ar" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-xdviolence-ar" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>As for XD-Violence, we found that it is very hard to describe videos in a few sentences due to their complicated contents and scenarios. Hence we changed focus and started a new line of audio-to-video retrieval due to its natural audiovisual information, that is, we use videos and synchronous audios for cross-modal anomaly retrieval. Unlike texts, audios have the same granularity as videos. Similar to UCF-Crime, XD-Violence is also a weakly supervised dataset, namely, frame-level annotations are unknown. XDViolence-AR is split into two subsets, with 3954 long videos for training and 800 for testing.</p>

<h2 class="relative group">C. Benchmark Statistics
    <div id="c-benchmark-statistics" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-benchmark-statistics" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We compare two benchmarks with several cross-modal retrieval/location datasets in Table I. As we can see that, video databases in UCFCrime-AR and XDViolence-AR are both large-scale and are made public in recent years, where the former is applied to video-text (V-T) anomaly retrieval, and the latter is applied to video-audio anomaly retrieval (VA). Notably, the average length of videos in VAR benchmarks is significantly longer than that of videos in traditional video retrieval datasets. For example, the average length of videos of UCFCrime-AR and XDViolence-AR are 242s and 164s,</p>
<p>TABLE I COMPARISON OF UCFCRIME-AR AND XDVIOLENCE-AR WITH SEVERAL VIDEO -TEXT AND VIDEO -AUDIO RETRIEVAL/LOCALIZATION DATASETS .</p>
<table>
  <thead>
      <tr>
          <th>Datasets</th>
          <th>Duration</th>
          <th>#Videos</th>
          <th>Avg.len.</th>
          <th>Type</th>
          <th>Year</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>MSR-VTT [70]</td>
          <td>40h</td>
          <td>7.2k</td>
          <td>20s</td>
          <td>V-T</td>
          <td>2016</td>
      </tr>
      <tr>
          <td>VATEX [71]</td>
          <td>114h</td>
          <td>41k</td>
          <td>10s</td>
          <td>V-T</td>
          <td>2019</td>
      </tr>
      <tr>
          <td>TVR [74]</td>
          <td>463h</td>
          <td>21.8k</td>
          <td>76s</td>
          <td>V-T</td>
          <td>2020</td>
      </tr>
      <tr>
          <td>AVE [45]</td>
          <td>11h</td>
          <td>4.1k</td>
          <td>10s</td>
          <td>V-A</td>
          <td>2018</td>
      </tr>
      <tr>
          <td>AudioCaps [72]</td>
          <td>127h</td>
          <td>46k</td>
          <td>10s</td>
          <td>V-A</td>
          <td>2019</td>
      </tr>
      <tr>
          <td>LLP [73]</td>
          <td>33h</td>
          <td>11k</td>
          <td>10s</td>
          <td>V-A</td>
          <td>2020</td>
      </tr>
      <tr>
          <td>UCFCrime-AR</td>
          <td>128h</td>
          <td>1.9k</td>
          <td>242s</td>
          <td>V-T</td>
          <td>2018</td>
      </tr>
      <tr>
          <td>XDViolence-AR</td>
          <td>217h</td>
          <td>4.8k</td>
          <td>164s</td>
          <td>V-A</td>
          <td>2020</td>
      </tr>
  </tbody>
</table>
<p>Fig. 3. Statistical histogram distributions on UCFCrime-AR. Left: text captions in English; Right: text captions in Chinese.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_85994240b47a82186dbafe28b67e844fe2f333493dfba5c89231268514e565cd.png"
    ></figure>
<p>whereas that of MSR-VTT [70], VATEX [71], VAE [45], AudioCaps [72], and LLP [73] are in the range of 10s to 20s, and TVR [74] is mainly applied to video moment retrieval task, its average length of videos is still much shorter than our benchmarks. Longer videos emphasize again the goal of VAR is to retrieve long and untrimmed videos, such a setup meets realistic requirements, and also reveals VAR is a more challenging task. For video-text UCFCrime-AR, we also present histogram distributions of captions in Figure 3. The average caption lengths of UCFCrime-AR-en, UCFCrime-ARzh are 16.3 and 22.4, which are longer than those of previous datasets in video retrieval. e.g., the average caption lengths of VATEX-en [71], VATEX-zh [71], and MSR-VTT [70] are 15.23, 13.95, and 9.28, respectively.</p>

<h2 class="relative group">IV. METHOD
    <div id="iv-method" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#iv-method" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this section, we introduce ALAN in detail. In Sec. IV-A, we first introduce three encoders in ALAN, namely, video encoder, text encoder, and audio encoder, the goal of these encoders is to project raw videos, texts, and audios into high-level representations. In Sec. IV-B, we introduce the anomaly-led sampling mechanism which is utilized in both video encoder and audio encoder. In Sec. IV-C, we describe a novel pretext task, i.e., VPMPM, which is applied to videotext anomaly retrieval. At last, we describe the cross-modal alignment and training objectives in Secs. IV-D and IV-E.</p>

<h2 class="relative group">A. Encoders
    <div id="a-encoders" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-encoders" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video encoder. Unlike images, videos possess space-time information [75], [76]. As a consequence, we consider both appearance and motion information to encode videos. Specifically, given an video v, we use I3D-RGB and I3D-Flow pre-trained on Kinetics [77] to extract frame-level object and motion features, respectively, then project these features into a d-dimensional space for the subsequent operations. Here, object and motion feature sequences are denoted as F o (v) and F m (v), respectively. Both sequences contain T clip features. For the sake of clarity, we use F(v) to denote F o (v) and F m (v). Taking into account the variety of anomalous event duration in untrimmed videos, we sample two sparse video clips with different concerns, i.e., U and R, from F(v) by means of the fixed-frame sampling and our proposed anomalyled sampling.</p>
<p>As demonstrated in Figure 4, the video encoder is a symmetric two-stream model, one stream takes as input object, and the other takes as input motion. In order to fuse features in different modalities and different temporalities for final representations, we employ the Transformer [78] as the base model, which has been widely used in VAD and VR tasks with good results. For example, Huang et al. [34], [79] and Zhao et al. [80] used Transformer to tackle VAD and VR tasks, respectively. We first concatenate two different sampling clips as a new sequence, i.e., [UCLS, U1, &hellip;, RN , RCLS, R1, &hellip;, RN ] , where UCLS and RCLS are [CLS] tokens, which are the average aggregation of all features in U and R, respectively. Then, we add positional embeddings [78] and sequence embeddings to this sequence. Here positional embeddings provide temporal information about the time in the video, and sequence embeddings depict that features in U and R stem from different sequences. In video encoder, Self Encoder is devised to capture contextual information, which is a standard encoder layer in Transformer. The following Cross Encoder takes the selfmodality as the query, and cross-modality contextual features as the key and value to encode cross-modal representations through cross-modal attention. Cross Encoder is composed of multi-head attention, a linear layer, a residual connection, and a layer normalization. Finally, we obtain two different video representations, one is the average of output from UCLS and RCLS, denoted as g v (including g vo and g vm ), the other is the mean of average pooling aggregation of output from U and R, denoted as h v (including h vo and h vm ). Such a simple pooling operation is parameter-free and effective in our work, enabling h v to involve local fine-grained information.</p>
<p>Text encoder. Give a text caption t, we aim to learn the alignment between it and a related video at two different levels. At first, we leverage a pre-trained BERT [13] to extract features</p>
<p>Fig. 4. Overview of our ALAN. It consists of several components, i.e., video encoder, text encoder, audio encoder, pretext task VPMPM, and cross-modal alignment.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_4cc93c367f5816af5df182bb8dc46c8bd4b37452aa63b98438de760d6d9b6afb.png"
    ></figure>
<p>aided by its widespread adoption and proven performance in language representations. Following the video encoder, we obtain g t from the [CLS] output of BERT, and h t by using the average pooling operation for word-level representations. To match the object and motion representations of videos, here we use the gated embedding unit [11] acting on g t and h t to produce g to , g tm and h to , h tm , respectively.</p>
<p>Audio encoder. Give an audio a, we first extract audio features using a pre-trained VGGish [81], and project these features into the d-dimensional space. As shown in Figure 4, the audio encoder is similar to video encoder in terms of structure. The difference lies in that audio encoder is a single-stream model and has no Cross Encoder. In a similar vein, two different audio representations g a and h a are obtained. The gated embedding unit is also applied to match the object and motion representations of videos.</p>

<h2 class="relative group">B. Anomaly-Led Sampling
    <div id="b-anomaly-led-sampling" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-anomaly-led-sampling" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>As mentioned, only fixed-frame sampling (FS) cannot capture variable anomalous events in anomaly videos. We make use of anomaly priors and propose an anomaly-led sampling (AS) to enable that anomalous clips are more likely to be selected. Since frame-level annotations are unknown, it is impossible to directly identify anomalous clips. To solve this problem, we leverage a weakly supervised anomaly detector to predict clip-level anomaly confidences l ∈ R T , where l i ∈ [0 , 1]. With l in hands, we expect that for a clip, the probability of being selected is positively correlated with its anomaly confidence. A natural way is to select the top several clips with the highest anomaly confidence, but such a solution is too strict to be flexible. We believe that those clips with low anomaly confidences should also have a certain probability to be selected, on the one hand for data augmentation, on the other hand for salvaging false negatives of the anomaly detector. Taking inspiration from the selection strategy of evolutionary algorithms [82], [83], our anomaly-led sampling is based on the classical roulette-wheel selection [84]. To be specific, we regard anomaly confidences l as the fitness, and then normalize all values to the interval [0,1] to ensure summation of selection probabilities equals one,</p>
<!-- formula-not-decoded -->
<p>where p is selection probabilities, and τ is a temperature hyper-parameter [85]. Then calculate cumulative probabilities,</p>
<!-- formula-not-decoded -->
<p>It should be noted that q0 = 0 and qT = 1. The final step, then, is to generate N uniformly distributed random numbers in the interval [0, 1]. For each generated number r, the i-th feature in F(v) is selected if qi − 1 &lt; r ≤ qi
. A sequence with N clip-level features is assembled in such a way, where the larger the anomaly confidence of a clip is, the more likely is its selection. We present the algorithm flow of Anomaly-led sampling in Algorithm 1.</p>
<p>This feature sequence based on anomaly-led sampling are mainly applied to cover anomalous segments, meanwhile, we also use fixed-frame sampling, e.g., uniform or random, to generate another sequence with N clips for the entirety and normal scenarios.</p>

<h2 class="relative group">C. Video Prompt Based Masked Phrase Modeling
    <div id="c-video-prompt-based-masked-phrase-modeling" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-video-prompt-based-masked-phrase-modeling" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We propose a novel pretext task, i.e., video prompt based masked phrase modeling, for cross-modal fine-grained associations in video-text anomaly retrieval. VPMPM takes video representations and text representations as input and predicts the masked phrases, which is related to the prevalent masked</p>

<h2 class="relative group">Algorithm 1: : Anomaly-led sampling based on roulette-wheel selection
    <div id="algorithm-1--anomaly-led-sampling-based-on-roulette-wheel-selection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#algorithm-1--anomaly-led-sampling-based-on-roulette-wheel-selection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Input: anomaly confidence: l; video features: F(v)</p>
<p>Output: N clip-level features step1: Compute the selection probability;</p>
<!-- formula-not-decoded -->
<p>step2: Compute the cumulative probability;</p>
<!-- formula-not-decoded -->
<p>k ← 0;</p>
<p>while k &lt; N do</p>
<p>Step3: Generate a random number r ∈ [0</p>
<p>//</p>
<p>uniform distribution step4: Select features;</p>
<p>if qi</p>
<p>−</p>
<p>1 &lt; r ≤ qi then i-th feature in F(v) is selected;</p>
<p>end k ← k + 1;</p>
<p>end language modeling in nature language processing. The main difference lies in that (1) VPMPM masks and predicts noun phrases and verb phrases instead of randomly selected words. Unlike single words, noun phrases and verb phrases comprise words of different parts of speech, e.g., nouns, adjectives, verbs, adverbs, etc., better correspond to the local objects and motions in video frames; (2) VPMPM fuses video representations with text representations through cross-modal attention, where video representations serve as fixed prompts [23]. Such two specific designs encourage video encoder and text encoder to capture cross-modal and contextual representation interactions.</p>
<p>To achieve this pretext task, we introduce a Prompting Decoder, which is a standard decoder layer used in the Transformer. Since VPMPM involves the objectives of predicting masked noun phrases and masked verb phrases, Prompting Decoder needs to process noun phrases and verb phrases separately in a parameter-shared manner. Given the final video frame-level representations X v and text word-level X t , we first randomly replace a noun phrase or verb phrase representations with mask embeddings [86], where each mask token is a shared, learned vector. Here we denote this masked text representation as X b t . Then we take X b t as the query, and X v as the key and value, feed them into Prompting Decoder to predict the masked contents.</p>

<h2 class="relative group">D. Cross-Modal Alignment
    <div id="d-cross-modal-alignment" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-cross-modal-alignment" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this paper, cross-modal alignment is used to match representations of different modalities, e.g., video-text and video-audio, from two complementary perspectives. Hence, we deal with CLS alignment and AVG alignment. Unless otherwise stated, here we take video-text as an example to describe these two alignments.</p>
<p>,</p>
<p>1];</p>
<p>CLS alignment. CLS alignment is intended to compute the similarity between g v and g t , and the similarity between them is a weighted sum [13], which is computed as,</p>
<!-- formula-not-decoded -->
<p>where cos(· , · ) is the cosine similarity between two vectors. wta and wtm are weights, which are obtained from g ta and g tm , respectively. Specifically, we pass g ta (g tm ) through a linear layer with softmax normalization, and output wta (wtm). AVG alignment. AVG alignment is intended to compute the similarity s h (v, t) between h v and h t , which is same as CLS alignment. Notably, AVG alignment introduces more finegrained information. The similarity is presented as,</p>
<!-- formula-not-decoded -->

<h2 class="relative group">E. Training Objectives
    <div id="e-training-objectives" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#e-training-objectives" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The final similarity between v and t is the weighted sum of s g (v, t) and s h (v, t), namely,</p>
<!-- formula-not-decoded -->
<p>where α is a hyper-parameter, which lies in the range of [0,1]. Following the previous work [13], we obtain the bi-directional max-margin ranking loss, which is given by,</p>
<!-- formula-not-decoded -->
<p>where B is batch size, sij = s(vi, ti) .</p>
<p>To optimize the weakly supervised anomaly detector in video encoder, we use the top-k strategy [32], [87] to obtain the video-level prediction from frame-level confidences l, which is calculated as,</p>
<!-- formula-not-decoded -->
<p>where k = ⌊ T 16 ⌋, and l topk is the set of k-max framelevel confidences in l for the video v. We train this detector with binary cross-entropy loss Ltopk between the video-level prediction ρ v and video-level binary label y v ,</p>
<!-- formula-not-decoded -->
<p>For VPMPM in video-text anomaly retrieval, we adopt the cross-entropy loss L mpm between the model&rsquo;s predicted probability ρ t (X b t , X v ) and ground truth y mask , which is presented as follows,</p>
<!-- formula-not-decoded -->
<p>where y mask is a one-hot vocabulary distribution.</p>
<p>At last, the overall loss is shown as follows,</p>
<!-- formula-not-decoded -->
<p>TABLE II COMPARISONS WITH THE STATE -OF -THE -ART METHODS ON UCFCRIME-AR.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Text→Video</th>
          <th>Text→Video</th>
          <th>Text→Video</th>
          <th>Text→Video</th>
          <th>Video→Text</th>
          <th>Video→Text</th>
          <th>Video→Text</th>
          <th>Video→Text</th>
          <th>SumR↑</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Method</td>
          <td>R@1↑</td>
          <td>R@5↑</td>
          <td>R@10↑</td>
          <td>MdR↓</td>
          <td>R@1↑</td>
          <td>R@5↑</td>
          <td>R@10↑</td>
          <td>MdR↓</td>
          <td>SumR↑</td>
      </tr>
      <tr>
          <td>Random Baseline</td>
          <td>0.3</td>
          <td>2.1</td>
          <td>3.4</td>
          <td>144.0</td>
          <td>0.3</td>
          <td>1.0</td>
          <td>3.1</td>
          <td>145.5</td>
          <td>10.2</td>
      </tr>
      <tr>
          <td>CE [12]</td>
          <td>6.6</td>
          <td>19.7</td>
          <td>32.4</td>
          <td>23.5</td>
          <td>5.5</td>
          <td>19.7</td>
          <td>32.4</td>
          <td>21.0</td>
          <td>116.3</td>
      </tr>
      <tr>
          <td>MMT [13]</td>
          <td>8.3</td>
          <td>26.2</td>
          <td>39.3</td>
          <td>16.0</td>
          <td>7.2</td>
          <td>23.1</td>
          <td>39.0</td>
          <td>16.0</td>
          <td>143.1</td>
      </tr>
      <tr>
          <td>T2VLAD [89]</td>
          <td>7.6</td>
          <td>23.4</td>
          <td>39.7</td>
          <td>15.5</td>
          <td>6.2</td>
          <td>27.9</td>
          <td>43.1</td>
          <td>14.0</td>
          <td>147.9</td>
      </tr>
      <tr>
          <td>X-CLIP [58]</td>
          <td>8.2</td>
          <td>27.2</td>
          <td>41.7</td>
          <td>16.0</td>
          <td>6.9</td>
          <td>25.8</td>
          <td>40.3</td>
          <td>15.0</td>
          <td>150.1</td>
      </tr>
      <tr>
          <td>HL-Net [7]</td>
          <td>5.5</td>
          <td>20.2</td>
          <td>38.3</td>
          <td>19.5</td>
          <td>5.5</td>
          <td>22.8</td>
          <td>35.5</td>
          <td>20.0</td>
          <td>127.8</td>
      </tr>
      <tr>
          <td>XML [74]</td>
          <td>6.9</td>
          <td>24.1</td>
          <td>42.4</td>
          <td>14.0</td>
          <td>6.6</td>
          <td>25.9</td>
          <td>43.4</td>
          <td>13.0</td>
          <td>149.3</td>
      </tr>
      <tr>
          <td>ALAN</td>
          <td>9.0</td>
          <td>27.9</td>
          <td>44.8</td>
          <td>14.0</td>
          <td>7.3</td>
          <td>24.8</td>
          <td>46.9</td>
          <td>12.0</td>
          <td>160.7</td>
      </tr>
  </tbody>
</table>
<p>TABLE III COMPARISONS WITH THE STATE -OF -THE -ART METHODS ON XDVIOLENCE-AR.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Audio→Video</th>
          <th>Audio→Video</th>
          <th>Audio→Video</th>
          <th>Audio→Video</th>
          <th>Video→Audio</th>
          <th>Video→Audio</th>
          <th>Video→Audio</th>
          <th>Video→Audio</th>
          <th>SumR↑</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Method</td>
          <td>R@1↑</td>
          <td>R@5↑</td>
          <td>R@10↑</td>
          <td>MdR↓</td>
          <td>R@1↑</td>
          <td>R@5↑</td>
          <td>R@10↑</td>
          <td>MdR↓</td>
          <td>SumR↑</td>
      </tr>
      <tr>
          <td>Random Baseline</td>
          <td>0.4</td>
          <td>0.6</td>
          <td>2.5</td>
          <td>399.5</td>
          <td>0.1</td>
          <td>0.6</td>
          <td>0.8</td>
          <td>399.5</td>
          <td>5.0</td>
      </tr>
      <tr>
          <td>CE [12]</td>
          <td>11.4</td>
          <td>33.3</td>
          <td>47.0</td>
          <td>12.5</td>
          <td>13.0</td>
          <td>34.3</td>
          <td>46.4</td>
          <td>13.0</td>
          <td>185.4</td>
      </tr>
      <tr>
          <td>MMT [13]</td>
          <td>20.5</td>
          <td>53.5</td>
          <td>68.0</td>
          <td>5.0</td>
          <td>23.0</td>
          <td>54.6</td>
          <td>69.5</td>
          <td>5.0</td>
          <td>289.1</td>
      </tr>
      <tr>
          <td>T2VLAD [89]</td>
          <td>22.4</td>
          <td>56.1</td>
          <td>71.0</td>
          <td>4.0</td>
          <td>23.2</td>
          <td>57.1</td>
          <td>73.5</td>
          <td>4.0</td>
          <td>303.3</td>
      </tr>
      <tr>
          <td>X-CLIP [58]</td>
          <td>26.4</td>
          <td>61.1</td>
          <td>73.9</td>
          <td>3.0</td>
          <td>26.4</td>
          <td>61.3</td>
          <td>73.8</td>
          <td>4.0</td>
          <td>322.9</td>
      </tr>
      <tr>
          <td>HL-Net [7]</td>
          <td>12.4</td>
          <td>36.6</td>
          <td>48.3</td>
          <td>11.0</td>
          <td>13.4</td>
          <td>38.3</td>
          <td>52.1</td>
          <td>10.0</td>
          <td>201.1</td>
      </tr>
      <tr>
          <td>XML [74]</td>
          <td>22.9</td>
          <td>55.6</td>
          <td>70.3</td>
          <td>5.0</td>
          <td>22.6</td>
          <td>57.4</td>
          <td>71.4</td>
          <td>4.0</td>
          <td>300.2</td>
      </tr>
      <tr>
          <td>ALAN</td>
          <td>29.8</td>
          <td>68.0</td>
          <td>82.0</td>
          <td>3.0</td>
          <td>32.3</td>
          <td>70.0</td>
          <td>82.3</td>
          <td>3.0</td>
          <td>364.4</td>
      </tr>
  </tbody>
</table>

<h2 class="relative group">V. EXPERIMENTS
    <div id="v-experiments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#v-experiments" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">A. Experimental Settings
    <div id="a-experimental-settings" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-experimental-settings" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Evaluation metrics. Following prior works, we use the rankbased metric for performance evaluation, i.e., Recall at K (R@K, K=1, 5, 10), Median Rank (MdR), and Sum of all Recalls (SumR) to measure the overall performance.</p>
<p>Implementation details. We use Spacy 1 to extract noun phrases and verb phrases. In video encoder and audio encoder, the anomaly detector is composed of 3 temporal convolution layers with kernel size of 7, the first layer has 128 units followed by 32 units and 1 unit layers. The first two layers are followed by ReLU, and the last layer is followed by Sigmoid. Dropout with rate of 0.6 is applied to the first two layers. In the text encoder, we use the &ldquo;BERT-base-cased model&rdquo; and fine-tune it with a dropout rate of 0.3.</p>
<p>Training. We train our model with a batch size of 64 using Adam [88] optimizer. The initial learning rate is set as 5 × 10 − 5 and decays by a multiplicative factor 0.95 per epoch. For hyper-parameters, hidden size d is set as 768, and temperature parameter τ in Eq. 1 is set as 0.7. Empirically, we found the weight ratio α=0.5 in Eq. 5 and sampling length N=50 worked well across different benchmarks. As the setup in [13], the margin ∆ in Eq. 6 is set as 0.05. λ1 and λ2 in Eq. 10 is set as 0.1 and 0.01, respectively, such a setup achieves optimal performance.</p>

<h2 class="relative group">B. Comparison with State-of-the-Art Methods
    <div id="b-comparison-with-state-of-the-art-methods" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-comparison-with-state-of-the-art-methods" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We conduct experiments on UCFCrime-AR and XDViolence-AR and compare our ALAN with several recent methods that are widely used in video retrieval, video moment retrieval and VAD. CE [12], MMT [13],</p>
<p>T2VLAD [89], and X-CLIP [58] are video retrieval methods; XML [74] is a video moment retrieval method, here it is used to retrieve videos, where the moment localization part is removed since moment annotations are unavailable in VAR. HL-Net [7] is a VAD method, since VAD is quite distinct from VAR, it is hard to directly use VAD method for VAR, here, we modify it as a video encoder for VAR. All methods use BERT to extract language features except CE that uses the word2vec word embeddings [90]. We present comparison results in Tables II and III, and observe that our ALAN shows a clear advantage over comparison methods in both text-video and audio-video VAR. Specifically, ALAN outperforms CE, MMT, T2VLAD, X-CLIP, HL-Net, and XML on UCFCrime-AR by 44.4, 17.6, 12.8, 10.6, 32.9, and 11.4 in terms of SumR, respectively. Furthermore, ALAN also achieves clear improvements against competitors on XDViolence-AR, which achieves a significant performance improvement of 41.5 in terms of SumR over the previous best method. Moreover, It can be found that, in comparison to the video and text, the video and audio are easier to align. We argue that video and audio are synchronous with concordant granularity, thereby leading to better align performance in VAR.</p>

<h2 class="relative group">C. Ablation Studies
    <div id="c-ablation-studies" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-ablation-studies" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Study on anomaly-led sampling. As aforementioned, we propose a novel sampling mechanism, i.e., anomaly-led sampling, which combines with the ordinary fixed-frame sampling, and the joint effort is devoted to capturing local anomalous segments as well as overall information. To investigate the effectiveness of anomaly-led sampling, we conduct experiments on two benchmarks, and show results on Tables IV and V. As we can see from the first two rows, only using fixed-frame sampling or anomaly-led sampling results in a clear performance</p>
<p>TABLE IV COMPARISONS OF DIFFERENT SAMPLINGS ON UCFCRIME-AR.</p>
<table>
  <thead>
      <tr>
          <th>Sampling</th>
          <th>Text→Video</th>
          <th>Text→Video</th>
          <th>Video→Text</th>
          <th>Video→Text</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Sampling</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
      </tr>
      <tr>
          <td>FS (N=50)</td>
          <td>6.6</td>
          <td>35.5</td>
          <td>4.8</td>
          <td>42.4</td>
      </tr>
      <tr>
          <td>AS (N=50)</td>
          <td>7.9</td>
          <td>37.6</td>
          <td>5.5</td>
          <td>41.7</td>
      </tr>
      <tr>
          <td>FS (N=100)</td>
          <td>6.6</td>
          <td>37.6</td>
          <td>6.2</td>
          <td>40.3</td>
      </tr>
      <tr>
          <td>FS+AS (N=50)</td>
          <td>9.0</td>
          <td>44.8</td>
          <td>7.3</td>
          <td>46.9</td>
      </tr>
  </tbody>
</table>
<p>TABLE V COMPARISONS OF DIFFERENT SAMPLINGS ON XDVIOLENCE-AR.</p>
<table>
  <thead>
      <tr>
          <th>Sampling</th>
          <th>Text→Video</th>
          <th>Text→Video</th>
          <th>Video→Text</th>
          <th>Video→Text</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Sampling</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
      </tr>
      <tr>
          <td>FS (N=50)</td>
          <td>29.6</td>
          <td>80.4</td>
          <td>31.1</td>
          <td>80.9</td>
      </tr>
      <tr>
          <td>AS (N=50)</td>
          <td>26.9</td>
          <td>78.6</td>
          <td>27.4</td>
          <td>78.9</td>
      </tr>
      <tr>
          <td>FS (N=100)</td>
          <td>28.5</td>
          <td>81.0</td>
          <td>29.8</td>
          <td>81.8</td>
      </tr>
      <tr>
          <td>FS+AS (N=50)</td>
          <td>29.8</td>
          <td>82.0</td>
          <td>32.3</td>
          <td>82.3</td>
      </tr>
  </tbody>
</table>
<p>drop on both UCFCrime-AR and XDViolence-AR. Besides, using anomaly-led sampling is inferior to using fixed-sampling on XDViolence-AR, we discover that the main reason for this problem is that the anomaly-led sampling mechanism is applied to both video and audio, resulting in key segments misalignment to some extent. Moreover, we also investigate the effect of sampling length. From the third row, we found that increasing the sampling length from 50 to 100 does not dramatically improve performance, and fixed-frame sampling still lags behind the combination of fixed-frame sampling and anomaly-led sampling, even though they both have the same sampling length at the moment. It also clearly demonstrates that the joint effect between anomaly-led sampling and fixedframe sampling enables our model to capture key anomalous segments as well as holistic data information, thus facilitating cross-modal alignment under local-anomaly and global-video perspectives. For example, in Figure 8, video frames that are selected by anomaly-led sampling are aligned with the key anomaly descriptions, e.g., two car collided violently, a man in black lay on the ground and shot. On another hand, these video frames selected by fixed-frame sampling are aligned with the complete descriptions.</p>
<p>Study on VPMPM. Here we conduct experiments to certify the advantage of VPMPM for video-text fine-grained associations. When ALAN removes VPMPM at training time, we observe the performance clearly drops as shown in Table VI. Besides, masking and predicting in the form of random words rather than noun phrases and verb phrases in VPMPM hurts performance. We can also see that, using noun phrases and verb phrases are superior to noun and verb words on most evaluation metrics. This demonstrates that noun phrases and verb phrases, as the sequences of words with different parts of speech, can better align with related local contents in videos. Study on cross-modal alignment. Tables VII and VIII present the performance of two different alignments in our ALAN. We found that CLS alignment and AVG alignment obtain worse results when used alone in comparison to the model of jointly using both. Such results demonstrate the complementarity of these two alignments. A key observation is the AVG alignment performs better than CLS alignment on XDViolence-AR, but</p>
<p>TABLE VI VPMPM STUDIES ON UCFCRIME-AR.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Text→Video</th>
          <th>Text→Video</th>
          <th>Video→Text</th>
          <th>Video→Text</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Method</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
      </tr>
      <tr>
          <td>w/o VPMPM</td>
          <td>7.9</td>
          <td>43.4</td>
          <td>7.2</td>
          <td>43.4</td>
      </tr>
      <tr>
          <td>random words</td>
          <td>8.6</td>
          <td>43.8</td>
          <td>6.2</td>
          <td>44.5</td>
      </tr>
      <tr>
          <td>noun&amp;verb words</td>
          <td>10.0</td>
          <td>44.5</td>
          <td>6.6</td>
          <td>42.8</td>
      </tr>
      <tr>
          <td>noun&amp;verb phrases</td>
          <td>9.0</td>
          <td>44.8</td>
          <td>7.3</td>
          <td>46.9</td>
      </tr>
  </tbody>
</table>
<p>TABLE VII COMPARISONS OF DIFFERENT ALIGNMENTS ON UCFCRIME-AR.</p>
<p>TABLE VIII COMPARISONS OF DIFFERENT ALIGNMENTS ON XDVIOLENCE-AR.</p>
<table>
  <thead>
      <tr>
          <th>Alignment</th>
          <th>Text→Video</th>
          <th>Text→Video</th>
          <th>Video→Text</th>
          <th>Video→Text</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Alignment</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
      </tr>
      <tr>
          <td>CLS</td>
          <td>6.2</td>
          <td>42.8</td>
          <td>7.6</td>
          <td>40.3</td>
      </tr>
      <tr>
          <td>AVG</td>
          <td>6.6</td>
          <td>33.8</td>
          <td>4.8</td>
          <td>36.9</td>
      </tr>
      <tr>
          <td>CLS+AVG</td>
          <td>9.0</td>
          <td>44.8</td>
          <td>7.3</td>
          <td>46.9</td>
      </tr>
  </tbody>
</table>
<p>Fig. 5. Influences of α on both UCFCrime-AR and XDViolence-AR.</p>
<table>
  <thead>
      <tr>
          <th>Alignment</th>
          <th>Audio→Video</th>
          <th>Audio→Video</th>
          <th>Video→Audio</th>
          <th>Video→Audio</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Alignment</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
      </tr>
      <tr>
          <td>CLS</td>
          <td>26.8</td>
          <td>77.9</td>
          <td>28.6</td>
          <td>77.4</td>
      </tr>
      <tr>
          <td>AVG</td>
          <td>28.0</td>
          <td>79.3</td>
          <td>30.0</td>
          <td>80.1</td>
      </tr>
      <tr>
          <td>CLS+AVG</td>
          <td>29.8</td>
          <td>82.0</td>
          <td>32.3</td>
          <td>82.3</td>
      </tr>
  </tbody>
</table>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000004_1fa8a41fdc19f493ea799cd30b866cdb173f596f5749198cec0a759a937b2a16.png"
    ></figure>
<p>the opposite is true on UCFCrime-AR, we suspect that video and audio are easier to align at the fine-grained level due to their concordant granularity. Moreover, we also investigate the influence of α. We try α with its value ranging from 0.0 to 1.0 with an interval of 0.1. As shown in Figure 5, with the increase of α, the performance gradually improves and then decreases, when α is set as 0.5, our method achieves the best performance. In order to further explore how to choose α , we also show the detailed retrieval results of different α in Tables IX and X. It is not hard to see that it is a balanced choice to set the value range of α to 0.4-0.6, where two different cross-modal alignments make nearly the same contribution.</p>

<h2 class="relative group">D. Qualitative Analyses
    <div id="d-qualitative-analyses" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-qualitative-analyses" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Visualization of retrieval results. Some text-to-video retrieval examples on UCFCrime-AR are exhibited in Figure 6, where retrieval results of a normal video is shown at the far right. We observe ALAN successfully retrieves the related video given</p>
<p>Fig. 6. Some retrieval examples on UCFCrime-AR. We visualize top 3 retrieved videos (green: correct; pink: incorrect).</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000005_50065715b9d076fcfdf43b5d22aaefcb3d327e2b96c3249fb73a1e9e52c04f76.png"
    ></figure>
<p>TABLE IX DETAILED INFLUENCES OF α ON UCFCRIME-AR.</p>
<table>
  <thead>
      <tr>
          <th>Value of α</th>
          <th>Audio→Video</th>
          <th>Audio→Video</th>
          <th>Video→Audio</th>
          <th>Video→Audio</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Value of α</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
      </tr>
      <tr>
          <td>0.0</td>
          <td>6.6</td>
          <td>33.8</td>
          <td>4.8</td>
          <td>36.9</td>
      </tr>
      <tr>
          <td>0.2</td>
          <td>8.3</td>
          <td>38.6</td>
          <td>6.2</td>
          <td>40.3</td>
      </tr>
      <tr>
          <td>0.4</td>
          <td>7.9</td>
          <td>42.4</td>
          <td>6.9</td>
          <td>44.5</td>
      </tr>
      <tr>
          <td>0.5</td>
          <td>9.0</td>
          <td>44.8</td>
          <td>7.3</td>
          <td>46.9</td>
      </tr>
      <tr>
          <td>0.6</td>
          <td>7.6</td>
          <td>45.2</td>
          <td>6.2</td>
          <td>47.2</td>
      </tr>
      <tr>
          <td>0.8</td>
          <td>7.6</td>
          <td>43.8</td>
          <td>5.5</td>
          <td>43.4</td>
      </tr>
      <tr>
          <td>1.0</td>
          <td>6.2</td>
          <td>42.8</td>
          <td>7.6</td>
          <td>40.3</td>
      </tr>
  </tbody>
</table>
<p>TABLE X DETAILED INFLUENCES OF α ON UCFCRIME-AR.</p>
<table>
  <thead>
      <tr>
          <th>Value of α</th>
          <th>Audio→Video</th>
          <th>Audio→Video</th>
          <th>Video→Audio</th>
          <th>Video→Audio</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Value of α</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
          <td>R@1↑</td>
          <td>R@10↑</td>
      </tr>
      <tr>
          <td>0.0</td>
          <td>28.0</td>
          <td>79.3</td>
          <td>30.0</td>
          <td>80.1</td>
      </tr>
      <tr>
          <td>0.2</td>
          <td>30.4</td>
          <td>80.8</td>
          <td>32.1</td>
          <td>82.6</td>
      </tr>
      <tr>
          <td>0.4</td>
          <td>31.9</td>
          <td>80.9</td>
          <td>32.3</td>
          <td>82.4</td>
      </tr>
      <tr>
          <td>0.5</td>
          <td>29.8</td>
          <td>82.0</td>
          <td>32.3</td>
          <td>82.3</td>
      </tr>
      <tr>
          <td>0.6</td>
          <td>31.0</td>
          <td>82.6</td>
          <td>32.8</td>
          <td>80.8</td>
      </tr>
      <tr>
          <td>0.8</td>
          <td>28.0</td>
          <td>79.8</td>
          <td>30.1</td>
          <td>79.3</td>
      </tr>
      <tr>
          <td>1.0</td>
          <td>26.8</td>
          <td>77.9</td>
          <td>28.6</td>
          <td>77.4</td>
      </tr>
  </tbody>
</table>
<p>a text query, and there are considerable similarities between the top 3 retrieved videos. This also demonstrates VAR is a challenging task as some scenes are similar with delicate differences.</p>
<p>Visualization of coarse caption retrieval. In VAR task, the purpose of using accurate captions is to distinguish fine differences and avoid being into a one-to-many dilemma [69]. To further verify the generalization capacity of ALAN, we use several coarse captions that are not directly applied in model training to retrieve videos, results in Figure 7 clearly show that ALAN works very well with different lengths of coarse captions, and also demonstrate ALAN has learned several abstract semantic information, e.g., explosion, fighting, traffic. This also convincingly indicates our methods can meet practical requirements where users cannot provide a complete text description of the videos they intend to search, such as the example in the lower right of Figure 7, users give the the retrieval model a incomplete description &ldquo;man robbed people&rdquo;, and the model returns top 3 related videos, in which the contents correspond to robbery, steal, man, and people.</p>
<p>Visualization of anomaly-led sampling. We visualize video frames selected by fixed-frame sampling and anomaly-led sampling in Figure 8. These examples are taken from videos</p>
<p>Fig. 7. Some coarse caption retrieval examples on UCFCrime-AR.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000006_8cbe8b72a6adfb6c8591506aa5ac370cd4b1e7fdf58c7a2c1ea47d26487ee223.png"
    ></figure>
<p>of road accident and shooting scenes. It can be seen from the second row that the duration of anomalous event accounts for less than one-fifth of the entire video length, therefore, frames related to the anomalous event are hard to select based on fixed-frame sampling. In stark contrast to fixedframe sampling, anomaly-led sampling is based on anomaly confidences generated by the anomaly detector, and it can select more frames related to anomalous events since the probability of being selected has positive correlations with anomaly confidences, where anomaly detector generates high confidences in anomalous segments which is shown in the second row.</p>
<p>Visualization of zero-shot retrieval. ALAN is trained on UCFCrime-AR and XDViolence-AR for text-video and audiovideo anomaly retrieval, respectively. Moreover, scenarios in these two benchmarks are different, because videos from UCFCrime-AR are captured with fixed cameras, whereas videos from XDViolence-AR are collected from movies and YouTube. Here we explore that, given a cross-modal query from UCFCrime-AR (or XDViolence-AR), is ALAN trained on UCFCrime-AR (or XDViolece-AR) capable of retrieving some relevant videos from XDViolence-AR (or texts from UCFCrime-AR)? We show the top 2 retrieval results in Figure 9. In text-to-video anomaly retrieval, we found that given text queries from UCFCrime-AR, ALAN can retrieve some videos from XDViolence-AR that look semantically plausible, even if there are no completely relevant videos in XDViolence-AR. Interestingly, the video in the bottom</p>
<p>Fixed</p>
<ul>
<li></li>
</ul>
<p>Frame</p>
<p>Sampling</p>
<p>Anomaly</p>
<p>Confidence</p>
<p>Anomaly-Led</p>
<p>Sampling</p>
<p>GT</p>
<p>At night</p>
<p>,</p>
<p>the two cars collided violently in the middle of the crossroad and crashed into the side of the road</p>
<p>.</p>
<p>Time Axis</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000007_22cc091177052528049679d9da8334a2a6ff611ee5116f01e6a2b7e84fb76d83.png"
    ></figure>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000008_e57f5efa0d6f55780c2fc18ca441e75c25108b1723deb362256ea921a782f9ea.png"
    ></figure>
<p>Fig. 8. Different samplings for video frame selection. Left: road accident; Right: Shooting.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000009_169a0f3bf67607415c77ee28a05f4745173aea22994081870a4f65a2c5f55d82.png"
    ></figure>
<p>Text</p>
<ul>
<li></li>
</ul>
<p>to-Video</p>
<p>Video</p>
<ul>
<li></li>
</ul>
<p>to-Text</p>
<p>Fig. 9. Zero-shot retrieval results. The left two columns present zero-shot text-to-video anomaly retrieval, and the right two columns present zero-shot video-to-text anomaly retrieval.</p>
<p>left is an animation. ALAN learns several local semantic contents and retrieves videos based on these local semantic contents, such as &ldquo;huge fire&rdquo; and &ldquo;mushroom cloud&rdquo;. In videoto-text anomaly retrieval, although retrieved text descriptions are not completely related, ALAN captures partial semantic information from movie videos, such as &ldquo;a man&rdquo;, &ldquo;a female companion&rdquo;, &ldquo;knock down somebody with fists&rdquo;, etc.</p>

<h2 class="relative group">E. Running Time
    <div id="e-running-time" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#e-running-time" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We report the retrieval time for UCFCrime-AR with 290 video-text test pairs and XDViolence-AR with 800 videoaudio test pairs, our method costs 2.7s and 5.6s, respectively. Generally, it only needs about 0.008s to process a pair on both datasets, showing its higher efficiency. The reason why our method remains high retrieval efficiency is that it has a dual-encoder structure during the test stage, that is, using two separate encoders to embed video and text features and project them into the latent joint space, and only the cosine similarity between video and text features is calculated as similarity, without complicated and inefficient cross-modal interactions. However, it is worth noting that, during the training phase, our method integrates text and video as inputs to a joint encoder for the cross-modality fusion, which can establish local correlation between video-text features and improve retrieval accuracy. Therefore, our method obtains the advantages of the above two kinds of methods, that is, achieving finegrained video-text interactions while maintaining high retrieval efficiency.</p>

<h2 class="relative group">VI. CONCLUSION
    <div id="vi-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#vi-conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this paper, we introduce a new task called video anomaly retrieval to remedy the inadequacy of video anomaly de- tection in terms of abnormal event depict, further facilitate video anomaly analysis research in cross-modal scenarios. We construct two VAR benchmarks, i.e., UCFCrime-AR and XDViolence-AR, based on popular VAD datasets. Moreover, we propose ALAN which includes several components, where anomaly-led sampling is used to capture local anomalous segments, which coordinates with ordinary fixed-frame sampling to achieve complementary effects; Video prompt based masked phrase modeling is used to learn cross-modal finegrained associations; Cross-modal alignment is used to match cross-modal representations from two perspectives. The future work will lie in two aspects, 1) exploiting cross-modal pretrained models to capture more powerful knowledge for VAR; 2) leveraging VAR to assist VAD methods for more precise anomaly detection.</p>

<h2 class="relative group">REFERENCES
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>
<p>[1] M. Sabokrou, M. Fayyaz, M. Fathy, and R. Klette, &ldquo;Deep-cascade: Cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes,&rdquo; IEEE Transactions on Image Processing , vol. 26, no. 4, pp. 1992–2004, 2017.</p>
</li>
<li>
<p>[2] W. Liu, W. Luo, D. Lian, and S. Gao, &ldquo;Future frame prediction for anomaly detection–a new baseline,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 6536–6545.</p>
</li>
<li>
<p>[3] P. Wu, J. Liu, and F. Shen, &ldquo;A deep one-class neural network for anomalous event detection in complex scenes,&rdquo; IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 7, pp. 2609–2622, 2019.</p>
</li>
<li>
<p>[4] H. Park, J. Noh, and B. Ham, &ldquo;Learning memory-guided normality for anomaly detection,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 14 372–14 381.</p>
</li>
<li>
<p>[5] M.-I. Georgescu, A. Barbalau, R. T. Ionescu, F. S. Khan, M. Popescu, and M. Shah, &ldquo;Anomaly detection in video via self-supervised and multi-task learning,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 12 742–12 752.</p>
</li>
<li>
<p>[6] W. Sultani, C. Chen, and M. Shah, &ldquo;Real-world anomaly detection in surveillance videos,&rdquo; in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 6479–6488.</p>
</li>
<li>
<p>[7] P. Wu, J. Liu, Y. Shi, Y. Sun, F. Shao, Z. Wu, and Z. Yang, &ldquo;Not only look, but also listen: Learning multimodal violence detection under weak supervision,&rdquo; in Proceedings of the European Conference on Computer Vision. Springer, 2020, pp. 322–339.</p>
</li>
<li>
<p>[8] J.-C. Feng, F.-T. Hong, and W.-S. Zheng, &ldquo;Mist: Multiple instance selftraining framework for video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 14 009–14 018.</p>
</li>
<li>
<p>[9] Y. Tian, G. Pang, Y. Chen, R. Singh, J. W. Verjans, and G. Carneiro, &ldquo;Weakly-supervised video anomaly detection with robust temporal feature magnitude learning,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.</p>
</li>
<li>
<p>[10] J. Wu, W. Zhang, G. Li, W. Wu, X. Tan, Y. Li, E. Ding, and L. Lin, &ldquo;Weakly-supervised spatio-temporal anomaly detection in surveillance video,&rdquo; arXiv preprint arXiv:2108.03825, 2021.</p>
</li>
<li>
<p>[11] A. Miech, I. Laptev, and J. Sivic, &ldquo;Learning a text-video embedding from incomplete and heterogeneous data,&rdquo; arXiv preprint arXiv:1804.02516 , 2018.</p>
</li>
<li>
<p>[12] Y. Liu, S. Albanie, A. Nagrani, and A. Zisserman, &ldquo;Use what you have: Video retrieval using representations from collaborative experts,&rdquo; arXiv preprint arXiv:1907.13487, 2019.</p>
</li>
<li>
<p>[13] V. Gabeur, C. Sun, K. Alahari, and C. Schmid, &ldquo;Multi-modal transformer for video retrieval,&rdquo; in Proceedings of the 16th European Conference on Computer Vision. Springer, 2020, pp. 214–229.</p>
</li>
<li>
<p>[14] X. Yang, S. Wang, J. Dong, J. Dong, M. Wang, and T.-S. Chua, &ldquo;Video moment retrieval with cross-modal neural architecture search,&rdquo; IEEE Transactions on Image Processing, vol. 31, pp. 1204–1216, 2022.</p>
</li>
<li>
<p>[15] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, &ldquo;Tubedetr: Spatiotemporal video grounding with transformers,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022, pp. 16 442–16 453.</p>
</li>
<li>
<p>[16] R. Cui, T. Qian, P. Peng, E. Daskalaki, J. Chen, X. Guo, H. Sun, and Y.-G. Jiang, &ldquo;Video moment retrieval from text queries via single frame annotation,&rdquo; in Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval , 2022, pp. 1033–1043.</p>
</li>
<li>
<p>[17] G. Wang, X. Xu, F. Shen, H. Lu, Y. Ji, and H. T. Shen, &ldquo;Cross-modal dynamic networks for video moment retrieval with text query,&rdquo; IEEE Transactions on Multimedia, vol. 24, pp. 1221–1232, 2022.</p>
</li>
<li>
<p>[18] Y. Han, G. Huang, S. Song, L. Yang, H. Wang, and Y. Wang, &ldquo;Dynamic neural networks: A survey,&rdquo; arXiv preprint arXiv:2102.04906, 2021.</p>
</li>
<li>
<p>[19] Y. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh, &ldquo;Dynamicvit: Efficient vision transformers with dynamic token sparsification,&rdquo; arXiv preprint arXiv:2106.02034, 2021.</p>
</li>
<li>
<p>[20] Y. Zhi, Z. Tong, L. Wang, and G. Wu, &ldquo;Mgsampler: An explainable sampling strategy for video action recognition,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.</p>
</li>
<li>
<p>[21] M. Fayyaz, S. A. Koohpayegani, F. R. Jafari, S. Sengupta, H. R. V. Joze, E. Sommerlade, H. Pirsiavash, and J. Gall, &ldquo;Adaptive token sampling for efficient vision transformers,&rdquo; in Proceedings of the European Conference on Computer Vision. Springer, 2022, pp. 396–414.</p>
</li>
<li>
<p>[22] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi, &ldquo;Align before fuse: Vision and language representation learning with momentum distillation,&rdquo; Advances in Neural Information Processing Systems, vol. 34, pp. 9694–9705, 2021.</p>
</li>
<li>
<p>[23] Z. Hou, F. Sun, Y.-K. Chen, Y. Xie, and S.-Y. Kung, &ldquo;Milan: Masked image pretraining on language assisted representation,&rdquo; arXiv preprint arXiv:2208.06049, 2022.</p>
</li>
<li>
<p>[24] S. Lee, H. G. Kim, and Y. M. Ro, &ldquo;Bman: Bidirectional multi-scale aggregation networks for abnormal event detection,&rdquo; IEEE Transactions on Image Processing, vol. 29, pp. 2395–2408, 2019.</p>
</li>
<li>
<p>[25] R. T. Ionescu, F. S. Khan, M.-I. Georgescu, and L. Shao, &ldquo;Object-centric auto-encoders and dummy anomalies for abnormal event detection in video,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7842–7851.</p>
</li>
<li>
<p>[26] D. Gong, L. Liu, V. Le, B. Saha, M. R. Mansour, S. Venkatesh, and A. v. d. Hengel, &ldquo;Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 1705–1714.</p>
</li>
<li>
<p>[27] G. Wang, Y. Wang, J. Qin, D. Zhang, X. Bao, and D. Huang, &ldquo;Video anomaly detection by solving decoupled spatio-temporal jigsaw puzzles,&rdquo; 2022.</p>
</li>
<li>
<p>[28] Z. Yang, P. Wu, J. Liu, and X. Liu, &ldquo;Dynamic local aggregation network with adaptive clusterer for anomaly detection,&rdquo; in Proceedings of the European Conference on Computer Vision. Springer, 2022, pp. 404– 421.</p>
</li>
<li>
<p>[29] Z. Yang, J. Liu, Z. Wu, P. Wu, and X. Liu, &ldquo;Video event restoration based on keyframes for video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp. 14 592–14 601 .</p>
</li>
<li>
<p>[30] C. Yan, S. Zhang, Y. Liu, G. Pang, and W. Wang, &ldquo;Feature prediction diffusion model for video anomaly detection,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 5527–5537 .</p>
</li>
<li>
<p>[31] H. Lv, C. Zhou, Z. Cui, C. Xu, Y. Li, and J. Yang, &ldquo;Localizing anomalies from weakly-labeled videos,&rdquo; IEEE transactions on image processing , vol. 30, pp. 4505–4515, 2021.</p>
</li>
<li>
<p>[32] P. Wu and J. Liu, &ldquo;Learning causal temporal relation and feature discrimination for anomaly detection,&rdquo; IEEE Transactions on Image Processing, vol. 30, pp. 3513–3527, 2021.</p>
</li>
<li>
<p>[33] C. Cao, X. Zhang, S. Zhang, P. Wang, and Y. Zhang, &ldquo;Adaptive graph convolutional networks for weakly supervised anomaly detection in videos,&rdquo; arXiv preprint arXiv:2202.06503, 2022.</p>
</li>
<li>
<p>[34] C. Huang, C. Liu, J. Wen, L. Wu, Y. Xu, Q. Jiang, and Y. Wang, &ldquo;Weakly supervised video anomaly detection via self-guided temporal discriminative transformer,&rdquo; IEEE Transactions on Cybernetics, 2022 .</p>
</li>
<li>
<p>[35] Y. Peng, X. Huang, and Y. Zhao, &ldquo;An overview of cross-media retrieval: Concepts, methodologies, benchmarks, and challenges,&rdquo; IEEE Transactions on Circuits and Systems for Video Technology, vol. 28, no. 9, pp. 2372–2385, 2017.</p>
</li>
<li>
<p>[36] Y. Peng, W. Zhu, Y. Zhao, C. Xu, Q. Huang, H. Lu, Q. Zheng, T. Huang, and W. Gao, &ldquo;Cross-media analysis and reasoning: advances and directions,&rdquo; Frontiers of Information Technology &amp; Electronic Engineering , vol. 18, no. 1, pp. 44–57, 2017.</p>
</li>
<li>
<p>[37] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu, &ldquo;Coca: Contrastive captioners are image-text foundation models,&rdquo; arXiv preprint arXiv:2205.01917, 2022.</p>
</li>
<li>
<p>[38] R. Zuo, X. Deng, K. Chen, Z. Zhang, Y.-K. Lai, F. Liu, C. Ma, H. Wang, Y.-J. Liu, and H. Wang, &ldquo;Fine-grained video retrieval with scene sketches,&rdquo; IEEE Transactions on Image Processing, 2023.</p>
</li>
<li>
<p>[39] M. Monfort, S. Jin, A. Liu, D. Harwath, R. Feris, J. Glass, and A. Oliva, &ldquo;Spoken moments: Learning joint audio-visual representations from video descriptions,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14 871–14 881.</p>
</li>
<li>
<p>[40] A.-M. Oncescu, A. Koepke, J. F. Henriques, Z. Akata, and S. Albanie, &ldquo;Audio retrieval with natural language queries,&rdquo; arXiv preprint arXiv:2105.02192, 2021.</p>
</li>
<li>
<p>[41] V. Gabeur, A. Nagrani, C. Sun, K. Alahari, and C. Schmid, &ldquo;Masking modalities for cross-modal video retrieval,&rdquo; arXiv preprint arXiv:2111.01300, 2021.</p>
</li>
<li>
<p>[42] A. Rouditchenko, A. Boggust, D. Harwath, S. Thomas, H. Kuehne, B. Chen, R. Panda, R. Feris, B. Kingsbury, M. Picheny et al., &ldquo;Cascaded multilingual audio-visual learning from videos,&rdquo; arXiv preprint arXiv:2111.04823, 2021.</p>
</li>
<li>
<p>[43] P. Morgado, N. Vasconcelos, and I. Misra, &ldquo;Audio-visual instance discrimination with cross-modal agreement,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 12 475–12 486.</p>
</li>
<li>
<p>[44] W. Shen, J. Song, X. Zhu, G. Li, and H. T. Shen, &ldquo;End-to-end pretraining with hierarchical matching and momentum contrast for textvideo retrieval,&rdquo; IEEE Transactions on Image Processing, 2023.</p>
</li>
<li>
<p>[45] Y. Tian, J. Shi, B. Li, Z. Duan, and C. Xu, &ldquo;Audio-visual event localization in unconstrained videos,&rdquo; in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 247–263 .</p>
</li>
<li>
<p>[46] Y. Wei, D. Hu, Y. Tian, and X. Li, &ldquo;Learning in audio-visual context: A review, analysis, and new perspective,&rdquo; arXiv preprint arXiv:2208.09579 , 2022 .</p>
</li>
<li>
<p>[47] Y. Wu, L. Zhu, Y. Yan, and Y. Yang, &ldquo;Dual attention matching for audiovisual event localization,&rdquo; in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 6292–6300 .</p>
</li>
<li>
<p>[48] Y.-B. Lin, Y.-L. Sung, J. Lei, M. Bansal, and G. Bertasius, &ldquo;Vision transformers are parameter-efficient audio-visual learners,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp. 2299–2309 .</p>
</li>
<li>
<p>[49] X. Li, C. Xu, G. Yang, Z. Chen, and J. Dong, &ldquo;W2vv++ fully deep learning for ad-hoc video search,&rdquo; in Proceedings of the 27th ACM International Conference on Multimedia, 2019, pp. 1786–1794.</p>
</li>
<li>
<p>[50] J. Dong, X. Li, C. Xu, X. Yang, G. Yang, X. Wang, and M. Wang, &ldquo;Dual encoding for video retrieval by text,&rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.</p>
</li>
<li>
<p>[51] S. Liu, H. Fan, S. Qian, Y. Chen, W. Ding, and Z. Wang, &ldquo;Hit: Hierarchical transformer with momentum contrast for video-text retrieval,&rdquo; arXiv preprint arXiv:2103.15049, 2021.</p>
</li>
<li>
<p>[52] M. Wray, D. Larlus, G. Csurka, and D. Damen, &ldquo;Fine-grained action retrieval through multiple parts-of-speech embeddings,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 450–459.</p>
</li>
<li>
<p>[53] P. Wu, X. He, M. Tang, Y. Lv, and J. Liu, &ldquo;Hanet: Hierarchical alignment networks for video-text retrieval,&rdquo; in Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 3518–3527.</p>
</li>
<li>
<p>[54] N. Han, J. Chen, G. Xiao, H. Zhang, Y. Zeng, and H. Chen, &ldquo;Finegrained cross-modal alignment network for text-video retrieval,&rdquo; in Proceedings of the 29th ACM International Conference on Multimedia , 2021, pp. 3826–3834.</p>
</li>
<li>
<p>[55] J. Yang, Y. Bisk, and J. Gao, &ldquo;Taco: Token-aware cascade contrastive learning for video-text alignment,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 11 562–11 572.</p>
</li>
<li>
<p>[56] W. Wang, M. Zhang, R. Chen, G. Cai, P. Zhou, P. Peng, X. Guo, J. Wu, and X. Sun, &ldquo;Dig into multi-modal cues for video retrieval with hierarchical alignment,&rdquo; in Proceedings of the International Joint Conference on Artificial Intelligence, 2021.</p>
</li>
<li>
<p>[57] Y. Ge, Y. Ge, X. Liu, D. Li, Y. Shan, X. Qie, and P. Luo, &ldquo;Bridging video-text retrieval with multiple choice questions,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022, pp. 16 167–16 176.</p>
</li>
<li>
<p>[58] Y. Ma, G. Xu, X. Sun, M. Yan, J. Zhang, and R. Ji, &ldquo;X-clip: Endto-end multi-grained contrastive learning for video-text retrieval,&rdquo; in Proceedings of the 30th ACM International Conference on Multimedia , 2022, pp. 638–647.</p>
</li>
<li>
<p>[59] L. Li, Y.-C. Chen, Y. Cheng, Z. Gan, L. Yu, and J. Liu, &ldquo;Hero: Hierarchical encoder for video+ language omni-representation pre-training,&rdquo; arXiv preprint arXiv:2005.00200, 2020.</p>
</li>
<li>
<p>[60] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, &ldquo;Less is more: Clipbert for video-and-language learning via sparse sampling,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 7331–7341.</p>
</li>
<li>
<p>[61] H. Luo, L. Ji, B. Shi, H. Huang, N. Duan, T. Li, J. Li, T. Bharti, and M. Zhou, &ldquo;Univl: A unified video and language pre-training model for multimodal understanding and generation,&rdquo; arXiv preprint arXiv:2002.06353, 2020.</p>
</li>
<li>
<p>[62] K. Ji, J. Liu, W. Hong, L. Zhong, J. Wang, J. Chen, and W. Chu, &ldquo;Cret: Cross-modal retrieval transformer for efficient text-video retrieval,&rdquo; in Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2022, pp. 949– 959.</p>
</li>
<li>
<p>[63] J. Dong, X. Chen, M. Zhang, X. Yang, S. Chen, X. Li, and X. Wang, &ldquo;Partially relevant video retrieval,&rdquo; in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 246–257.</p>
</li>
<li>
<p>[64] J. Gao, C. Sun, Z. Yang, and R. Nevatia, &ldquo;Tall: Temporal activity localization via language query,&rdquo; in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 5267–5275.</p>
</li>
<li>
<p>[65] D. Zhang, X. Dai, X. Wang, Y.-F. Wang, and L. S. Davis, &ldquo;Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 1247–1257.</p>
</li>
<li>
<p>[66] N. C. Mithun, S. Paul, and A. K. Roy-Chowdhury, &ldquo;Weakly supervised video moment retrieval from text queries,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 11 592–11 601.</p>
</li>
<li>
<p>[67] X. Ding, N. Wang, S. Zhang, Z. Huang, X. Li, M. Tang, T. Liu, and X. Gao, &ldquo;Exploring language hierarchy for video grounding,&rdquo; IEEE Transactions on Image Processing, vol. 31, pp. 4693–4706, 2022.</p>
</li>
<li>
<p>[68] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, &ldquo;Howto100m: Learning a text-video embedding by watching hundred million narrated video clips,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 2630–2640.</p>
</li>
<li>
<p>[69] M. Wray, H. Doughty, and D. Damen, &ldquo;On semantic similarity in video retrieval,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3650–3660.</p>
</li>
<li>
<p>[70] J. Xu, T. Mei, T. Yao, and Y. Rui, &ldquo;Msr-vtt: A large video description dataset for bridging video and language,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2016, pp. 5288–5296.</p>
</li>
<li>
<p>[71] X. Wang, J. Wu, J. Chen, L. Li, Y.-F. Wang, and W. Y. Wang, &ldquo;Vatex: A large-scale, high-quality multilingual dataset for video-and-language research,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 4581–4591.</p>
</li>
<li>
<p>[72] C. D. Kim, B. Kim, H. Lee, and G. Kim, &ldquo;Audiocaps: Generating captions for audios in the wild,&rdquo; in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019, pp. 119–132.</p>
</li>
<li>
<p>[73] Y. Tian, D. Li, and C. Xu, &ldquo;Unified multisensory perception: Weaklysupervised audio-visual video parsing,&rdquo; in Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16, 2020, pp. 436–454 .</p>
</li>
<li>
<p>[74] J. Lei, L. Yu, T. L. Berg, and M. Bansal, &ldquo;Tvr: A large-scale dataset for video-subtitle moment retrieval,&rdquo; in Proceedings of the 16th European Conference on Computer Vision. Springer, 2020, pp. 447–463.</p>
</li>
<li>
<p>[75] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Luciˇ ˇ c, and C. Schmid, ´ ´ &ldquo;Vivit: A video vision transformer,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6836–6846.</p>
</li>
<li>
<p>[76] D. Neimark, O. Bar, M. Zohar, and D. Asselmann, &ldquo;Video transformer network,&rdquo; in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 3163–3172.</p>
</li>
<li>
<p>[77] J. Carreira and A. Zisserman, &ldquo;Quo vadis, action recognition? a new model and the kinetics dataset,&rdquo; in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 6299–6308.</p>
</li>
<li>
<p>[78] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, &ldquo;Attention is all you need,&rdquo; in Advances in neural information processing systems, 2017, pp. 5998–6008.</p>
</li>
<li>
<p>[79] C. Huang, Y. Liu, Z. Zhang, C. Liu, J. Wen, Y. Xu, and Y. Wang, &ldquo;Hierarchical graph embedded pose regularity learning via spatio-temporal transformer for abnormal behavior detection,&rdquo; in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 307–315 .</p>
</li>
<li>
<p>[80] S. Zhao, L. Zhu, X. Wang, and Y. Yang, &ldquo;Centerclip: Token clustering for efficient text-video retrieval,&rdquo; in Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2022, pp. 970–981 .</p>
</li>
<li>
<p>[81] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, &ldquo;Audio set: An ontology and humanlabeled dataset for audio events,&rdquo; in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2017, pp. 776–780.</p>
</li>
<li>
<p>[82] K. Wu, J. Liu, X. Hao, P. Liu, and F. Shen, &ldquo;An evolutionary multiobjective framework for complex network reconstruction using community structure,&rdquo; IEEE Transactions on Evolutionary Computation, vol. 25, no. 2, pp. 247–261, 2020.</p>
</li>
<li>
<p>[83] Y. Jin, H. Wang, T. Chugh, D. Guo, and K. Miettinen, &ldquo;Data-driven evolutionary optimization: An overview and case studies,&rdquo; IEEE Transactions on Evolutionary Computation, vol. 23, no. 3, pp. 442–458, 2018.</p>
</li>
<li>
<p>[84] T. Back, Evolutionary algorithms in theory and practice: evolution strategies, evolutionary programming, genetic algorithms. Oxford university press, 1996.</p>
</li>
<li>
<p>[85] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, &ldquo;Unsupervised feature learning via non-parametric instance discrimination,&rdquo; in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 3733–3742.</p>
</li>
<li>
<p>[86] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, &ldquo;Masked au- ´ ´ toencoders are scalable vision learners,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 000–16 009.</p>
</li>
<li>
<p>[87] S. Paul, S. Roy, and A. K. Roy-Chowdhury, &ldquo;W-talc: Weakly-supervised temporal activity localization and classification,&rdquo; in Proceedings of the European Conference on Computer Vision, 2018, pp. 563–579.</p>
</li>
<li>
<p>[88] D. P. Kingma and J. Ba, &ldquo;Adam: A method for stochastic optimization,&rdquo; arXiv preprint arXiv:1412.6980, 2014.</p>
</li>
<li>
<p>[89] X. Wang, L. Zhu, and Y. Yang, &ldquo;T2vlad: global-local sequence alignment for text-video retrieval,&rdquo; in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 5079–5088.</p>
</li>
<li>
<p>[90] T. Mikolov, K. Chen, G. Corrado, and J. Dean, &ldquo;Efficient estimation of word representations in vector space,&rdquo; arXiv preprint arXiv:1301.3781 , 2013.</p>
</li>
</ul>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/Toward Video Anomaly Retrieval From Video.md"
          data-oid-likes="likes_papers/Toward Video Anomaly Retrieval From Video.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/towards-generic-anomaly-detection-and-understanding/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/text-prompt-with-normality-guidance-for-weakly-supervised-video-anomaly-detection/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
