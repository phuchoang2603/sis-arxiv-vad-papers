<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/visiongpt-llm-assisted-real-time-anomaly-detection-for-safe-visual-navigation/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/visiongpt-llm-assisted-real-time-anomaly-detection-for-safe-visual-navigation/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/visiongpt-llm-assisted-real-time-anomaly-detection-for-safe-visual-navigation\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "7867"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>7867 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">37 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">VISIONGPT: LLM-ASSISTED REAL-TIME ANOMALY DETECTION FOR SAFE VISUAL NAVIGATION
    <div id="visiongpt-llm-assisted-real-time-anomaly-detection-for-safe-visual-navigation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#visiongpt-llm-assisted-real-time-anomaly-detection-for-safe-visual-navigation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Hao Wang</p>
<p>School of Computing Clemson University Clemson, SC, USA <a
  href="mailto:hao9@g.clemson.edu">hao9@g.clemson.edu</a></p>
<p>Ashish Bastola School of Computing Clemson University Clemson, SC, USA <a
  href="mailto:abastol@g.clemson.edu">abastol@g.clemson.edu</a></p>

<h2 class="relative group">Jiayou Qin
    <div id="jiayou-qin" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#jiayou-qin" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Department of Electrical and Computer Engineering Stevens Institute of Technology Hoboken, NJ, USA <a
  href="mailto:jqin6@stevens.edu">jqin6@stevens.edu</a></p>
<p>Xiwen Chen School of Computing Clemson University Clemson, SC, USA</p>
<p><a
  href="mailto:xiwenc@g.clemson.edu">xiwenc@g.clemson.edu</a></p>

<h2 class="relative group">John Suchanek
    <div id="john-suchanek" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#john-suchanek" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>School of Computing Clemson University Clemson, SC, USA</p>
<p><a
  href="mailto:jsuchan@g.clemson.edu">jsuchan@g.clemson.edu</a></p>

<h2 class="relative group">Zihao Gong
    <div id="zihao-gong" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#zihao-gong" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>School of Cultural and Social Studies Tokai University Tokyo, Japan <a
  href="mailto:0CPD1206@mail.u-tokai.ac.jp">0CPD1206@mail.u-tokai.ac.jp</a></p>
<p>Abolfazl Razi School of Computing Clemson University Clemson, SC, USA <a
  href="mailto:arazi@clemson.edu">arazi@clemson.edu</a></p>

<h2 class="relative group">ABSTRACT
    <div id="abstract" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#abstract" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This paper explores the potential of Large Language Models(LLMs) in zero-shot anomaly detection for safe visual navigation. With the assistance of the state-of-the-art real-time open-world object detection model Yolo-World and specialized prompts, the proposed framework can identify anomalies within camera-captured frames that include any possible obstacles, then generate concise, audio-delivered descriptions emphasizing abnormalities, assist in safe visual navigation in complex circumstances. Moreover, our proposed framework leverages the advantages of LLMs and the open-vocabulary object detection model to achieve the dynamic scenario switch, which allows users to transition smoothly from scene to scene, which addresses the limitation of traditional visual navigation. Furthermore, this paper explored the performance contribution of different prompt components, provided the vision for future improvement in visual accessibility, and paved the way for LLMs in video anomaly detection and vision-language understanding.</p>
<p>Keywords Open World Object Detection · Anomaly Detection · Large Language Model · Vision-language Understanding · Prompt Engineering · Generative AI · GPT</p>

<h2 class="relative group">1 Introduction
    <div id="1-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Accessible technologies have seen remarkable development in recent years due to the rise of machine learning and mobile computing [1–5]. Deep learning has significantly enhanced the accuracy and speed of object detection and segmentation models [5–9], which catalyzed a surge of real-world applications, impacting numerous aspects of daily life, industry, and transportation. Visual navigation has benefited significantly from the evolution of such computer vision techniques [5, 10, 11].</p>
<p>Consequently, innovations such as Augmented Reality (AR) have been instrumental in enhancing the safety and mobility of individuals across various scenarios, including driving and walking. Many of these technologies aim to bridge the</p>
<p>gap between the physical world and digital assistance, highlighting the critical need for adaptive solutions to navigate the complexities of real-world environments.</p>
<p>However, visual navigation presents significant challenges in dynamic urban environments [11–13]. Although the newborn zero-shot object detection [14] addresses the significant limitations of classical object detection models such as YOLOv8 [15, 16] in complex scenarios, it encounters difficulties in developing custom class labels for dynamic environments due to the long-tail response. Furthermore, real-time vision-language understanding can be critical for complex scenarios for safety concerns, especially for visually impaired individuals who must traverse streets, sidewalks, and other public spaces.</p>
<p>Vision-language understanding has recently become a hotspot due to the emergence of Multimodal Large Language Models (LLMs) [17]. Multimodal LLMs represent an evolutionary leap in the field of artificial intelligence as they integrate the processing of text, images, and even audio and video [18] to create a comprehensive understanding of the world that mirrors human cognitive abilities more closely than ever before, making it possible to handle more advances tasks for robotics [19, 20]. Specifically, GPT-4V is now being heavily used in image tasks such as data evaluation [21, 22], medical image diagnosis [23–25], and content creation [26–28].</p>
<p>Figure 1: Framework for vision-language processing and prompting.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_aa050c186e2c0b0cd0f9b2083b687e5dbb817beab7fd69d0d27572446fca11ef.png"
    ></figure>
<p>Multimodal LLMs possess substantial improvement in interpreting, analyzing, and generating content across different modalities [29], bringing the possibility to interdisciplinary applications [30, 31]. Interestingly, LLMs also exhibit impressive zero-shot and few-shot learning abilities, potentially enabling them to capture visual concepts with minimal training data. This opens a way to address object detection challenges, particularly in data with limited annotation [32]. Recent research attempts to bring LLMs to the accessibility field, yet most work only focuses on basic natural language processing such as text reading, image recognition, and voice assistance [33].</p>
<p>Therefore, a critical gap exists when using the vision-language understanding of LLMs for safety and accessible applications. Despite past works that investigated the use of LLMs in visual assistance [34] and visual navigation [35–37], only a few focused on the safety aspects [32, 38] but barely considered the induced latency during the inference.</p>
<p>Our research introduces a framework that combines the speed of locally executed open-world object detection with the intelligence of LLMs to create a universal anomaly detection system. The primary goal of this system is to deliver real-time, personalized scene descriptions and safety notifications, ensuring the safety and ease of navigation for visually impaired users by identifying and alerting them to potential obstacles and hazards in their path, where these obstacles and hazards can be considered &ldquo;anomalies&rdquo; in the context of a safe and clear path for navigation. The proposed framework can also be applied to robotic systems, augmented reality platforms, and all other mobile computing edge units.</p>
<p>Particularly, the major contributions of this paper are summarized as follows:</p>
<ul>
<li>Zero-shot anomaly detection: The proposed integration is train-free and ready for video anomaly detection and annotation with different response preferences.</li>
<li>Real-time feedback: Our framework is optimized for real-time response in complex scenarios with very low latency.</li>
<li>Dynamic scene transition and interest setting: This framework can dynamically switch the object detection classes based on the user&rsquo;s needs. Furthermore, users can interact with the LLM module and setting a prior task (e.g., find the nearest bench).</li>
</ul>

<h2 class="relative group">2 Related Work
    <div id="2-related-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-related-work" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">2.1 Open-vocabulary object detection
    <div id="21-open-vocabulary-object-detection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#21-open-vocabulary-object-detection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Open-vocabulary object detection (OVD) [39] represents a significant shift in object detection focusing on identifying items outside predefined categories. Initial efforts [40] trained on known classes for evaluating the detection of novel objects facing generalization and adaptability issues due to limited datasets and vocabularies. Recent approaches, however, [41 – 43] employ image-text matching with extensive data to expand the training vocabularies inspired by vision language pre-training [29, 44]. OWL-ViTs [45] and GLIP [46] utilize vision transformers and phrase grounding for effective OVD while Grounding DINO [47] combines these with detection transformers for crossmodality fusion. Despite the promise, existing methods often rely on complex detectors increasing computational demands significantly [48,49]. ZSD-YOLO [50] also explored an open-vocabulary detection with YOLO using language model alignment; however, YOLO-world [51] presents a more efficient and real-time OVD solution aiming to be much more efficient with real-time inference using an effective pre-training strategy while still being highly generalizable.</p>

<h2 class="relative group">2.2 Prompt Engineering
    <div id="22-prompt-engineering" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#22-prompt-engineering" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Prompt engineering has emerged as a critical technique for unlocking the capabilities of large language models (LLMs) [52 – 55] to various applications without finetuning on large datasets. This involves carefully crafting text prompts, instructions, or examples to guide LLM behavior and elicit desired responses. Researchers are actively exploring prompt engineering using various prompting techniques such as zero-shot prompting [56], Few-shot prompting [57], Chain-ofthought prompting [58], self-ask prompting [59], etc. to fine-tune LLMs for various tasks, demonstrating significant performance gains compared to traditional model training approaches. Studies have showcased how prompt engineering can adapt LLMs for diverse natural language tasks like question-answering [60], smart-reply [55], summarization [61], and text classification [62, 63]. Furthermore, researchers are increasingly developing frameworks to systematize prompt engineering efforts. Such frameworks aim to simplify the creation of effective prompts and facilitate the adaptation of LLMs to specific domains and applications and are highly customizable to user needs. While prompt engineering has seen significant improvements in natural language processing, its potential in computer vision on accessibility remains less explored. Our work builds upon the success of prompt engineering in NLP, exploring its application in the visual domain to enhance object detection and description.</p>

<h2 class="relative group">2.3 Accessible Technology
    <div id="23-accessible-technology" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#23-accessible-technology" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Computer vision-driven accessible technologies are mostly designed to empower individuals with visual impairments through enhanced scene understanding and hazard detection. A range of solutions exist, including mobile apps that provide object recognition and audio descriptions of surroundings [64–66], to wearable systems that offer real-time alerts about obstacles or potential dangers [67, 68]. For example, technologies that detect approaching vehicles and crosswalk signals significantly improve the safety of visually impaired pedestrians in urban environments. Moreover, computer vision is integrated into assistive technologies for reading text aloud from documents and identifying objects in daily life, enabling greater independence [5]. Research in this domain also focuses on indoor navigation, where object detection and spatial mapping can guide users within buildings and public spaces [69]. The core emphasis of these computer vision-powered accessibility technologies aims to enhance safety. By providing real-time information on key elements within an individual&rsquo;s surroundings, the risk of accidents and injuries is significantly reduced. Identifying potential hazards, such as oncoming traffic, obstacles on sidewalks, or unattended objects, allows visually impaired individuals to navigate with greater confidence and autonomy.</p>

<h2 class="relative group">3 Methodology
    <div id="3-methodology" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-methodology" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Our system offers real-time anomaly alerts by integrating object detection with large language model capabilities, featuring a multi-module architecture. The system operates continuously with the object detection module processing</p>
<p>real-time camera frames. Multi-frame object information is then included in specially engineered prompts and submitted to the LLM module. The system then processes the LLM&rsquo;s response, classifying potential anomalies. Finally, the LLM module conveys important alerts and essential scene descriptions to the user.</p>
<p>The proposed project is fully open-sourced and available at: <a
  href="https://github.com/AIS-Clemson/VisionGPT"
    target="_blank"
  >https://github.com/AIS-Clemson/VisionGPT</a></p>

<h2 class="relative group">3.1 Object Detection Module
    <div id="31-object-detection-module" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#31-object-detection-module" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To ensure real-time performance on mobile devices, we employ lightweight yet powerful object detection models for real-time detection. Specifically, we applied the state-of-the-art YOLO-World model for open-vocabulary detection whose detection classes are customizable for a wide range of scenarios. As we focus on accessible visual navigation, we prompt the proposed LLM module to personalize the detection classes relevant to safe navigation in daily use circumstances, including pedestrians, vehicles, bicycles, traffic signals, and any potential road hazards or obstacles. Therefore, our proposed multi-functional prompt manager allows users to switch detection classes dynamically.</p>

<h2 class="relative group">3.2 Detection Class Manager
    <div id="32-detection-class-manager" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#32-detection-class-manager" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This sub-module aims to create a detailed categorization for object detection algorithms, enabling them to identify and distinguish potential obstacles, hazards, and useful landmarks. This approach ensures the detection system is finely tuned to urban navigation&rsquo;s specific needs and challenges, enhancing the user&rsquo;s ability to move safely and independently through city streets. By focusing on road hazards and obstacles, the updated list aims to provide a more relevant and focused set of detection classes for the &lsquo;urban walking&rsquo; context, optimizing the system&rsquo;s utility and effectiveness for the visually impaired user.</p>
<p>As shown in Figure 1, the user can interact with the LLM (advance) module. Based on this operation logic, the user can ask to change the object detection classes based on scenarios. For instance, if a user experiences a scene transition from sidewalk to park, the detection classes specialized for sidewalk objects (e.g., car, road cone, traffic signal, etc.) can be replaced by new object classes that are more relevant to the park scene to adapt to the situation.</p>
<p>Original prompt: &ldquo;The user is switching the scene to custom_scene please generate a new list that contains the top 100 related objects, including especially road hazards and possible obstacles&rdquo;</p>

<h2 class="relative group">3.3 Anomaly Handle Module
    <div id="33-anomaly-handle-module" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#33-anomaly-handle-module" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The proposed anomaly detection system aims to enhance navigation safety and awareness in various environments, particularly for visually impaired individuals and others requiring navigation assistance (e.g., robotics systems). The system analyzes real-time imagery captured from a camera and splits the image into four distinct regions based on an &lsquo;H&rsquo; pattern.</p>
<p>Figure 2: Type H image splitter. (1) and (2) represent the left and right area, (3) represent the ground area, and (4) represent the front area.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_5a60d5a4092ee116eb08597a250185020c2c8483346015f76bde881ff9ad799c.png"
    ></figure>
<p>Specifically, The system categorizes detected objects into four types based on their location within the image, each corresponding to a specific splitting part of the &lsquo;H&rsquo; pattern segmentation: Left , Right , Front, and Ground. This categorization helps identify and respond to potential hazards more effectively. As shown in Figure 2.</p>
<p>Moreover, we find that:</p>
<ul>
<li>Left and Right: These regions cover the left and right 25% of the image, respectively. Objects detected here are typically in motion and may occupy much of the visual field. This area is crucial for identifying moving hazards such as vehicles or cyclists that may approach the user from the sides.</li>
<li>Front: This region focuses on the center 50% of the image&rsquo;s width and the upper half vertically. It captures objects still at a distance but directly ahead of the user. Identifying objects in this region is necessary for assessing the overall situation and planning movements, especially in detecting upcoming objects at high speed such as cars and cyclists.</li>
<li>Ground: Occupying the center 50% of the image&rsquo;s width and the lower half vertically, this area highlights objects nearby on the ground. Immediate attention to detections in this area is critical for avoiding hazards that require cautious navigation, such as cracks, puddles, or uneven surfaces.</li>
</ul>
<p>The system then records detailed information for each object, including classification, size, and position. All size and position data have been converted into percentage expressions for a better interpretation by LLM. Finally, by analyzing objects&rsquo; locations and sizes, alerts for anomalies are generated for objects that appear on the &lsquo;ground&rsquo; area or occupy significant space (&gt; 10% in this study) in the &rsquo;left&rsquo; or &lsquo;right&rsquo; regions. The detection and movement information is then post-processed into a structured format, supporting LLM for better understanding.</p>
<p>Original prompt: &ldquo;The location information center x , center y , height, width of objects is the proportion to the image, the detected objects are categorized into 4 type based on the image region. Left and Right: objects located on left 25% or right 25% of the image, these objects are usually moving and has large proportion.Front: objects that may still far away, can be used to discriminate the current situation.Ground: objects that may nearby.&rdquo;</p>

<h2 class="relative group">3.4 Data Collection
    <div id="34-data-collection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#34-data-collection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Even though various datasets exist for static images [70] and CCTV camera feeds [71–73], no extensive datasets are available for detecting large anomalies in visual navigation from the first person&rsquo;s perspective.</p>
<p>Thus, we collected 50 video clips of point-of-view cruising in various scenarios. These custom videos are filmed in public spaces with first-person view and continuous forward-moving. Table ?? shows the details of the collected data.</p>
<p>Table 1: Collected data for video anomaly detection.</p>
<table>
  <thead>
      <tr>
          <th>Location</th>
          <th>Scene</th>
          <th>Movement</th>
          <th>Weather</th>
          <th>Clips</th>
          <th>Total length</th>
          <th>Unique Classes</th>
          <th>Total detected objects</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Urban</td>
          <td>Sidewalk</td>
          <td>Scooter</td>
          <td>Cloudy</td>
          <td>8</td>
          <td>10 mins</td>
          <td>31</td>
          <td>16944</td>
      </tr>
      <tr>
          <td>Suburban</td>
          <td>Bikeline</td>
          <td>Scooter</td>
          <td>Cloudy</td>
          <td>5</td>
          <td>6 mins</td>
          <td>26</td>
          <td>8394</td>
      </tr>
      <tr>
          <td>Urban</td>
          <td>Park</td>
          <td>Scooter</td>
          <td>Cloudy</td>
          <td>6</td>
          <td>5 mins</td>
          <td>23</td>
          <td>15310</td>
      </tr>
      <tr>
          <td>City</td>
          <td>Road</td>
          <td>Biking</td>
          <td>Sunny</td>
          <td>5</td>
          <td>5 mins</td>
          <td>21</td>
          <td>5464</td>
      </tr>
      <tr>
          <td>City</td>
          <td>Sidewalk</td>
          <td>Biking</td>
          <td>Sunny</td>
          <td>7</td>
          <td>6 mins</td>
          <td>27</td>
          <td>9569</td>
      </tr>
      <tr>
          <td>City</td>
          <td>Park</td>
          <td>Biking</td>
          <td>Cloudy</td>
          <td>5</td>
          <td>5 mins</td>
          <td>19</td>
          <td>4781</td>
      </tr>
      <tr>
          <td>Town</td>
          <td>Park</td>
          <td>Walking</td>
          <td>Cloudy</td>
          <td>6</td>
          <td>4 mins</td>
          <td>18</td>
          <td>5156</td>
      </tr>
      <tr>
          <td>Town</td>
          <td>Sidewalk</td>
          <td>Walking</td>
          <td>Sunny</td>
          <td>8</td>
          <td>7 mins</td>
          <td>14</td>
          <td>8274</td>
      </tr>
      <tr>
          <td>City</td>
          <td>Coast</td>
          <td>Walking</td>
          <td>Sunny</td>
          <td>2</td>
          <td>5 mins</td>
          <td>37</td>
          <td>29280</td>
      </tr>
      <tr>
          <td>Suburban</td>
          <td>Theme Park</td>
          <td>Walking</td>
          <td>Rain</td>
          <td>3</td>
          <td>6 mins</td>
          <td>34</td>
          <td>24180</td>
      </tr>
  </tbody>
</table>
<p>We then conducted the experiments by combining the open-vocabulary object detection model with our novel imagesplitting method to annotate the frames as anomalies.</p>
<p>Specifically, a frame is labeled as an anomaly if it meets either of the following criteria:</p>
<ol>
<li>Objects are detected within the Ground area.</li>
<li>Objects appear in either the Left or the Right areas of the image and occupy more than 10% of the total image area.</li>
</ol>
<p>We set this rule-based method as the baseline of anomaly detection in this study, as our captured video clips are customized for this H-splitting principle.</p>

<h2 class="relative group">3.5 LLM Module
    <div id="35-llm-module" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#35-llm-module" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This module processes the detected object information and passing to the LLM. Specifically, we use both GPT-3.5 and GPT-4 to process the information. First, GPT-3.5 is mainly used for low-level information processes such as</p>
<p>object detection data analysis, data format converting, and prompt reasoning. Therefore, GPT-4 is used for a high-level command instance understanding and a comprehensive vision-language understanding.</p>
<p>The system sensitivity settings indicate a focus on identifying and reporting hazards based on their potential impact on the user&rsquo;s safety and navigation. The system&rsquo;s goal to report objects based on their level of inconvenience or danger aligns with the anomaly detection objective of identifying and reacting to deviations that matter most in the given context. Note that the system sensitivity in the context is distinct from the model sensitivity as a statistical term.</p>
<p>These prompts sketch the conceptual framework and operational guidelines for a voice-assisted navigation system for visual accessibility. The system utilizes data from a phone camera, which is always facing forward, to detect objects and categorize their location within the field of view. Based on these analyses, the system provides auditory feedback to users, helping them navigate their environment safely and avoid potential hazards. Furthermore, the annotated data can be used for the training of other anomaly detection models.</p>
<p>The main LLM prompts consist of:</p>
<ul>
<li>Prompt instruction: &ldquo;You are a voice assistant for a visually impaired user, the input is the actual data collected by a phone camera, and the phone is always facing front, please provide the key information for the blind user to help him navigate and avoid potential danger. Please note that the center_x and center_y represent the object location (proportional to the image), object height and width are also a proportion.&rdquo;</li>
<li>Prompt sensitivity: &ldquo;System sensitivity: Incorporate the sensitivity setting in your response. For a lowsensitivity setting, identify and report only imminent and direct threats to safety. For medium sensitivity, include potential hazards that could pose a risk if not avoided. For high sensitivity, report all detected objects that could cause any inconvenience or danger. Current sensitivity: low.&rdquo;</li>
</ul>

<h2 class="relative group">4 Experiments
    <div id="4-experiments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#4-experiments" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We compare our proposed vision-LLM system with the rule-based anomaly detection (baseline) to show its performance and reliability.</p>

<h2 class="relative group">4.1 System Optimization
    <div id="41-system-optimization" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#41-system-optimization" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>While the proposed system is running, we input captured images into the object detection model every 5 frames to boost the FPS (Frame-Per-Second), this can significantly improve the performance, especially for mobile devices that have limited computation resources. Then, we send the detected information to the anomaly handle module to label the frames as the baseline. With the frame compensation, the real-time detection performance is boosted from 16 FPS to 73 FPS, as shown in Table 4.</p>
<p>Meanwhile, we apply the LLM module to process the detected information every 30 frames in parallel due to the latency of LLMs. To optimize the latency for better performance, the proposed system uses the GPT 3.5 Turbo model as the core of the LLM module.</p>

<h2 class="relative group">4.2 Detection Accuracy
    <div id="42-detection-accuracy" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#42-detection-accuracy" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>By setting the rule-based detector as the baseline, this study aims to test the zero-shot learning capability of the LLM detector, and meanwhile, interpret which prompt may impact the performance significantly.</p>
<p>After comparing the annotation results of prompt-based anomaly detection with the rule-based anomaly detection on our collected data, we find that prompt-based anomaly detection achieves high precision with all prompt modules working properly. Specifically, we compared the LLM anomaly detection with different sensitivity settings: low, normal, and high. As shown in Figure 3, the Receiver Operating Characteristic (ROC) curve indicates that a low system sensitivity leads to better performance, as it is less sensitive than the rule-based detector. For instance, objects detected by the rule-based detector with low confidence and classes of low risk will be filtered by LLM due to no emergency. Conversely, the higher the system sensitivity, the worse the performance, as the system tends to categorize all possible anomalies as immediate emergencies.</p>
<p>As shown in Figure 4, the LLM anomaly detector with low-system sensitivity captures more True Positive and True Negative cases and tries to minimize the False Positive rate.</p>
<p>Figure 3: ROC curve.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_3c8cfc34f084872bf66b41c35ee9bf84ec4ee01b067d81d78539c2a5524ea86e.png"
    ></figure>

<h2 class="relative group">4.3 Quality Evaluation
    <div id="43-quality-evaluation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#43-quality-evaluation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We picked one of the video clips to analyze the detection difference between the rule-based detector and the LLM anomaly detector. Specifically, in Figure 5, the first row shows the anomalies labeled by the rule-based detector, while the second row indicates the anomalies predicted by the LLM-based detector (low sensitivity setting). As shown in Figure 5, the proposed LLM detector has less acuity with a low-sensitivity prompt setting, which tends to filter anomalies that are non-emergency. Table 2 shows the selected sample output of the LLM module.</p>
<p>Figure 5: Anomaly annotation. The first row represents the labeled anomalies by the rule-based detector (binary), and the second row represents the anomalies predicted by the proposed LLM detector (float). Color represents the probability of anomalies.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_3ea8c6c2b75119ac247fe62346361465c5c9ffbd06bf4dd6be6da610be206b0b.png"
    ></figure>
<p>Figure 4: Confusion matrix of total frames. LLM setting is low-system sensitivity setting.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000004_d7d3e8ede793115f7128a8f2ff8370f056a70764eb4b28c1d3aeeae3f255ced2.png"
    ></figure>
<p>Table 2: Selected feedback and caption of the output of the LLM module. Frane ID indicates the video frame index, the Anomaly Index represents the predicted anomalies of LLM, and Reason represents the response message from LLM for anomaly interpretation.</p>
<table>
  <thead>
      <tr>
          <th>Frame ID</th>
          <th>Anomaly Index</th>
          <th>Reason</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>2290</td>
          <td>0.85</td>
          <td>’Car and people nearby.’</td>
      </tr>
      <tr>
          <td>3770</td>
          <td>0.2</td>
          <td>’Green traffic light detected.’</td>
      </tr>
      <tr>
          <td>4430</td>
          <td>0.5</td>
          <td>’Obstacles in path.’</td>
      </tr>
      <tr>
          <td>5450</td>
          <td>0.7</td>
          <td>’Car on the left’</td>
      </tr>
      <tr>
          <td>7000</td>
          <td>0</td>
          <td>’No immediate danger.’</td>
      </tr>
      <tr>
          <td>8230</td>
          <td>1</td>
          <td>’Bike in close proximity’</td>
      </tr>
      <tr>
          <td>9800</td>
          <td>1</td>
          <td>High risk of collision with multiple people</td>
      </tr>
  </tbody>
</table>

<h2 class="relative group">4.4 Ablation Study
    <div id="44-ablation-study" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#44-ablation-study" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To explore the contribution of different prompt modules, we conducted the ablation study of each module. Table 3 shows that the proposed system performs worse without specific prompt modules. For instance, while the instruction prompt is missing, the system may generate random content due to the confusion of the current task and lack of instruction. Moreover, missing region information of detected objects may also weaken the performance, as the system cannot evaluate the priority of the emergency.</p>
<p>Table 3: Ablation study. ✓indicates incorporated modules of system, and ✗indicate missing modules</p>
<table>
  <thead>
      <tr>
          <th>Sensitivity</th>
          <th>Location</th>
          <th>Instruction</th>
          <th>AP</th>
          <th>AUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Low</td>
          <td>✓</td>
          <td>✓</td>
          <td>88.01</td>
          <td>87.26</td>
      </tr>
      <tr>
          <td>Normal</td>
          <td>✓</td>
          <td></td>
          <td>82.73</td>
          <td>81.17</td>
      </tr>
      <tr>
          <td>High</td>
          <td>✓</td>
          <td>✓</td>
          <td>72.35</td>
          <td>77.29</td>
      </tr>
      <tr>
          <td>g Low</td>
          <td>✓</td>
          <td>✗</td>
          <td>68.84</td>
          <td>73.64</td>
      </tr>
      <tr>
          <td>Low</td>
          <td>✗</td>
          <td>✓</td>
          <td>69.57</td>
          <td>75.56</td>
      </tr>
      <tr>
          <td>✗</td>
          <td>✓</td>
          <td>✓</td>
          <td>69.16</td>
          <td>80.39</td>
      </tr>
  </tbody>
</table>
<p>Moreover, we find that LLM produced different performances with different sensitivity prompts. Unexpectedly, Low system sensitivity appears higher accuracy and precision, as the system tries to catch True Positive cases as much as possible and avoid false alarms. This is significant for visually impaired navigation, as the user can efficiently avoid misinformation and frequent-unnecessary alerts.</p>

<h2 class="relative group">4.5 Performance Evaluation
    <div id="45-performance-evaluation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#45-performance-evaluation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We further explore the performance efficiency of the proposed system on multiple platforms to reveal its potential for other applications.</p>
<p>Latency: As shown in Table 4, we measured end-to-end system latency and individual module processing times to identify bottlenecks and optimize for real-time performance. Results indicated an average end-to-end latency of 60 ms on the mobile device (e.g., smartphone) with neural engines, ensuring timely feedback.</p>
<table>
  <thead>
      <tr>
          <th>Task</th>
          <th>Model</th>
          <th>Framework</th>
          <th>Chipset</th>
          <th>Architecture</th>
          <th>FPS</th>
          <th>Latency</th>
          <th>w/ Frame Compensation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Detection</td>
          <td>Yolo-v8l</td>
          <td>Pytorch</td>
          <td>V100</td>
          <td>GPU</td>
          <td>22.01</td>
          <td>45 ms</td>
          <td>102.56</td>
      </tr>
      <tr>
          <td>Detection</td>
          <td>Yolo-v8x</td>
          <td>Pytorch</td>
          <td>V100</td>
          <td>GPU</td>
          <td>14.22</td>
          <td>71 ms</td>
          <td>70.11</td>
      </tr>
      <tr>
          <td>Segmentation</td>
          <td>Yolo-v8x-seg</td>
          <td>Pytorch</td>
          <td>V100</td>
          <td>GPU</td>
          <td>12.06</td>
          <td>83 ms</td>
          <td>59.68</td>
      </tr>
      <tr>
          <td>Detection</td>
          <td>Yolov8-World</td>
          <td>Pytorch</td>
          <td>V100</td>
          <td>GPU</td>
          <td>20.12</td>
          <td>50 ms</td>
          <td>98.06</td>
      </tr>
      <tr>
          <td>Detection</td>
          <td>Yolov8x-World-v2</td>
          <td>Pytorch</td>
          <td>V100</td>
          <td>GPU</td>
          <td>16.74</td>
          <td>62 ms</td>
          <td>76.88</td>
      </tr>
      <tr>
          <td>Detection</td>
          <td>Yolov8x-World-v2</td>
          <td>CoreML</td>
          <td>M2</td>
          <td>CPU</td>
          <td>5.01</td>
          <td>199 ms</td>
          <td>N/A</td>
      </tr>
      <tr>
          <td>Detection</td>
          <td>Yolov8x-World-v2</td>
          <td>CoreML</td>
          <td>M2</td>
          <td>Neural Engine</td>
          <td>19.6</td>
          <td>51 ms</td>
          <td>N/A</td>
      </tr>
      <tr>
          <td>Detection</td>
          <td>Yolov8x-World-v2</td>
          <td>CoreML</td>
          <td>A16-Bionic</td>
          <td>CPU</td>
          <td>1.26</td>
          <td>789 ms</td>
          <td>N/A</td>
      </tr>
      <tr>
          <td>Detection</td>
          <td>Yolov8x-World-v2</td>
          <td>CoreML</td>
          <td>A16-Bionic</td>
          <td>Neural Engine</td>
          <td>16.24</td>
          <td>61 ms</td>
          <td>N/A</td>
      </tr>
  </tbody>
</table>
<p>Table 4: Object Detector Test on multiple platforms. A16-Bionic processors are used in iPhone 14 pro max, and M2 processors are widely used in Vision Pro and the latest Mac models. The PyTorch-based implementation was run on NVIDIA GPU.</p>
<p>Economy: We further investigated the system latency and token consumption for economy evaluation. We designed three different modes for users to choose from:</p>
<ul>
<li>Voice only: only output voice messages for emergency response, minimum latency.</li>
<li>Annotation: output both anomaly index and reason for system testing and practical annotation.</li>
<li>Full: output full information in a structured JSON format. (See the original prompt for more information)</li>
</ul>
<p>Table 5: Economy and latency test.</p>
<table>
  <thead>
      <tr>
          <th>Mode</th>
          <th>LLM</th>
          <th>Latency</th>
          <th>Completion Token</th>
          <th>Prompt Tokens</th>
          <th>Total Tokens</th>
          <th>Charge (USD/day)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Voice only</td>
          <td>GPT3.5</td>
          <td>407</td>
          <td>35</td>
          <td>573</td>
          <td>608</td>
          <td>2.44</td>
      </tr>
      <tr>
          <td>y  Annotation</td>
          <td>GPT3.5</td>
          <td>628</td>
          <td>48</td>
          <td>573</td>
          <td>617</td>
          <td>2.58</td>
      </tr>
      <tr>
          <td>Full</td>
          <td>GPT3.5</td>
          <td>1818</td>
          <td>176</td>
          <td>1195</td>
          <td>1371</td>
          <td>13.53</td>
      </tr>
  </tbody>
</table>
<p>As shown in Table 5, we estimated the cost of our system with different modes. The prices are calculated with an average of 2 hours of daily usage and are based on the chatGPT API pricing policy.</p>
<p>Original prompt:</p>
<ul>
<li>Prompt_format_full: &lsquo;Please organize your output into this format: &ldquo;scene&rdquo;: quickly describe the current situation for blind user; &ldquo;key_objects&rdquo;: quickly and roughly locate the key objects for blind user; &ldquo;anomaly_checker&rdquo;: quickly diagnose if there is potential danger for a blind person; &ldquo;anomaly_label&rdquo;: output 1 if there is an emergency, output 0 if not; &ldquo;anomaly_index&rdquo;: object_id, danger_index, estimate a score from 0 to 1 about each objects that may cause danger; &ldquo;voice_guide&rdquo;: the main output to instant alert the blind person for emergency.&rsquo;</li>
<li>Prompt_format_voice: &lsquo;Please organize your output into this format: &ldquo;voice_guide&rdquo;: the main output to instantly alert the blind person for an emergency.&rsquo;</li>
<li>Prompt_format_annotation: &lsquo;Please organize your output into this format: &ldquo;anomaly_score&rdquo;: predict a score from 0 to 1 to evaluate the emergency level; &ldquo;reason&rdquo;: explain your annotation reason within 10 words.&rsquo;</li>
</ul>

<h2 class="relative group">5 Conclusion
    <div id="5-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#5-conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This research demonstrates the significant potential of combining lightweight mobile object detection with large language models to enhance accessibility for visually impaired individuals. Our system successfully provides real-time scene descriptions and hazard alerts, achieving low latency and demonstrating the flexibility of prompt engineering for tailoring LLM output to this unique domain. Our experiments highlight the importance of balancing detection accuracy with computational efficiency for mobile deployment. Prompt design is a key component of our system in guiding LLM responses and ensuring the relevance of generated descriptions. Additionally, the integration of user feedback proved invaluable for refining the system&rsquo;s usability and overall user experience.</p>
<p>While this project offers a promising foundation, further research is warranted. Explorations into even more advanced prompt engineering for complex scenarios would pave the way for the wide adoption of such assistive technologies. Our findings illustrate the power of integrating computer vision and large language models, leading to greater independence and safety in daily life: a true testament to AI&rsquo;s ability to improve the quality of life for all.</p>

<h2 class="relative group">References
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>
<p>[1] Jonathan Donner. After access: Inclusion, development, and a more mobile Internet. MIT press, 2015.</p>
</li>
<li>
<p>[2] Fatma Al-Muqbali, Noura Al-Tourshi, Khuloud Al-Kiyumi, and Faizal Hajmohideen. Smart technologies for visually impaired: Assisting and conquering infirmity of blind people using ai technologies. In 2020 12th Annual Undergraduate Research Conference on Applied Computing (URC), pages 1–4. IEEE, 2020.</p>
</li>
<li>
<p>[3] Muiz Ahmed Khan, Pias Paul, Mahmudur Rashid, Mainul Hossain, and Md Atiqur Rahman Ahad. An ai-based visual aid with integrated reading assistant for the completely blind. IEEE Transactions on Human-Machine Systems, 50(6):507–517, 2020.</p>
</li>
<li>
<p>[4] Bing Li, Juan Pablo Munoz, Xuejian Rong, Qingtian Chen, Jizhong Xiao, Yingli Tian, Aries Arditi, and Mohammed Yousuf. Vision-based mobile indoor assistive navigation aid for blind people. IEEE transactions on mobile computing, 18(3):702–714, 2018.</p>
</li>
<li>
<p>[5] Ashish Bastola, Md Atik Enam, Ananta Bastola, Aaron Gluck, and Julian Brinkley. Multi-functional glasses for the blind and visually impaired: Design and development. In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, volume 67, pages 995–1001. SAGE Publications Sage CA: Los Angeles, CA, 2023.</p>
</li>
<li>
<p>[6] Mouna Afif, Riadh Ayachi, Yahia Said, Edwige Pissaloux, and Mohamed Atri. An evaluation of retinanet on indoor object detection for blind and visually impaired persons assistance navigation. Neural Processing Letters , 51:2265–2279, 2020.</p>
</li>
<li>
<p>[7] Hernisa Kacorri, Kris M Kitani, Jeffrey P Bigham, and Chieko Asakawa. People with visual impairment training personal object recognizers: Feasibility and challenges. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pages 5839–5849, 2017.</p>
</li>
<li>
<p>[8] Abinash Bhandari, PWC Prasad, Abeer Alsadoon, and Angelika Maag. Object detection and recognition: using deep learning to assist the visually impaired. Disability and Rehabilitation: Assistive Technology, 16(3):280–288, 2021.</p>
</li>
<li>
<p>[9] Fahad Ashiq, Muhammad Asif, Maaz Bin Ahmad, Sadia Zafar, Khalid Masood, Toqeer Mahmood, Muhammad Tariq Mahmood, and Ik Hyun Lee. Cnn-based object recognition and tracking system to assist visually impaired people. IEEE access, 10:14819–14834, 2022.</p>
</li>
<li>
<p>[10] Askat Kuzdeuov, Shakhizat Nurgaliyev, and Hüseyin Atakan Varol. Chatgpt for visually impaired and blind. Authorea Preprints, 2023.</p>
</li>
<li>
<p>[11] Ashish Bastola, Julian Brinkley, Hao Wang, and Abolfazl Razi. Driving towards inclusion: Revisiting in-vehicle interaction in autonomous vehicles. arXiv preprint arXiv:2401.14571, 2024.</p>
</li>
<li>
<p>[12] Yi Zhao, Yilin Zhang, Rong Xiang, Jing Li, and Hillming Li. Vialm: A survey and benchmark of visually impaired assistance with large models. arXiv preprint arXiv:2402.01735, 2024.</p>
</li>
<li>
<p>[13] Ashish Bastola, Aaron Gluck, and Julian Brinkley. Feedback mechanism for blind and visually impaired: a review. In Proceedings of the Human Factors and Ergonomics Society Annual Meeting, volume 67, pages 1748–1754. SAGE Publications Sage CA: Los Angeles, CA, 2023.</p>
</li>
<li>
<p>[14] Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chellappa, and Ajay Divakaran. Zero-shot object detection. In Proceedings of the European conference on computer vision (ECCV), pages 384–400, 2018.</p>
</li>
<li>
<p>[15] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788, 2016.</p>
</li>
<li>
<p>[16] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics YOLO, January 2023.</p>
</li>
<li>
<p>[17] Wansen Wu, Tao Chang, Xinmeng Li, Quanjun Yin, and Yue Hu. Vision-language navigation: A survey and taxonomy. Neural Computing and Applications, 36(7):3291–3316, 2024.</p>
</li>
<li>
<p>[18] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
</li>
<li>
<p>[19] Zhen Zhang, Anran Lin, Chun Wai Wong, Xiangyu Chu, Qi Dou, and KW Au. Interactive navigation in environments with traversable obstacles using large language and vision-language models. arXiv preprint arXiv:2310.08873, 2023.</p>
</li>
<li>
<p>[20] Jialu Li and Mohit Bansal. Improving vision-and-language navigation by generating future-view image semantics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10803–10812, 2023.</p>
</li>
<li>
<p>[21] Feng Tian, Yuanyuan Lu, Fang Liu, Guibao Ma, Neili Zong, Xin Wang, Chao Liu, Ningbin Wei, and Kaiguang Cao. Supervised abnormal event detection based on chatgpt attention mechanism. Multimedia Tools and Applications , pages 1–19, 2024.</p>
</li>
<li>
<p>[22] Yunkang Cao, Xiaohao Xu, Chen Sun, Xiaonan Huang, and Weiming Shen. Towards generic anomaly detection and understanding: Large-scale visual-linguistic model (gpt-4v) takes the lead. arXiv preprint arXiv:2311.02782 , 2023.</p>
</li>
<li>
<p>[23] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.</p>
</li>
<li>
<p>[24] Zhichao Yang, Zonghai Yao, Mahbuba Tasmin, Parth Vashisht, Won Seok Jang, Feiyun Ouyang, Beining Wang, Dan Berlowitz, and Hong Yu. Performance of multimodal gpt-4v on usmle with image: Potential for imaging diagnostic support with explanations. medRxiv, pages 2023–10, 2023.</p>
</li>
<li>
<p>[25] Ethan Waisberg, Joshua Ong, Mouayad Masalkhi, Sharif Amit Kamran, Nasif Zaman, Prithul Sarker, Andrew G Lee, and Alireza Tavakkoli. Gpt-4: a new era of artificial intelligence in medicine. Irish Journal of Medical Science (1971-), 192(6):3197–3200, 2023.</p>
</li>
<li>
<p>[26] Pragnya Sridhar, Aidan Doyle, Arav Agarwal, Christopher Bogart, Jaromir Savelka, and Majd Sakr. Harnessing llms in curricular design: Using gpt-4 to support authoring of learning objectives. arXiv preprint arXiv:2306.17459 , 2023.</p>
</li>
<li>
<p>[27] Zhaoyi Sun, Hanley Ong, Patrick Kennedy, Liyan Tang, Shirley Chen, Jonathan Elias, Eugene Lucas, George Shih, and Yifan Peng. Evaluating gpt-4 on impressions generation in radiology reports. Radiology, 307(5):e231259, 2023.</p>
</li>
<li>
<p>[28] Henner Gimpel, Kristina Hall, Stefan Decker, Torsten Eymann, Luis Lämmermann, Alexander Mädche, Maximilian Röglinger, Caroline Ruiner, Manfred Schoch, Mareike Schoop, et al. Unlocking the power of generative ai models and systems such as gpt-4 and chatgpt for higher education: A guide for students and lecturers. Technical report, Hohenheim Discussion Papers in Business, Economics and Social Sciences, 2023.</p>
</li>
<li>
<p>[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021.</p>
</li>
<li>
<p>[30] Timo Lüddecke and Alexander Ecker. Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7086–7096, 2022.</p>
</li>
<li>
<p>[31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.</p>
</li>
<li>
<p>[32] Haicheng Liao, Huanming Shen, Zhenning Li, Chengyue Wang, Guofa Li, Yiming Bie, and Chengzhong Xu. Gpt-4 enhanced multimodal grounding for autonomous driving: Leveraging cross-modal attention with large language models. Communications in Transportation Research, 4:100116, 2024.</p>
</li>
<li>
<p>[33] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023.</p>
</li>
<li>
<p>[34] Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, and Kwan-Yee K Wong. Mapgpt: Map-guided prompting for unified vision-and-language navigation. arXiv preprint arXiv:2401.07314, 2024.</p>
</li>
<li>
<p>[35] Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, and Yoon Kim. Langnav: Language as a perceptual representation for navigation. arXiv preprint arXiv:2310.07889, 2023.</p>
</li>
<li>
<p>[36] Dhruv Shah, Błazej Osi ˙ ˙ nski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-trained models of ´ ´ language, vision, and action. In Conference on Robot Learning, pages 492–504. PMLR, 2023.</p>
</li>
<li>
<p>[37] Yue Zhang, Quan Guo, and Parisa Kordjamshidi. Navhint: Vision and language navigation agent with a hint generator. arXiv preprint arXiv:2402.02559, 2024.</p>
</li>
<li>
<p>[38] Hochul Hwang, Sunjae Kwon, Yekyung Kim, and Donghyun Kim. Is it safe to cross? interpretable risk assessment with gpt-4v for safety-aware street crossing. arXiv preprint arXiv:2402.06794, 2024.</p>
</li>
<li>
<p>[39] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14393–14402, 2021.</p>
</li>
<li>
<p>[40] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.</p>
</li>
<li>
<p>[41] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16793–16803, 2022.</p>
</li>
<li>
<p>[42] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krähenbühl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In European Conference on Computer Vision, pages 350–368. Springer, 2022.</p>
</li>
<li>
<p>[43] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for openvocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15254–15264, 2023.</p>
</li>
<li>
<p>[44] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904–4916. PMLR, 2021.</p>
</li>
<li>
<p>[45] Georg Heigold, Matthias Minderer, Alexey Gritsenko, Alex Bewley, Daniel Keysers, Mario Luciˇ ˇ c, Fisher Yu, ´ ´ and Thomas Kipf. Video owl-vit: Temporally-consistent open-world localization in video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13802–13811, 2023.</p>
</li>
<li>
<p>[46] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965–10975, 2022.</p>
</li>
<li>
<p>[47] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.</p>
</li>
<li>
<p>[48] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605 , 2022.</p>
</li>
<li>
<p>[49] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759–9768, 2020.</p>
</li>
<li>
<p>[50] Johnathan Xie and Shuai Zheng. Zsd-yolo: Zero-shot yolo detection using vision-language knowledgedistillation. arXiv preprint arXiv:2109.12066, 1(2):3, 2021.</p>
</li>
<li>
<p>[51] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. arXiv preprint arXiv:2401.17270, 2024.</p>
</li>
<li>
<p>[52] Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr. A systematic survey of prompt engineering on vision-language foundation models. arXiv preprint arXiv:2307.12980, 2023.</p>
</li>
<li>
<p>[53] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 2023.</p>
</li>
<li>
<p>[54] Jiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing Dai, Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen Hu, et al. Prompt engineering for healthcare: Methodologies and applications. arXiv preprint arXiv:2304.14670, 2023.</p>
</li>
<li>
<p>[55] Ashish Bastola, Hao Wang, Judsen Hembree, Pooja Yadav, Nathan McNeese, and Abolfazl Razi. Llm-based smart reply (lsr): Enhancing collaborative performance with chatgpt-mediated smart reply system (acm)(draft) llm-based smart reply (lsr): Enhancing collaborative performance with chatgpt-mediated smart reply system. arXiv preprint arXiv:2306.11980, 2023.</p>
</li>
<li>
<p>[56] Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan O Arik, and Tomas Pfister. Better zero-shot reasoning with self-adaptive prompting. arXiv preprint arXiv:2305.14106, 2023.</p>
</li>
<li>
<p>[57] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.</p>
</li>
<li>
<p>[58] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022.</p>
</li>
<li>
<p>[59] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.</p>
</li>
<li>
<p>[60] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset for llm question answering with external tools. Advances in Neural Information Processing Systems, 36, 2024.</p>
</li>
<li>
<p>[61] Jonathan Pilault, Raymond Li, Sandeep Subramanian, and Christopher Pal. On extractive and abstractive neural document summarization with transformer language models. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 9308–9319, 2020.</p>
</li>
<li>
<p>[62] Raul Puri and Bryan Catanzaro. Zero-shot text classification with generative language models. arXiv preprint arXiv:1912.10165, 2019.</p>
</li>
<li>
<p>[63] Benjamin Clavié, Alexandru Ciceu, Frederick Naylor, Guillaume Soulié, and Thomas Brightwell. Large language models in the workplace: A case study on prompt engineering for job type classification. In International Conference on Applications of Natural Language to Information Systems, pages 3–17. Springer, 2023.</p>
</li>
<li>
<p>[64] Rakesh Chandra Joshi, Saumya Yadav, Malay Kishore Dutta, and Carlos M Travieso-Gonzalez. Efficient multiobject detection and smart navigation using artificial intelligence for visually impaired people. Entropy, 22(9):941, 2020.</p>
</li>
<li>
<p>[65] Lilit Hakobyan, Jo Lumsden, Dympna O&rsquo;Sullivan, and Hannah Bartlett. Mobile assistive technologies for the visually impaired. Survey of ophthalmology, 58(6):513–528, 2013.</p>
</li>
<li>
<p>[66] Karol Matusiak, Piotr Skulimowski, and P Strurniłło. Object recognition in a mobile phone application for visually impaired users. In 2013 6th International Conference on Human System Interactions (HSI), pages 479–484. IEEE, 2013.</p>
</li>
<li>
<p>[67] Shafin Rahman, Salman Khan, and Nick Barnes. Improved visual-semantic alignment for zero-shot object detection. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 11932–11939, 2020.</p>
</li>
<li>
<p>[68] Ali Jasim Ramadhan. Wearable smart system for visually impaired people. sensors, 18(3):843, 2018.</p>
</li>
<li>
<p>[69] Navid Fallah, Ilias Apostolopoulos, Kostas Bekris, and Eelke Folmer. The user as a sensor: navigating users with visual impairments in indoor spaces using tactile landmarks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 425–432, 2012.</p>
</li>
<li>
<p>[70] Wu Tang, De-er Liu, Xiaoli Zhao, Zenghui Chen, and Chen Zhao. A dataset for the recognition of obstacles on blind sidewalk. Universal Access in the Information Society, 22(1):69–82, 2023.</p>
</li>
<li>
<p>[71] Xi Li, Anthony Dick, Chunhua Shen, Anton Van Den Hengel, and Hanzi Wang. Incremental learning of 3d-dct compact representations for robust visual tracking. IEEE transactions on pattern analysis and machine intelligence , 35(4):863–881, 2012.</p>
</li>
<li>
<p>[72] Ramin Mehran, Alexis Oyama, and Mubarak Shah. Abnormal crowd behavior detection using social force model. In 2009 IEEE conference on computer vision and pattern recognition, pages 935–942. IEEE, 2009.</p>
</li>
<li>
<p>[73] Dong-in Kim and Jangwon Lee. Anomaly detection for visually impaired people using a 360 degree wearable camera. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2022.</p>
</li>
</ul>

<h2 class="relative group">A Original Prompt
    <div id="a-original-prompt" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-original-prompt" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This section illustrates all used prompts in the proposed system.</p>

<h2 class="relative group">A.1 LLM Instruction
    <div id="a1-llm-instruction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a1-llm-instruction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>System instructions are usually directly fed into LLMs as a self-prompt and generally do not consume token usage.</p>
<p>Main Instruction: &ldquo;You are a voice assistant for a visually impaired user, the input is the actual data collected by a phone camera, and the phone is always facing front, please provide the key information for the blind user to help him navigate and avoid potential danger. Please note that the center_x and center_y represent the object location (proportional to the image), object height and width are also a proportion.&rdquo; System Sensitivity Prompt: &ldquo;System sensitivity: Incorporate the sensitivity setting in your response. For a low-sensitivity setting, identify and report only imminent and direct threats to safety. For medium sensitivity, include potential hazards that could pose a risk if not avoided. For high sensitivity, report all detected objects that could cause any inconvenience or danger. Current sensitivity: low.&rdquo;</p>
<p>Location Prompt: &ldquo;The location information center x , center y , height, width of objects is the proportion to the image, the detected objects are categorized into 4 type based on the image region. Left and Right: objects located on left 25% or right 25% of the image, these objects are usually moving and has large proportion.Front: objects that may still far away, can be used to discriminate the current situation.Ground: objects that may nearby.&rdquo;</p>
<p>Motion Prompt: &ldquo;Using the information from last frame and current frame to analyze the movement (speed and direction) and location of each object to determine its trajectory relative to the user. Use this information to assess whether an object is moving towards the user or they are static. If moving, how quickly a potential collision might occur based on the object&rsquo;s speed and direction of movement.&rdquo;</p>

<h2 class="relative group">A.2 LLM Prompt
    <div id="a2-llm-prompt" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a2-llm-prompt" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>LLM prompts are the master prompts that are directly input from the user end, text tokens are counted to the usage. To control and optimize the usage, we designed three different output modes. Furthermore, the designed prompt will guide LLM to generate a structured data format (dictionary, list, JSON, etc.).</p>
<p>Full diagnose mode: &lsquo;Please organize your output into this format: &ldquo;scene&rdquo;: quickly describe the current situation for blind user; &ldquo;key_objects&rdquo;: quickly and roughly locate the key objects for blind user; &ldquo;anomaly_checker&rdquo;: quickly diagnose if there is potential danger for a blind person; &ldquo;anomaly_label&rdquo;: output 1 if there is an emergency, output 0 if not; &ldquo;anomaly_index&rdquo;: object_id, danger_index, estimate a score from 0 to 1 about each objects that may cause danger; &ldquo;voice_guide&rdquo;: the main output to instant alert the blind person for emergency.&rsquo;</p>
<p>Voice-only mode: &lsquo;Please organize your output into this format: &ldquo;voice_guide&rdquo;: the main output to instantly alert the blind person for an emergency.&rsquo;</p>
<p>Annotation mode: &lsquo;Please organize your output into this format: &ldquo;anomaly_score&rdquo;: predict a score from 0 to 1 to evaluate the emergency level; &ldquo;reason&rdquo;: explain your annotation reason within 10 words.&rsquo;</p>

<h2 class="relative group">A.3 Other Prompts
    <div id="a3-other-prompts" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a3-other-prompts" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Detection Classes Switch: &ldquo;The user is switching the scene to custom_scene please generate a new list that contains the top 100 related objects, including especially road hazards and possible obstacles&rdquo;</p>
<p>Interest Target Setting: &ldquo;Please analyze the user command and extract the user required object, output into this format: &ldquo;add&rdquo;: object_name.&rdquo;</p>

<h2 class="relative group">B Detection Labels for Custom Scenes
    <div id="b-detection-labels-for-custom-scenes" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-detection-labels-for-custom-scenes" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This section illustrates the detection classes generated by GPT-4 that are customized for specific scenes.</p>
<p>Visually impaired navigation: [ &lsquo;car&rsquo;, &lsquo;person&rsquo;, &lsquo;bus&rsquo;, &lsquo;bicycle&rsquo;, &lsquo;motorcycle&rsquo;, &rsquo;traffic light&rsquo;, &lsquo;stop sign&rsquo;, &lsquo;fountain&rsquo;, &lsquo;crosswalk&rsquo;, &lsquo;sidewalk&rsquo;, &lsquo;door&rsquo;, &lsquo;stair&rsquo;, &rsquo;escalator&rsquo;, &rsquo;elevator&rsquo;, &lsquo;ramp&rsquo;, &lsquo;bench&rsquo;, &rsquo;trash can&rsquo;, &lsquo;pole&rsquo;, &lsquo;fence&rsquo;, &rsquo;tree&rsquo;, &lsquo;dog&rsquo;, &lsquo;cat&rsquo;, &lsquo;bird&rsquo;, &lsquo;parking meter&rsquo;, &lsquo;mailbox&rsquo;, &lsquo;manhole&rsquo;, &lsquo;puddle&rsquo;, &lsquo;construction sign&rsquo;, &lsquo;construction barrier&rsquo;, &lsquo;scaffolding&rsquo;, &lsquo;hole&rsquo;, &lsquo;crack&rsquo;, &lsquo;speed bump&rsquo;, &lsquo;curb&rsquo;, &lsquo;guardrail&rsquo;, &rsquo;traffic cone&rsquo;, &rsquo;traffic barrel&rsquo;, &lsquo;pedestrian signal&rsquo;, &lsquo;street sign&rsquo;, &lsquo;fire hydrant&rsquo;, &rsquo;lamp post&rsquo;, &lsquo;bench&rsquo;, &lsquo;picnic table&rsquo;, &lsquo;public restroom&rsquo;, &lsquo;fountain&rsquo;, &lsquo;statue&rsquo;, &lsquo;monument&rsquo;, &lsquo;directional sign&rsquo;, &lsquo;information sign&rsquo;, &lsquo;map&rsquo;, &rsquo;emergency exit&rsquo;, &rsquo;no smoking sign&rsquo;, &lsquo;wet floor sign&rsquo;, &lsquo;closed sign&rsquo;, &lsquo;open sign&rsquo;, &rsquo;entrance sign&rsquo;, &rsquo;exit sign&rsquo;, &lsquo;stairs sign&rsquo;, &rsquo;escalator sign&rsquo;, &rsquo;elevator sign&rsquo;, &lsquo;restroom sign&rsquo;, &lsquo;men restroom sign&rsquo;, &lsquo;women restroom sign&rsquo;, &lsquo;unisex restroom sign&rsquo;, &lsquo;baby changing station&rsquo;, &lsquo;wheelchair accessible sign&rsquo;, &lsquo;braille sign&rsquo;, &lsquo;audio signal device&rsquo;, &rsquo;tactile paving&rsquo;, &lsquo;detectable warning surface&rsquo;, &lsquo;guide rail&rsquo;, &lsquo;handrail&rsquo;, &rsquo;turnstile&rsquo;, &lsquo;gate&rsquo;, &rsquo;ticket barrier&rsquo;, &lsquo;security checkpoint&rsquo;, &lsquo;metal detector&rsquo;, &lsquo;baggage claim&rsquo;, &rsquo;lost and found&rsquo;, &lsquo;information desk&rsquo;, &lsquo;meeting point&rsquo;, &lsquo;waiting area&rsquo;, &lsquo;seating area&rsquo;, &lsquo;boarding area&rsquo;, &lsquo;disembarking area&rsquo;, &lsquo;charging station&rsquo;, &lsquo;water dispenser&rsquo;, &lsquo;vending machine&rsquo;, &lsquo;ATM&rsquo;, &lsquo;kiosk&rsquo;, &lsquo;public telephone&rsquo;, &lsquo;public Wi-Fi hotspot&rsquo;, &rsquo;emergency phone&rsquo;, &lsquo;first aid station&rsquo;, &lsquo;defibrillator&rsquo;, &rsquo;tree&rsquo;, &lsquo;pole&rsquo;, &rsquo;lamp post&rsquo;, &lsquo;staff&rsquo;, &lsquo;road hazard&rsquo;]</p>
<p>Urban Walking: [&lsquo;pedestrian&rsquo;, &lsquo;cyclist&rsquo;, &lsquo;car&rsquo;, &lsquo;bus&rsquo;, &lsquo;motorcycle&rsquo;, &lsquo;scooter&rsquo;, &rsquo;electric scooter&rsquo;, &rsquo;traffic light&rsquo;, &lsquo;stop sign&rsquo;, &lsquo;crosswalk&rsquo;, &lsquo;sidewalk&rsquo;, &lsquo;curb&rsquo;, &lsquo;ramp&rsquo;, &lsquo;stair&rsquo;, &rsquo;escalator&rsquo;, &rsquo;elevator&rsquo;, &lsquo;bench&rsquo;, &rsquo;trash can&rsquo;, &lsquo;pole&rsquo;, &lsquo;fence&rsquo;, &rsquo;tree&rsquo;, &lsquo;fire hydrant&rsquo;, &rsquo;lamp post&rsquo;, &lsquo;construction barrier&rsquo;, &lsquo;construction sign&rsquo;, &lsquo;scaffolding&rsquo;, &lsquo;hole&rsquo;, &lsquo;crack&rsquo;, &lsquo;speed bump&rsquo;, &lsquo;puddle&rsquo;, &lsquo;manhole&rsquo;, &lsquo;drain&rsquo;, &lsquo;grate&rsquo;, &rsquo;loose gravel&rsquo;, &lsquo;ice patch&rsquo;, &lsquo;snow pile&rsquo;, &rsquo;leaf pile&rsquo;, &lsquo;standing water&rsquo;, &lsquo;mud&rsquo;, &lsquo;sand&rsquo;, &lsquo;street sign&rsquo;, &lsquo;directional sign&rsquo;, &lsquo;information sign&rsquo;, &lsquo;parking meter&rsquo;, &lsquo;mailbox&rsquo;, &lsquo;bicycle rack&rsquo;, &lsquo;outdoor seating&rsquo;, &lsquo;planter box&rsquo;, &lsquo;bollard&rsquo;, &lsquo;guardrail&rsquo;, &rsquo;traffic cone&rsquo;, &rsquo;traffic barrel&rsquo;, &lsquo;pedestrian signal&rsquo;, &lsquo;crowd&rsquo;, &lsquo;animal&rsquo;, &lsquo;dog&rsquo;, &lsquo;bird&rsquo;, &lsquo;cat&rsquo;, &lsquo;public restroom&rsquo;, &lsquo;fountain&rsquo;,</p>
<p>&lsquo;statue&rsquo;, &lsquo;monument&rsquo;, &lsquo;picnic table&rsquo;, &lsquo;outdoor advertisement&rsquo;, &lsquo;vendor cart&rsquo;, &lsquo;food truck&rsquo;, &rsquo;emergency exit&rsquo;, &rsquo;no smoking sign&rsquo;, &lsquo;wet floor sign&rsquo;, &lsquo;closed sign&rsquo;, &lsquo;open sign&rsquo;, &rsquo;entrance sign&rsquo;, &rsquo;exit sign&rsquo;, &lsquo;stairs sign&rsquo;, &rsquo;escalator sign&rsquo;, &rsquo;elevator sign&rsquo;, &lsquo;restroom sign&rsquo;, &lsquo;braille sign&rsquo;, &lsquo;audio signal device&rsquo;, &rsquo;tactile paving&rsquo;, &lsquo;detectable warning surface&rsquo;, &lsquo;guide rail&rsquo;, &lsquo;handrail&rsquo;, &rsquo;turnstile&rsquo;, &lsquo;gate&rsquo;, &lsquo;security checkpoint&rsquo;, &lsquo;water dispenser&rsquo;, &lsquo;vending machine&rsquo;, &lsquo;ATM&rsquo;, &lsquo;kiosk&rsquo;, &lsquo;public telephone&rsquo;, &lsquo;public Wi-Fi hotspot&rsquo;, &rsquo;emergency phone&rsquo;, &lsquo;charging station&rsquo;, &lsquo;first aid station&rsquo;, &lsquo;defibrillator&rsquo;, &rsquo;tree&rsquo;, &lsquo;pole&rsquo;, &rsquo;lamp post&rsquo;, &lsquo;staff&rsquo;, &lsquo;road hazard&rsquo;]</p>
<p>Walking General: [&lsquo;vehicles&rsquo;, &lsquo;pedestrians&rsquo;, &rsquo;traffic signs and signals&rsquo;, &lsquo;roadway features&rsquo;, &lsquo;surface conditions&rsquo;, &lsquo;street furniture&rsquo;, &lsquo;construction areas&rsquo;, &lsquo;vegetation&rsquo;, &lsquo;animals&rsquo;, &lsquo;public amenities&rsquo;, &rsquo;navigation aids&rsquo;, &rsquo;temporary obstacles&rsquo;, &rsquo;emergency facilities&rsquo;, &rsquo;transportation hubs&rsquo;, &rsquo;electronic devices&rsquo;, &lsquo;safety features&rsquo;]</p>
<p>Urban Walking Hazards: [&lsquo;person&rsquo;, &lsquo;cyclist&rsquo;, &lsquo;car&rsquo;, &lsquo;bus&rsquo;, &lsquo;motorcycle&rsquo;, &lsquo;scooter&rsquo;, &lsquo;fountain&rsquo;, &lsquo;red traffic light&rsquo;, &lsquo;green traffic light&rsquo;, &lsquo;stop sign&rsquo;, &lsquo;curb&rsquo;, &lsquo;ramp&rsquo;, &lsquo;stair&rsquo;, &rsquo;escalator&rsquo;, &rsquo;elevator&rsquo;, &lsquo;bench&rsquo;, &rsquo;trash can&rsquo;, &lsquo;pole&rsquo;, &rsquo;tree&rsquo;, &lsquo;fire hydrant&rsquo;, &rsquo;lamp post&rsquo;, &lsquo;construction barrier&rsquo;, &lsquo;construction sign&rsquo;, &lsquo;scaffolding&rsquo;, &lsquo;hole&rsquo;, &lsquo;crack&rsquo;, &lsquo;speed bump&rsquo;, &lsquo;puddle&rsquo;, &lsquo;manhole&rsquo;, &lsquo;drain&rsquo;, &lsquo;grate&rsquo;, &rsquo;loose gravel&rsquo;, &lsquo;ice patch&rsquo;, &lsquo;snow pile&rsquo;, &rsquo;leaf pile&rsquo;, &lsquo;standing water&rsquo;, &lsquo;mud&rsquo;, &lsquo;sand&rsquo;, &lsquo;street sign&rsquo;, &lsquo;directional sign&rsquo;, &lsquo;information sign&rsquo;, &lsquo;parking meter&rsquo;, &lsquo;mailbox&rsquo;, &lsquo;bicycle rack&rsquo;, &lsquo;outdoor seating&rsquo;, &lsquo;planter box&rsquo;, &lsquo;bollard&rsquo;, &lsquo;guardrail&rsquo;, &rsquo;traffic cone&rsquo;, &rsquo;traffic barrel&rsquo;, &lsquo;pedestrian signal&rsquo;, &lsquo;crowd&rsquo;, &lsquo;animal&rsquo;, &lsquo;dog&rsquo;, &lsquo;bird&rsquo;, &lsquo;cat&rsquo;, &lsquo;public restroom&rsquo;, &lsquo;fountain&rsquo;, &lsquo;statue&rsquo;, &lsquo;monument&rsquo;, &lsquo;picnic table&rsquo;, &lsquo;outdoor advertisement&rsquo;, &lsquo;vendor cart&rsquo;, &lsquo;food truck&rsquo;, &rsquo;emergency exit&rsquo;, &rsquo;no smoking sign&rsquo;, &lsquo;wet floor sign&rsquo;, &lsquo;closed sign&rsquo;, &lsquo;open sign&rsquo;, &rsquo;entrance sign&rsquo;, &rsquo;exit sign&rsquo;, &lsquo;stairs sign&rsquo;, &rsquo;escalator sign&rsquo;, &rsquo;elevator sign&rsquo;, &lsquo;restroom sign&rsquo;, &lsquo;braille sign&rsquo;, &lsquo;audio signal device&rsquo;, &rsquo;tactile paving&rsquo;, &lsquo;detectable warning surface&rsquo;, &lsquo;guide rail&rsquo;, &lsquo;handrail&rsquo;, &rsquo;turnstile&rsquo;, &lsquo;gate&rsquo;, &lsquo;security checkpoint&rsquo;, &lsquo;water dispenser&rsquo;, &lsquo;vending machine&rsquo;, &lsquo;ATM&rsquo;, &lsquo;kiosk&rsquo;, &lsquo;public telephone&rsquo;, &rsquo;emergency phone&rsquo;, &lsquo;charging station&rsquo;, &lsquo;first aid station&rsquo;, &lsquo;defibrillator&rsquo;, &lsquo;oil spill&rsquo;, &lsquo;road debris&rsquo;, &lsquo;branches&rsquo;, &lsquo;water&rsquo; &rsquo;low-hanging signage&rsquo;, &lsquo;road signs&rsquo;, &lsquo;roadworks&rsquo;, &rsquo;excavation sites&rsquo;, &lsquo;utility works&rsquo;, &lsquo;fallen objects&rsquo;, &lsquo;spilled cargo&rsquo;, &lsquo;flood&rsquo;, &lsquo;ice&rsquo;, &lsquo;snowdrift&rsquo;, &rsquo;landslide debris&rsquo;, &rsquo;erosion damage&rsquo;, &lsquo;parked vehicles&rsquo;, &lsquo;moving equipment&rsquo;, &rsquo;large gatherings&rsquo;, &lsquo;parade&rsquo;, &lsquo;marathon&rsquo;, &lsquo;street fair&rsquo;, &lsquo;scaffolding&rsquo;, &rsquo;electrical hazards&rsquo;, &lsquo;wire tangle&rsquo;, &lsquo;manhole covers&rsquo;, &lsquo;street elements&rsquo;, &lsquo;road hazards&rsquo;, &rsquo;toxic spill&rsquo;, &lsquo;biohazard materials&rsquo;, &lsquo;wildlife crossings&rsquo;, &lsquo;stray animals&rsquo;, &lsquo;pets&rsquo;, &lsquo;flying debris&rsquo;, &lsquo;air pollution&rsquo;,&lsquo;smoke plumes&rsquo;, &lsquo;dust storms&rsquo;, &lsquo;sandstorms&rsquo;, &lsquo;floods&rsquo;, &lsquo;road crack&rsquo;]</p>
<p>Walking test: [ &lsquo;vehicle&rsquo;, &lsquo;pedestrian&rsquo;, &lsquo;cyclist&rsquo;, &rsquo;traffic signal&rsquo;, &lsquo;street sign&rsquo;, &lsquo;crosswalk&rsquo;, &lsquo;sidewalk&rsquo;, &lsquo;curb&rsquo;, &lsquo;ramp&rsquo;, &lsquo;stair&rsquo;, &rsquo;escalator&rsquo;, &rsquo;elevator&rsquo;, &lsquo;public seating&rsquo;, &rsquo;trash receptacle&rsquo;, &lsquo;street furniture&rsquo;, &rsquo;tree&rsquo;, &lsquo;construction site&rsquo;, &lsquo;road obstruction&rsquo;, &rsquo;loose materials&rsquo;, &lsquo;slick surface&rsquo;, &lsquo;animal&rsquo;, &lsquo;outdoor advertisement&rsquo;, &lsquo;vendor&rsquo;, &lsquo;water feature&rsquo;, &lsquo;monument&rsquo;, &lsquo;information point&rsquo;, &lsquo;access point&rsquo;, &lsquo;safety equipment&rsquo;, &rsquo;navigation aid&rsquo;, &lsquo;public amenity&rsquo;, &rsquo;transport hub&rsquo;, &lsquo;obstacle crowd&rsquo; ]</p>
<p>Annotation mask = ’people’, ’human face’, ’car license plate’, ’license plate’, ’plate’</p>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/VisionGPT LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation.md"
          data-oid-likes="likes_papers/VisionGPT LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/vlavad-vision-language-models-assisted-unsupervised-vad/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/video-anomaly-detection-and-explanation-via-large-language-models/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
