<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/anyanomaly-zero-shot-customizable-video-anomaly-detection-with-lvlm/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/anyanomaly-zero-shot-customizable-video-anomaly-detection-with-lvlm/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/anyanomaly-zero-shot-customizable-video-anomaly-detection-with-lvlm\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "8115"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>8115 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">39 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM
    <div id="anyanomaly-zero-shot-customizable-video-anomaly-detection-with-lvlm" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#anyanomaly-zero-shot-customizable-video-anomaly-detection-with-lvlm" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Sunghyun Ahn * *, Youngwan Jo * *, Kijung Lee, Sein Kwon, Inpyo Hong, Sanghyun Park † Yonsei University, Seoul, Korea</p>
<p>{skd, jyy1551, rlwjd4177, seinkwon97, hip9863, <a
  href="mailto:sanghyun%7d@yonsei.ac.kr">sanghyun}@yonsei.ac.kr</a></p>

<h2 class="relative group">Abstract
    <div id="abstract" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#abstract" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers userdefined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive performance on VAD benchmark datasets, achieving state-of-the-art results on the UBnormal dataset and outperforming other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly .</p>

<h2 class="relative group">1. Introduction
    <div id="1-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video anomaly detection (VAD) aims to detect abnormal events in video streams. Abnormal events include the actions of objects that are inappropriate for the environment (e.g., climbing over a fence) or objects with unusual appearances (e.g., a bicycle on a walkway). However, abnormal events are rare and diverse, making it difficult to construct large-scale datasets for VAD. Therefore, the VAD is recognized as a highly challenging problem.</p>
<p>To overcome these limitations, previous studies primarily used one-class classification (OCC) methods that learn only from normal data. In the OCC approach, the model</p>
<ul>
<li>Equal contribution</li>
</ul>
<p>† Corresponding author</p>
<p>Figure 1. Comparison of traditional video Anomaly Detection (VAD) and customizable video anomaly detection (C-VAD). Traditional VAD models struggle with generalization, making them hard to apply in diverse environments, while C-VAD can handle various video environments.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_de4d99bf1134e66877792d95fa37b7df251643a344c5bf965766f0b1d3c990b1.png"
    ></figure>
<p>learns normal patterns and classifies the cases that deviate from them as abnormal. Representative OCC methods include classification-[14 , 30 , 39], distance-[2 , 27 , 29], and prediction-based methods [10 , 15 , 17 , 37], all of which have demonstrated excellent performance in VAD tasks.</p>
<p>However, because normal and abnormal classes can be defined differently depending on the environment, OCC methods cannot always guarantee a generalized performance. For example, as shown on the left side of Figure 1, a model trained in a campus environment learns the characteristics of a &lsquo;person&rsquo; as a normal pattern and classifies a &lsquo;car&rsquo; as abnormal. However, when this model is applied to a road environment, a &lsquo;car&rsquo; is still detected as abnormal, which can increase the number of false positives. Therefore, OCC methods require the retraining of normal patterns for each new environment, which entails additional costs, such as data collection, expert intervention, and high-performance equipment. Because of these limitations, the application of VAD models to real-world scenarios is challenging.</p>
<p>To address this issue, we propose a novel technique called customizable video anomaly detection (C-VAD). CVAD considers user-defined text as abnormal events and detects the frames containing these events in the video. For instance, in campus videos, &lsquo;car&rsquo; can be set as an abnormal event, while in road videos, &lsquo;person&rsquo; can be set as an abnor-</p>
<p>mal event. In contrast to existing VAD models, which judge abnormalities based on learned normal patterns, C-VAD dynamically detects abnormal patterns based on the text provided. This implies that as the generalizability of visual text analysis improves, anomaly detection becomes more effective in various environments. Consequently, we introduce a zero-shot capable C-VAD approach, as shown on the right side of Figure 1, and propose the AnyAnomaly model, which allows for VAD in various environments without the need for additional training.</p>
<p>An effective method to implement zero-shot C-VAD is to leverage large vision language models (LVLMs). Recently, LVLMs have demonstrated outstanding generalization performance in visual text analysis. By leveraging this capability, C-VAD can be performed effectively across various environments. The most intuitive method involves performing visual question answering (VQA) [4] on each frame to estimate the anomaly score. For instance, one could provide the model with the prompt: &ldquo;Return a value between 0 (no) and 1 (yes) indicating how well the input image represents the text provided by the user&rdquo;. This was used as the baseline model. However, through experiments, we observed the following limitations of the baseline model: 1 Due to the large computational cost of LVLMs, the latency is high. 2 Difficulty in analyzing specific objects due to the characteristics of surveillance videos, such as foreground-background imbalance and object congestion. 3 Difficulty in detecting action-related anomalies because of the inability to utilize temporal information.</p>
<p>To overcome these limitations, we designed an AnyAnomaly model with the structure shown in Figure 2 . First, to reduce the latency, we adopted a segment-level approach that groups consecutive frames into a single segment for processing. For this purpose, we introduced a key frames selection module (KSM) that selects key frames representing the segment and performed VQA per segment. Second, instead of performing simple image-text matching, we introduced a context-aware VQA approach to enable a deeper understanding of the scene. To this end, we additionally utilized two types of information: position context, P C and temporal context, T C . P C is a context that emphasizes important locations within a frame, enhancing the object analysis capability of the LVLM. T C is a context that structures scene changes over time into a grid format, improving the action analysis capability of the LVLM. Notably, the proposed KSM and context generation modules operate in a training-free manner, allowing for easy application of C-VAD without additional data training.</p>
<p>To evaluate the performance of C-VAD, we classified existing VAD benchmark datasets based on anomaly types to create the C-VAD datasets. Through this process, we demonstrated the superiority of AnyAnomaly. Furthermore, despite being a zero-shot approach, AnyAnomaly achieved</p>
<p>Figure 2. The architecture of AnyAnomaly</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_303abafa5ae0e38e9e449277d28df1cc6dc7bd68fd814ea2b2aab2ada21c5e5d.png"
    ></figure>
<p>competitive performance on VAD datasets compared to traditional OCC-based VAD models. It achieved state-of-theart (SOTA) results on the UBnormal dataset [1] and showed superior generalization across all datasets. The proposed approach is expected to be an effective solution for deploying VAD technology in real-world applications. The contributions of this study are as follows:</p>
<ul>
<li>We propose the C-VAD technique for anomaly detection in diverse environments. To the best of our knowledge, it is the first to perform VAD based on user-defined anomalies.</li>
<li>We develop the AnyAnomaly model, which applies context-aware VQA to perform C-VAD effectively.</li>
<li>To evaluate the performance of C-VAD, we construct new C-VAD datasets and experimentally verify the superiority of AnyAnomaly.</li>
<li>AnyAnomaly achieves SOTA performance on UBnormal dataset and outperforms other methods in generalization across all datasets.</li>
</ul>

<h2 class="relative group">2. Related Work
    <div id="2-related-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-related-work" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video Anomaly Detection. Most VAD models adopt the OCC approach to detect anomalies by learning normal patterns. Among them, prediction-based methods train models to predict future or past frames based on normal frames, assuming that abnormal frames exhibit larger prediction errors. Liu [17] proposed a method that utilizes FlowNet [11] and GANs [23] to predict the t + 1-th frame</p>
<p>given t input frames. Yang [37] introduced an approach that selects key frames from t input frames to predict the entire sequence. However, because the definitions of normal and abnormal patterns may vary depending on the environment, the OCC approach, which relies on learned normal patterns, has a limited generalization performance. To mitigate this limitation, recent studies have explored cross-domain VAD (xVAD). Notably, zxVAD [3] enhances the adaptability to new environments by synthesizing abnormal patterns using the cut mix technique on images from auxiliary datasets. However, these approaches depend on fixed data transformations, making it difficult to fully capture the diverse abnormal patterns that may occur in real-world scenarios. Therefore, we propose a novel VAD method that uses textual information to dynamically detect abnormal patterns that vary depending on the environment.</p>
<p>Large Vision Language Models. Large language models have primarily been used in natural language processing; however, they have recently been applied to multimodal tasks such as image captioning and VQA. For example, MiniGPT-4 [41] processes multimodal inputs by connecting a pre-trained vision encoder to the Vicuna [7] model through a linear layer. Recent LVLMs have employed novel visual encoding techniques to better understand images. Chat-UniVi [13] generates dynamic tokens for images, thereby reducing unnecessary information and effectively extracting key visual features. This model enables flexible analysis by applying dynamic tokens across various resolutions. MiniCPM-V [38] applies the best partition technique according to the image resolution and generates tokens optimized for each segment, thereby improving the memory efficiency. However, despite the advancements in LVLMs, they are trained for general purposes, making their direct application in VAD challenging. To handle VAD effectively, it is essential to consider the characteristics of surveillance videos and leverage temporal information. Therefore, we propose a training-free approach to minimize the domain gap between the LVLMs and VAD tasks.</p>

<h2 class="relative group">3. Method
    <div id="3-method" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-method" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">3.1. Overview
    <div id="31-overview" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#31-overview" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure 2 illustrates the structure of the AnyAnomaly model, which performs context-aware VQA. The input is a video segment S = {s0, . . . , sN − 1 } comprising N frames, where N is a multiple of 4. The KSM selects key frames K = {k0, . . . , k3} from S. Among the selected key frames, the representative frame ˆ k is used to generate PC, whereas K is used to create T C. Subsequently, ˆ k , P C, and T C are utilized as image inputs for the LVLM, whereas the userprovided text X is combined with a prompt and used as the text input. Finally, the LVLM&rsquo;s response results are integrated to compute an anomaly score.</p>
<p>The user-defined text X refers to a natural language description of the anomaly the user wishes to detect. It can be a single word (e.g., &ldquo;bicycle&rdquo;), diverse events (e.g., &ldquo;jumping-falling&rdquo;), or a complex behavior (e.g., &ldquo;driving outside the lane&rdquo;). In the case of diverse events, each event keyword is processed individually as a single word.</p>

<h2 class="relative group">3.2. Key frames Selection Module
    <div id="32-key-frames-selection-module" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#32-key-frames-selection-module" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure 3a shows KSM, a key component of the segmentlevel approach. For this purpose, we selected four frames representing the segment as K and utilized the CLIP [25] model, which was trained to match the images and text.</p>
<p>Specifically, S and X are inputs to the image encoder E I and text encoder E T , respectively, and the similarity is calculated using the dot product of N image and text embeddings. The frame with the highest similarity is selected as the representative frame ˆ k .</p>
<!-- formula-not-decoded -->
<p>The index of the representative frame ˆ k, denoted as ˆ i, is used to select the other key frames. We divide the segment into four groups of equal size and select the ˆ i mod N 4 -th frame from each group. For example, when N = 8 and ˆ i = 4, the 0-th frame from each group is selected and the final set is K = {s0, s2, s4, s6}. This process is defined as follows:</p>
<!-- formula-not-decoded -->
<p>Using the KSM, K is generated by considering both text alignment and temporal uniformity, thereby enabling effective context generation. A comparative analysis of the key frames selection method is presented in Section 4.5 .</p>

<h2 class="relative group">3.3. Context Generation
    <div id="33-context-generation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#33-context-generation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>P C and T C are key elements of context-aware VQA, serving as additional information that complements the input image. P C enhances the object analysis capability of the LVLM and is generated through WinCLIP-based attention (WA). T C strengthens the action analysis ability of the LVLM and is created through grid image generation (GIG).</p>
<p>WinCLIP-based Attention. Figure 3b illustrates the WA method. We emphasize the regions related to X at ˆ k based on WinCLIP, as proposed by Jeong [12]. First, ˆ k is divided into multiple windows, and individual embeddings are generated from each window using EI . For example, when the image size is 240 × 240, it is divided into 25 windows of size 48 × 48, and the embeddings of each window are</p>
<p>Figure 3. Architecture of the proposed modules. KSM is essential for the segment-level approach, and WA and GIG are crucial for context generation.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_120b73b5ab81e9f9ec5c9cffa8c54725b2f35d700d760c0bfa61525093765c30.png"
    ></figure>
<p>collected to create a small-scale window embedding map W s ∈ R 25×D . By adjusting the window size, a middlescale window embedding map W m and large-scale window embedding map W l are also generated, and the similarity between these embedding maps and the text embedding z ∈ R D is calculated. The final similarity map M is generated by averaging the similarities calculated on three scales:</p>
<!-- formula-not-decoded -->
<p>We combined the template proposed by Jeong [12] with X and passed it through ET to generate z. Finally, we multiplied M and ˆ k to create P C:</p>
<!-- formula-not-decoded -->
<p>Here, fn fnorm represents min-max normalization, and ⊙ denotes element-wise multiplication. M was used after interpolation and reshaping to match the resolution of ˆ k . Because P C is created by integrating similarities from multiple scales, it is robust to object size and location, and operates effectively even in situations with multiple objects.</p>
<p>Grid Image Generation. Figure 3c illustrates the GIG method, which comprises two stages. In the multiscale grid generation stage, K is used to create grid images at different scales. Similar to the process described in WA, each frame of K is divided into multiple windows, and the windows at the same position are connected in a 2×2 grid format to create a single grid image. This process is defined as follows:</p>
<!-- formula-not-decoded -->
<p>Here, u i j refers to the i-th window created from k j , and g i refers to the i-th grid image. We defined the sets of grid images generated using small-, middle-, and large-scale windows as G s , G m , and G l , respectively.</p>
<p>In the grid image selection stage, the previously created sets are aggregated to generate G all . Then, using the same method as in KSM to select ˆ k, the grid image with the highest similarity to the text is chosen to generate T C:</p>
<!-- formula-not-decoded -->
<!-- formula-not-decoded -->
<p>The T C generated through this process represents the object movement over time within the same background, making it advantageous for action analysis and robust to various object sizes. An analysis of the window sizes used in the WA and GIG is presented in Section 4.5 .</p>

<h2 class="relative group">3.4. Anomaly Detection
    <div id="34-anomaly-detection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#34-anomaly-detection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Instead of tuning the LVLM, we propose a new prompt and context for performing context-aware VQA. The VQA results were used as anomaly scores to enable training-free zero-shot anomaly detection.</p>
<p>Prompt Design. Figure 4 illustrates the proposed prompt P. The prompt comprises three main components: &rsquo;task&rsquo;, &lsquo;consideration&rsquo;, and &lsquo;output&rsquo;. First, &rsquo;task&rsquo; defines the operation the LVLM should perform, specifically evaluating whether X is present in the image. Next, &lsquo;consideration&rsquo; specifies factors to be taken into account during evaluation, while &lsquo;output&rsquo; defines the format for presenting the evaluation results. To leverage the chain-of-thought [33] effect,</p>
<p>Figure 4. Proposed prompt for VQA</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_8c266fb996b0bc137f5100f09982790a1a4b57bb7d4824f3aa9c14b7d46b843d.png"
    ></figure>
<p>we instructed the model to provide brief reasoning along with the anomaly score, rounded to one decimal place. When conducting VQA using T C, an additional element, &lsquo;context&rsquo;, is inserted between &rsquo;task&rsquo; and &lsquo;consideration&rsquo; in the prompt. This context element conveys the meaning of the rows and columns of T C to the LVLM. We define the modified prompt as P ∗ . A comparative analysis of prompts is presented in supplementary materials.</p>
<p>Anomaly Scoring. The context serves as supplementary information to the image. However, because the LVLM accepts only a single image as input, it is challenging to utilize both the original and additional information simultaneously. To address this issue, we adopt a late fusion approach. Specifically, ˆ k , P C, and T C were used as the image inputs for the LVLM. The LVLM returns an anomaly score for each input, and these three scores are combined to compute the final ascore:</p>
<!-- formula-not-decoded -->
<p>Here, γ is a hyperparameter that adjusts the proportion of context reflected in ascore. A performance comparison experiment based on the hyperparameter tuning is presented in supplementary materials.</p>
<p>Consequently, even if the abnormal frame ˆ k receives a low score, the final anomaly score will be high if the additional information, P C or T C, is assigned a high score. This enabled accurate anomaly detection. Finally, to create frame-level anomaly scores, we duplicated the ascore for the length of each segment and then applied a temporal 1-D Gaussian filter for smoothing, following prior works [27 , 32].</p>
<p>Figure 5. Comparison between the VAD and C-VAD datasets</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000004_7a5aa1b14cec46a84714eef56048bbe01dbee060ea5944e5af748a754cf92e81.png"
    ></figure>

<h2 class="relative group">4. Experiments
    <div id="4-experiments" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#4-experiments" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">4.1. Datasets
    <div id="41-datasets" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#41-datasets" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure 5 illustrates the composition of the VAD and proposed C-VAD datasets. In conventional VAD datasets, videos are not categorized by an abnormal class type. In contrast, the proposed C-VAD datasets are organized by abnormal event type, with videos classified as positive or negative based on the presence of each abnormality. This categorization enables a precise evaluation of detection performance for specific types of abnormalities (e.g., bicycle). In this study, we validated the effectiveness of the proposed method on three VAD datasets: CUHK Avenue (Ave) [18], ShanghaiTech Campus (ShT) [20], and UBnormal (UB) [1] as well as two C-VAD datasets: Customizable-ShT (CShT) and Customizable-Ave (C-Ave). Further details of the datasets are provided in supplementary materials.</p>

<h2 class="relative group">4.2. Evaluation Criteria
    <div id="42-evaluation-criteria" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#42-evaluation-criteria" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To ensure consistency with previous VAD studies, the performance of the proposed model was evaluated using the micro-averaged area under the receiver operating characteristic curve (micro AUROC) metric. Specifically, the anomaly scores of all the frames in the dataset were aggregated, and the threshold of the anomaly score was progressively adjusted to compute the final evaluation.</p>

<h2 class="relative group">4.3. Results
    <div id="43-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#43-results" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Tables 1 and 2 present the evaluation results on the C-VAD datasets. The baseline, as described in Section 1, performs VQA at the frame level to compute anomaly scores. The proposed model achieved performance improvements of 9.88% and 13.65% compared to the baseline on the C-ShT and C-Ave datasets, respectively. Specifically, it showed improvements of 14.34% and 8.2% in the action class, and 3.25% and 21.98% in the appearance class, respectively.</p>
<p>When only KSM was applied to the baseline, the execution time decreased in proportion to the segment length, whereas the average performance remained similar to that</p>
<p>Table 1. Performance comparison on C-ShT dataset</p>
<table>
  <thead>
      <tr>
          <th>Category</th>
          <th>Class</th>
          <th>Baseline</th>
          <th>+KSM</th>
          <th>+KSM/PC</th>
          <th>+KSM/TC</th>
          <th>Proposed</th>
          <th>Improvement (%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td>Skateboarding</td>
          <td>61.3</td>
          <td>57.06</td>
          <td>57.79</td>
          <td>73.66</td>
          <td>73.66</td>
          <td>20.16</td>
      </tr>
      <tr>
          <td></td>
          <td>Throwing</td>
          <td>91.41</td>
          <td>72.82</td>
          <td>88.74</td>
          <td>82.53</td>
          <td>90.67</td>
          <td>-0.81</td>
      </tr>
      <tr>
          <td></td>
          <td>Running</td>
          <td>53.13</td>
          <td>51.93</td>
          <td>53.68</td>
          <td>59.77</td>
          <td>60.11</td>
          <td>13.14</td>
      </tr>
      <tr>
          <td>Action</td>
          <td>Loitering</td>
          <td>61.98</td>
          <td>51.96</td>
          <td>81.27</td>
          <td>76.94</td>
          <td>81.27</td>
          <td>31.12</td>
      </tr>
      <tr>
          <td></td>
          <td>Jumping</td>
          <td>82.84</td>
          <td>92.89</td>
          <td>92.91</td>
          <td>95.31</td>
          <td>95.31</td>
          <td>15.05</td>
      </tr>
      <tr>
          <td></td>
          <td>Falling</td>
          <td>78.31</td>
          <td>78.95</td>
          <td>79.24</td>
          <td>88.01</td>
          <td>88.01</td>
          <td>12.39</td>
      </tr>
      <tr>
          <td></td>
          <td>Fighting</td>
          <td>84.48</td>
          <td>91.18</td>
          <td>91.18</td>
          <td>98.06</td>
          <td>98.06</td>
          <td>16.07</td>
      </tr>
      <tr>
          <td></td>
          <td>Average</td>
          <td>73.35</td>
          <td>72</td>
          <td>77.83</td>
          <td>82.04</td>
          <td>83.87</td>
          <td>14.34</td>
      </tr>
      <tr>
          <td>Appearance</td>
          <td>Car</td>
          <td>88.72</td>
          <td>90.96</td>
          <td>91.46</td>
          <td>90.96</td>
          <td>91.46</td>
          <td>3.09</td>
      </tr>
      <tr>
          <td>Appearance</td>
          <td>Hand truck</td>
          <td>95.5</td>
          <td>98.2</td>
          <td>98.91</td>
          <td>99.2</td>
          <td>99.2</td>
          <td>3.87</td>
      </tr>
      <tr>
          <td>Appearance</td>
          <td>Bicycle</td>
          <td>72.36</td>
          <td>72.46</td>
          <td>78.47</td>
          <td>72.46</td>
          <td>78.47</td>
          <td>8.44</td>
      </tr>
      <tr>
          <td>Appearance</td>
          <td>Motorcycle</td>
          <td>88.04</td>
          <td>86.72</td>
          <td>86.72</td>
          <td>86.72</td>
          <td>86.72</td>
          <td>-1.5</td>
      </tr>
      <tr>
          <td>Appearance</td>
          <td>Average</td>
          <td>86.16</td>
          <td>87.09</td>
          <td>88.89</td>
          <td>87.34</td>
          <td>88.95</td>
          <td>3.25</td>
      </tr>
      <tr>
          <td>Overall Average</td>
          <td>Overall Average</td>
          <td>78.01</td>
          <td>77.48</td>
          <td>81.85</td>
          <td>83.97</td>
          <td>85.72</td>
          <td>9.88</td>
      </tr>
  </tbody>
</table>
<p>Table 2. Performance comparison on C-Ave dataset</p>
<table>
  <thead>
      <tr>
          <th>Category</th>
          <th>Class</th>
          <th>Baseline</th>
          <th>+KSM</th>
          <th>+KSM/PC</th>
          <th>+KSM/TC</th>
          <th>Proposed</th>
          <th>Improvement (%)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Action</td>
          <td>Throwing</td>
          <td>78.44</td>
          <td>80.13</td>
          <td>89.77</td>
          <td>82.4</td>
          <td>89.77</td>
          <td>14.44</td>
      </tr>
      <tr>
          <td>Action</td>
          <td>Running</td>
          <td>75.82</td>
          <td>77.67</td>
          <td>77.67</td>
          <td>77.9</td>
          <td>77.9</td>
          <td>2.74</td>
      </tr>
      <tr>
          <td>Action</td>
          <td>Dancing</td>
          <td>85.65</td>
          <td>72.28</td>
          <td>76.64</td>
          <td>91.92</td>
          <td>91.92</td>
          <td>7.32</td>
      </tr>
      <tr>
          <td>Action</td>
          <td>Average</td>
          <td>79.97</td>
          <td>76.69</td>
          <td>81.36</td>
          <td>84.07</td>
          <td>86.53</td>
          <td>8.2</td>
      </tr>
      <tr>
          <td>Appearance</td>
          <td>Too close</td>
          <td>57.23</td>
          <td>61.48</td>
          <td>61.48</td>
          <td>91.78</td>
          <td>91.78</td>
          <td>60.37</td>
      </tr>
      <tr>
          <td>Appearance</td>
          <td>Bicycle</td>
          <td>99.99</td>
          <td>99.84</td>
          <td>99.99</td>
          <td>99.93</td>
          <td>100</td>
          <td>0.01</td>
      </tr>
      <tr>
          <td>Appearance</td>
          <td>Average</td>
          <td>78.61</td>
          <td>80.66</td>
          <td>80.74</td>
          <td>95.86</td>
          <td>95.89</td>
          <td>21.98</td>
      </tr>
      <tr>
          <td>Overall Average</td>
          <td>Overall Average</td>
          <td>79.43</td>
          <td>78.28</td>
          <td>81.11</td>
          <td>88.79</td>
          <td>90.27</td>
          <td>13.65</td>
      </tr>
  </tbody>
</table>
<p>of the baseline. This is because the CLIP effectively selects representative frames for each segment, thereby compensating for the loss of temporal information. However, because it does not fully capture fine-grained spatio-temporal details, its performance significantly decreases for certain classes. Therefore, we address these issues using the proposed contextual information. First, using P C resulted in performance improvements of 5.64% and 3.62% compared to the KSM, as the LVLM focused on analyzing objects related to X. Additionally, applying T C led to performance improvements of 8.38% and 14.43% over the KSM, respectively, with particularly notable enhancements observed in the action class. This indicates that utilizing the temporal information provided in the grid image is essential for action analysis. Additional validations, such as FPS comparisons by segment length and performance evaluations across various LVLMs, are presented in supplementary materials.</p>

<h2 class="relative group">4.4. Qualitative Analysis
    <div id="44-qualitative-analysis" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#44-qualitative-analysis" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To analyze the effect of context-aware VQA, we present the visualization results for the anomaly scores and input frames in Figure 6. When P C was not applied, the bicycle object appeared smaller then the other objects, leading to a lower detection performance. Once P C is applied, the bicycle region is emphasized, thereby enhancing the object recognition capability of the LVLM. Similarly, without T C , the model misinterpreted fighting as standing, resulting in lower detection performance. Incorporating temporal information through T C improves the action recognition capability of the LVLM. These results demonstrate that contextaware VQA is more effective than the conventional VQA.</p>

<h2 class="relative group">4.5. Ablation Study
    <div id="45-ablation-study" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#45-ablation-study" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Key frames Selection. We conducted an ablation study on key frames selection from two perspectives: temporal</p>
<p>Figure 6. Anomaly score comparison and context visualization</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000005_31d66e8355c18a0543ff6a848e6c967571a91f00ea99fa41d24ea56f8d3525f3.png"
    ></figure>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000006_b8d327d27dcee3abafeae6fbe50e6082297777300a517252864a9d5fe12f9991.png"
    ></figure>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000007_105e7a6ba27e37363e34a4dcdaa03bb850cb025c8830dbb4f83f43987c0b1e03.png"
    ></figure>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000008_5ea4e8be54eb3faf1c12b22209bc80bd7774a06ff969b5e4c056c1aa01129e45.png"
    ></figure>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000009_e5d5dbb2cc79b0584f1764f0b2416c64c8a12a632c3d83644a3c1a235509a934.png"
    ></figure>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000010_67edad1410ed51accf5bcc05f0530aaf74cd74946f90c0df8538e5e18347e03b.png"
    ></figure>
<p>Table 3. Comparison on key frames selection method. RD, CP and Gr. indicate random, CLIP and grouping, respectively. * indicates testing without context. Act. and App. indicate action and appearance class, respectively.</p>
<table>
  <thead>
      <tr>
          <th>Key frames</th>
          <th>C-ShT</th>
          <th>C-ShT</th>
          <th>C-ShT</th>
          <th>C-Ave ATl</th>
          <th>C-Ave ATl</th>
          <th>C-Ave ATl</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Key frames</td>
          <td>Act.</td>
          <td>App.</td>
          <td>Total</td>
          <td>Act.</td>
          <td>App.</td>
          <td>Total</td>
      </tr>
      <tr>
          <td>RD*</td>
          <td>69.9</td>
          <td>84.0</td>
          <td>75.0</td>
          <td>66.4</td>
          <td>78.8</td>
          <td>71.3</td>
      </tr>
      <tr>
          <td>CP*</td>
          <td>72.0</td>
          <td>87.1</td>
          <td>77.5</td>
          <td>76.7</td>
          <td>80.7</td>
          <td>78.3</td>
      </tr>
      <tr>
          <td>RD</td>
          <td>80.0</td>
          <td>89.1</td>
          <td>83.3</td>
          <td>79.1</td>
          <td>92.3</td>
          <td>84.4</td>
      </tr>
      <tr>
          <td>CP</td>
          <td>81.2</td>
          <td>88.9</td>
          <td>84.0</td>
          <td>84.3</td>
          <td>81.2</td>
          <td>83.1</td>
      </tr>
      <tr>
          <td>Gr. → CP</td>
          <td>82.2</td>
          <td>88.8</td>
          <td>84.7</td>
          <td>83.9</td>
          <td>92.2</td>
          <td>87.2</td>
      </tr>
      <tr>
          <td>CP → Gr.</td>
          <td>83.9</td>
          <td>89.0</td>
          <td>85.7</td>
          <td>86.5</td>
          <td>95.9</td>
          <td>90.3</td>
      </tr>
  </tbody>
</table>
<p>uniformity and text alignment. The random method considers neither of these aspects, whereas the CLIP-based approach considers only text alignment. Selecting key frames using CLIP after grouping ensures text alignment but does not guarantee temporal uniformity. Applying grouping after CLIP resulted in evenly distributed key frames, thereby considering both temporal uniformity and text alignment. As shown in Table 3, incorporating both factors yielded the best performance for C-VAD, highlighting the critical role of temporal uniformity in action recognition. Furthermore, RD* and CP*, which do not utilize the contextual information, perform worse than the random method, which disregards both temporal uniformity and text alignment. This demonstrates the importance of leveraging the contextual information.</p>
<p>Window Size. Table 4 presents the experimental results based on the window sizes used in P C and T C. For the action classes, the best performance was achieved with the large window size in C-ShT and the middle window size in C-Ave. This indicates that middle or large window sizes are more effective in capturing temporal movements and interactions between multiple objects. For appearance classes, the optimal performance was observed with the small window size in C-ShT and the middle window size in C-Ave, suggesting that the appropriate window size varies depending on the dataset owing to differences in object sizes. To enhance the generalization performance of the model, we adopted an approach that utilized all three window sizes and found that incorporating them yielded the best overall performance.</p>

<h2 class="relative group">4.6. Comparison with SOTA
    <div id="46-comparison-with-sota" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#46-comparison-with-sota" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>To assess the effectiveness of AnyAnomaly in handling multiple text inputs, we conducted experiments on the VAD benchmark datasets. For performance evaluation, each anomaly class in the dataset was treated as X, and the maximum anomaly score among all computed scores was assigned to the corresponding segment. Table 5 presents a performance comparison with frame-centric VAD methods.</p>
<p>Table 4. Comparison on window size.</p>
<table>
  <thead>
      <tr>
          <th>Window Size</th>
          <th>C-ShT</th>
          <th>C-ShT</th>
          <th>C-ShT</th>
          <th>C-Ave ATtl</th>
          <th>C-Ave ATtl</th>
          <th>C-Ave ATtl</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Window Size</td>
          <td>Act.</td>
          <td>App.</td>
          <td>Total</td>
          <td>Act.</td>
          <td>App.</td>
          <td>Total</td>
      </tr>
      <tr>
          <td>small</td>
          <td>78.8</td>
          <td>90.6</td>
          <td>83.1</td>
          <td>84.7</td>
          <td>87.1</td>
          <td>85.7</td>
      </tr>
      <tr>
          <td>middle</td>
          <td>81.2</td>
          <td>89.0</td>
          <td>84.1</td>
          <td>87.5</td>
          <td>92.0</td>
          <td>89.3</td>
      </tr>
      <tr>
          <td>large</td>
          <td>82.1</td>
          <td>89.7</td>
          <td>84.9</td>
          <td>86.8</td>
          <td>86.4</td>
          <td>86.6</td>
      </tr>
      <tr>
          <td>all</td>
          <td>83.9</td>
          <td>89.0</td>
          <td>85.7</td>
          <td>86.5</td>
          <td>95.9</td>
          <td>90.3</td>
      </tr>
  </tbody>
</table>
<p>Table 5. Comparison with state-of-the-art VAD methods. * indicates testing without context.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Venue</th>
          <th>Zero-shot</th>
          <th>Ave</th>
          <th>ShT</th>
          <th>UB</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>AMMC-Net[6]</td>
          <td>AAAI 21</td>
          <td>✗</td>
          <td>86.6</td>
          <td>73.7</td>
          <td>-</td>
      </tr>
      <tr>
          <td>STEAL-Net[5]</td>
          <td>ICCV 21</td>
          <td>✗</td>
          <td>87.1</td>
          <td>73.7</td>
          <td>-</td>
      </tr>
      <tr>
          <td>MPN[21]</td>
          <td>CVPR 21</td>
          <td>✗</td>
          <td>89.5</td>
          <td>73.8</td>
          <td>-</td>
      </tr>
      <tr>
          <td>DLAN-AC[36]</td>
          <td>ECCV 22</td>
          <td>✗</td>
          <td>89.9</td>
          <td>74.7</td>
          <td>-</td>
      </tr>
      <tr>
          <td>UBnormal[1]</td>
          <td>CVPR 22</td>
          <td>✗</td>
          <td>-</td>
          <td>-</td>
          <td>68.5</td>
      </tr>
      <tr>
          <td>FPDM[34]</td>
          <td>ICCV 23</td>
          <td>✗</td>
          <td>90.1</td>
          <td>78.6</td>
          <td>62.7</td>
      </tr>
      <tr>
          <td>SLM[31]</td>
          <td>ICCV 23</td>
          <td>✗</td>
          <td>90.9</td>
          <td>78.8</td>
          <td>-</td>
      </tr>
      <tr>
          <td>USTN-DSC[37]</td>
          <td>CVPR 23</td>
          <td>✗</td>
          <td>89.9</td>
          <td>73.8</td>
          <td>-</td>
      </tr>
      <tr>
          <td>AnomalyRuler[35]</td>
          <td>ECCV 24</td>
          <td>✗</td>
          <td>89.7</td>
          <td>85.2</td>
          <td>71.9</td>
      </tr>
      <tr>
          <td>MULDE[24]</td>
          <td>CVPR 24</td>
          <td>✗</td>
          <td>-</td>
          <td>81.3</td>
          <td>72.8</td>
      </tr>
      <tr>
          <td>AED-MAE[28]</td>
          <td>CVPR 24</td>
          <td>✗</td>
          <td>91.3</td>
          <td>79.1</td>
          <td>58.5</td>
      </tr>
      <tr>
          <td>MA-PDM[40]</td>
          <td>AAAI 25</td>
          <td>✗</td>
          <td>91.3</td>
          <td>79.2</td>
          <td>63.4</td>
      </tr>
      <tr>
          <td>AccI-VAD[27]</td>
          <td>TMLR 25</td>
          <td>✗</td>
          <td>-</td>
          <td>-</td>
          <td>66.8</td>
      </tr>
      <tr>
          <td>AnyAnomaly*</td>
          <td>-</td>
          <td>✓</td>
          <td>81.4</td>
          <td>77.2</td>
          <td>73.1</td>
      </tr>
      <tr>
          <td>AnyAnomaly</td>
          <td>-</td>
          <td>✓</td>
          <td>87.3</td>
          <td>79.7</td>
          <td>74.5</td>
      </tr>
  </tbody>
</table>
<p>Table 6. Generalization performance comparison. Tr.: crossdomain training where models trained on one VAD dataset are evaluated on another. Few.: methods that adapt to the target domain using only a few training samples, Aux.: methods that utilize auxiliary datasets, *: since competitors did not perform crossdomain evaluations on ShT, we present their same-domain results instead.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Tr.</th>
          <th>Few.</th>
          <th>Aux.</th>
          <th>Ave</th>
          <th>ShT</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>STEAL-Net[5]</td>
          <td>✓</td>
          <td>✗</td>
          <td>✗</td>
          <td>54.3</td>
          <td>51.7</td>
      </tr>
      <tr>
          <td>Jigsaw[32]</td>
          <td>✓</td>
          <td>✗</td>
          <td>✗</td>
          <td>62.9</td>
          <td>59.3</td>
      </tr>
      <tr>
          <td>rGAN[19]</td>
          <td>✓</td>
          <td>✓</td>
          <td>✗</td>
          <td>76.6</td>
          <td>77.9*</td>
      </tr>
      <tr>
          <td>MPN[21]</td>
          <td>✓</td>
          <td>✓</td>
          <td>✗</td>
          <td>78.9</td>
          <td>73.8*</td>
      </tr>
      <tr>
          <td>zxVAD[3]</td>
          <td>✓</td>
          <td>✗</td>
          <td>✓</td>
          <td>82.2</td>
          <td>71.6*</td>
      </tr>
      <tr>
          <td>Shibao et al.[8]</td>
          <td>✓</td>
          <td>✗</td>
          <td>✓</td>
          <td>86.2</td>
          <td>78.7</td>
      </tr>
      <tr>
          <td>ZS CLIP[25]</td>
          <td>✗</td>
          <td>✗</td>
          <td>✗</td>
          <td>62.3</td>
          <td>60.9</td>
      </tr>
      <tr>
          <td>ZS ImageBind[9]</td>
          <td>✗</td>
          <td>✗</td>
          <td>✗</td>
          <td>64.5</td>
          <td>61.3</td>
      </tr>
      <tr>
          <td>LLaVA-1.5[16]</td>
          <td>✗</td>
          <td>✗</td>
          <td>✗</td>
          <td>67.4</td>
          <td>59.6</td>
      </tr>
      <tr>
          <td>Video-ChatGPT[22]</td>
          <td>✗</td>
          <td>✗</td>
          <td>✗</td>
          <td>76.9</td>
          <td>69.1</td>
      </tr>
      <tr>
          <td>AnyAnomaly</td>
          <td>✗</td>
          <td>✗</td>
          <td>✗</td>
          <td>87.3</td>
          <td>79.7</td>
      </tr>
  </tbody>
</table>
<p>Despite not being trained on VAD datasets, AnyAnomaly demonstrated a performance comparable to that of SOTA methods. Notably, it achieved a new SOTA performance of 74.5% on the UB dataset, which contains 29 diverse backgrounds and 22 abnormal event types, demonstrating the effectiveness of the proposed model in various environments. Furthermore, while LLM-based methods (e.g., AnomalyRuler) require rule generation and aggregation using a few normal samples, the proposed method achieves competitive performance solely through zero-shot inference, highlighting its practical applicability.</p>

<h2 class="relative group">4.7. Generalization Performance Comparison
    <div id="47-generalization-performance-comparison" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#47-generalization-performance-comparison" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Table 6 presents a comparison of the generalization performance of AnyAnomaly. Although STEAL-Net [5] and Jigsaw [32] achieved high accuracy in same-domain testing, their performance was significantly degraded in crossdomain settings. Specifically, on the Ave dataset, the performances of STEAL-Net and Jigsaw decreased as 87.1% → 54.3% and 92.2% → 62.9%, respectively. Similarly, on the ShT dataset, their performance decreased as 73.7% → 51.7% and 84.3% → 59.3%, respectively. This suggests that the existing OCC-based VAD models tend to overfit the training data, making them less effective when applied to new environments. For instance, &lsquo;Too close&rsquo; where an object is in close proximity to the camera is considered anomalous in the Ave dataset but normal in the ShT dataset. Consequently, OCC-based models trained on ShT struggle to detect such anomalies.</p>
<p>The zero- and few-shot VAD models designed for xVAD exhibited better generalization performance than the OCCbased models. However, few-shot models depend heavily on the number of K-shot samples, whereas zero-shot models require auxiliary datasets. Training-free methods using VLMs, such as ZS-CLIP and Video-ChatGPT, leverage strong image and video understanding capabilities, outperforming some VAD models. Nevertheless, their performance is still limited by domain gaps. In contrast, AnyAnomaly effectively overcomes these gaps by incorporating contextual information, achieving superior performance.</p>

<h2 class="relative group">5. Conclusion
    <div id="5-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#5-conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We propose AnyAnomaly, a novel approach that leverages the LVLM for universal VAD. AnyAnomaly effectively performs the C-VAD by incorporating a segmentlevel approach and context-aware VQA. This design reduces latency when processing large videos and minimizes the domain gap between the LVLM and VAD task. Despite being a zero-shot method, AnyAnomaly demonstrates competitive performance on benchmark datasets and holds promise for real-world VAD. Furthermore, because it operates without any training and enables anomaly detection in any video, it significantly improves accessibility in the VAD domain. We anticipate that AnyAnomaly will contribute substantially to VAD research and practical deployment.</p>

<h2 class="relative group">References
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>[1] Andra Acsintoae, Andrei Florescu, Mariana-Iuliana Georgescu, Tudor Mare, Paul Sumedrea, Radu Tudor Ionescu, Fahad Shahbaz Khan, and Mubarak Shah. Ubnormal: New benchmark for supervised open-set video anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 20143–20153, 2022. 2 , 5 , 8 , 11</li>
<li>[2] Sunghyun Ahn, Youngwan Jo, Kijung Lee, and Sanghyun Park. Videopatchcore: An effective method to memorize normality for video anomaly detection. In Proceedings of the Asian Conference on Computer Vision, pages 2179–2195, 2024. 1</li>
<li>[3] Abhishek Aich, Kuan-Chuan Peng, and Amit K RoyChowdhury. Cross-domain video anomaly detection without target domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 2579–2591, 2023. 3 , 8</li>
<li>[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425– 2433, 2015. 2</li>
<li>[5] Marcella Astrid, Muhammad Zaigham Zaheer, and Seung-Ik Lee. Synthetic temporal anomaly guided end-to-end video anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 207–214, 2021. 8</li>
<li>[6] Ruichu Cai, Hao Zhang, Wen Liu, Shenghua Gao, and Zhifeng Hao. Appearance-motion memory consistency network for video anomaly detection. In Proceedings of the AAAI conference on artificial intelligence, pages 938–946, 2021. 8</li>
<li>[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. 3</li>
<li>[8] Shibo Gao, Peipei Yang, and Linlin Huang. Scene-adaptive svad based on multi-modal action-based feature extraction. In Proceedings of the Asian Conference on Computer Vision , pages 2471–2488, 2024. 8</li>
<li>[9] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15180–15190, 2023. 8</li>
<li>[10] Seungkyun Hong, Sunghyun Ahn, Youngwan Jo, and Sanghyun Park. Making anomalies more anomalous: Video anomaly detection using a novel generator and destroyer. IEEE Access, 2024. 1</li>
<li>[11] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Pro-</li>
</ul>
<ol start="12">
<li>ceedings of the IEEE conference on computer vision and pattern recognition, pages 2462–2470, 2017. 2</li>
</ol>
<ul>
<li>
<p>[12] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer. Winclip: Zero/few-shot anomaly classification and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19606–19616, 2023. 3 , 4</p>
</li>
<li>
<p>[13] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13700– 13710, 2024. 3 , 11 , 12 , 13</p>
</li>
<li>
<p>[14] Dongha Lee, Sehun Yu, and Hwanjo Yu. Multi-class data description for out-of-distribution detection. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pages 1362–1370, 2020. 1</p>
</li>
<li>
<p>[15] Kijung Lee, Youngwan Jo, Sunghyun Ahn, and Sanghyun Park. Mdvad: Multimodal diffusion for video anomaly detection. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 121–133. Springer, 2025. 1</p>
</li>
<li>
<p>[16] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296–26306, 2024. 8</p>
</li>
<li>
<p>[17] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Future frame prediction for anomaly detection–a new baseline. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6536–6545, 2018. 1 , 2</p>
</li>
<li>
<p>[18] Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event detection at 150 fps in matlab. In Proceedings of the IEEE international conference on computer vision, pages 2720–2727, 2013. 5 , 11</p>
</li>
<li>
<p>[19] Yiwei Lu, Frank Yu, Mahesh Kumar Krishna Reddy, and Yang Wang. Few-shot scene-adaptive anomaly detection. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16 , pages 125–141. Springer, 2020. 8</p>
</li>
<li>
<p>[20] Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse coding based anomaly detection in stacked rnn framework. In Proceedings of the IEEE international conference on computer vision, pages 341–349, 2017. 5 , 11</p>
</li>
<li>
<p>[21] Hui Lv, Chen Chen, Zhen Cui, Chunyan Xu, Yong Li, and Jian Yang. Learning normal dynamics in videos with meta prototype network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15425–15434, 2021. 8</p>
</li>
<li>
<p>[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12585–12602, 2024. 8</p>
</li>
<li>
<p>[23] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2794–2802, 2017. 2</p>
</li>
<li>
<p>[24] Jakub Micorek, Horst Possegger, Dominik Narnhofer, Horst Bischof, and Mateusz Kozinski. Mulde: Multiscale logdensity estimation via denoising score matching for video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18868–18877, 2024. 8</p>
</li>
<li>
<p>[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 3 , 8</p>
</li>
<li>
<p>[26] Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad S. Khan. Llava++: Extending visual capabilities with llama-3 and phi-3, 2024. 12 , 13</p>
</li>
<li>
<p>[27] Tal Reiss and Yedid Hoshen. An attribute-based method for video anomaly detection. Transactions on Machine Learning Research . 1 , 5 , 8</p>
</li>
<li>
<p>[28] Nicolae-C Ristea, Florinel-Alin Croitoru, Radu Tudor Ionescu, Marius Popescu, Fahad Shahbaz Khan, Mubarak Shah, et al. Self-distilled masked auto-encoders are efficient video anomaly detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15984–15995, 2024. 8</p>
</li>
<li>
<p>[29] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Scholkopf, Thomas Brox, and Peter Gehler. Towards to- ¨ ¨ tal recall in industrial anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14318–14328, 2022. 1</p>
</li>
<li>
<p>[30] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classifica- ¨ ¨ tion. In International conference on machine learning, pages 4393–4402. PMLR, 2018. 1</p>
</li>
<li>
<p>[31] Chenrui Shi, Che Sun, Yuwei Wu, and Yunde Jia. Video anomaly detection via sequentially learning multiple pretext tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10330–10340, 2023. 8</p>
</li>
<li>
<p>[32] Guodong Wang, Yunhong Wang, Jie Qin, Dongming Zhang, Xiuguo Bao, and Di Huang. Video anomaly detection by solving decoupled spatio-temporal jigsaw puzzles. In European Conference on Computer Vision, pages 494–511. Springer, 2022. 5 , 8</p>
</li>
<li>
<p>[33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022. 4 , 11</p>
</li>
<li>
<p>[34] Cheng Yan, Shiyu Zhang, Yang Liu, Guansong Pang, and Wenjun Wang. Feature prediction diffusion model for video anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5527–5537, 2023. 8</p>
</li>
<li>
<p>[35] Yuchen Yang, Kwonjoon Lee, Behzad Dariush, Yinzhi Cao, and Shao-Yuan Lo. Follow the rules: Reasoning for video anomaly detection with large language models. ArXiv , abs/2407.10299, 2024. 8</p>
</li>
<li>
<p>[36] Zhiwei Yang, Peng Wu, Jing Liu, and Xiaotao Liu. Dynamic local aggregation network with adaptive clusterer for anomaly detection. In European Conference on Computer Vision, pages 404–421. Springer, 2022. 8</p>
</li>
<li>
<p>[37] Zhiwei Yang, Jing Liu, Zhaoyang Wu, Peng Wu, and Xiaotao Liu. Video event restoration based on keyframes for video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14592–14601, 2023. 1 , 3 , 8</p>
</li>
<li>
<p>[38] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 3 , 11 , 12 , 13</p>
</li>
<li>
<p>[39] Jihun Yi and Sungroh Yoon. Patch svdd: Patch-level svdd for anomaly detection and segmentation. In Proceedings of the Asian conference on computer vision, 2020. 1</p>
</li>
<li>
<p>[40] Hang Zhou, Jiale Cai, Yuteng Ye, Yonghui Feng, Chenxing Gao, Junqing Yu, Zikai Song, and Wei Yang. Video anomaly detection with motion and appearance guided patch diffusion model. arXiv preprint arXiv:2412.09026, 2024. 8</p>
</li>
<li>
<p>[41] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 3 , 12 , 13</p>
</li>
</ul>

<h2 class="relative group">AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM
    <div id="anyanomaly-zero-shot-customizable-video-anomaly-detection-with-lvlm-1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#anyanomaly-zero-shot-customizable-video-anomaly-detection-with-lvlm-1" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">Supplementary Material
    <div id="supplementary-material" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#supplementary-material" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Table S1. Comparison on prompt tuning</p>
<table>
  <thead>
      <tr>
          <th>Prompt Tuning</th>
          <th>C-ShT</th>
          <th>C-Ave</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Baseline (simple)</td>
          <td>70.38</td>
          <td>67.58</td>
      </tr>
      <tr>
          <td>Baseline (+reasoning)</td>
          <td>71.58</td>
          <td>72.79</td>
      </tr>
      <tr>
          <td>Baseline (+reasoning, consideration)</td>
          <td>78.01</td>
          <td>79.43</td>
      </tr>
      <tr>
          <td>Proposed (simple)</td>
          <td>79.29</td>
          <td>74.01</td>
      </tr>
      <tr>
          <td>Proposed (+reasoning)</td>
          <td>79.79</td>
          <td>82.09</td>
      </tr>
      <tr>
          <td>Proposed (+reasoning, consideration)</td>
          <td>85.72</td>
          <td>90.27</td>
      </tr>
  </tbody>
</table>
<p>Table S2. Comparison on segment length</p>
<table>
  <thead>
      <tr>
          <th>Segment length</th>
          <th>C-ShT</th>
          <th>C-Ave</th>
          <th>FPS</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Baseline</td>
          <td>78.01</td>
          <td>79.43</td>
          <td>0.96</td>
      </tr>
      <tr>
          <td>8</td>
          <td>83.83</td>
          <td>83.96</td>
          <td>2.67</td>
      </tr>
      <tr>
          <td>16</td>
          <td>83.45</td>
          <td>87.45</td>
          <td>4.49</td>
      </tr>
      <tr>
          <td>24</td>
          <td>85.72</td>
          <td>90.27</td>
          <td>6.67</td>
      </tr>
      <tr>
          <td>32</td>
          <td>82.5</td>
          <td>85.94</td>
          <td>8.45</td>
      </tr>
  </tbody>
</table>
<p>chain-of-thought [33] effect by requiring a simple reason along with the anomaly score. This helps to break down the problem step-by-step, guiding the model to resolve complex issues more systematically. For example, the question &ldquo;Does the image include jumping? can be divided into two steps: 1. &ldquo;Is there an object related to jumping (e.g., a person)?&rdquo; and 2. &ldquo;Is the object performing a jumping action?&rdquo; This allows object-level image analysis, leading to more refined predictions. The consideration prompt encourages the assignment of a high score even when X is not central within the image. This prompt was introduced to address the issue where low scores are assigned simply because X exists but is not the central element. The effectiveness of this prompt tuning is compared and analyzed in Table S1 .</p>
<p>The simple prompt instructs the LVLM to output only the anomaly score, while adding reasoning prompt the model to perform reasoning during the score calculation process, and applying consideration prompt encourages the model to focus on the given text. Experimental results showed that using both reasoning and consideration prompt achieved the best performance, suggesting that when the LVLM includes reasoning in the process, it produces more accurate results and can respond more precisely to user instructions through consideration prompt.</p>

<h2 class="relative group">A. Experiment Details
    <div id="a-experiment-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-experiment-details" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">A.1. Dataset Details
    <div id="a1-dataset-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a1-dataset-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>VAD Dataset. We used the CUHK Avenue (Ave) [18], ShanghaiTech Campus (ShT) [20], and UBnormal (UB) [1] datasets. Ave comprises of videos captured by a single camera on a university campus, containing five types of abnormal events; throwing paper, running, dancing, approaching the camera (Too close) and bicycle. ShT is a campus CCTV dataset that includes 13 different background scenes and 11 types of abnormal events; such as bicycles, cars, fighting, and jumping. UB is a synthetic dataset generated using the Cinema4D software, encompassing 29 diverse background scenes, including indoor environments, sidewalks, and etc. It provides a total of 22 abnormal events, including not only challenging-to-detect events such as smoking and stealing but also complex scenarios such as driving outside the lane and people-car accidents.</p>
<p>C-VAD Dataset. We constructed the Customizable-ShT (C-ShT) and Customizable-Ave (C-Ave) datasets. C-ShT reorganizes the test data of ShT into 11 abnormal event types and assigns new labels to each type. For example, in the bicycle category, videos containing bicycles were assigned to positive, whereas all other videos were assigned to negative. The frame-level labels were set to 1 only for frames in which a bicycle appeared in the positive videos. C-Ave was constructed by reorganizing the test data of Ave into 5 abnormal event types, following the same labeling methodology as C-ShT.</p>

<h2 class="relative group">A.2. Implementation Details
    <div id="a2-implementation-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a2-implementation-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In a key experiment using the C-VAD datasets, we employed an efficient Chat-UniVi [13] 7B model, considering the balance between performance and speed. For the VAD dataset experiment, we utilized the effective MiniCPM-V [38] 8B model to achieve optimal performance and compared it with state-of-the-art (SOTA) models. The CLIP model used for key frames selection and context generation was ViT-B/32. For context generation, we adopted large, middle, and small window sizes of (120,120), (80,80), and (48,48), respectively. For C-Ave and Ave, the large window size was set to (240,240). All the experiments were conducted on a single NVIDIA GeForce RTX 3090 GPU.</p>

<h2 class="relative group">A.3. Prompt Details
    <div id="a3-prompt-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a3-prompt-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure S1 shows the detailed prompts used in the experiments. First, a reasoning prompt is designed to obtain the</p>
<p>Figure S1. Prompt details. The content written in the simple version is not utilized when applying reasoning.</p>
<p>Table S3. Comparison of different methods on various datasets</p>
<table>
  <thead>
      <tr>
          <th>Dataset</th>
          <th>Method</th>
          <th>Value</th>
          <th>AUC</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>w/o context  w/o tuning</td>
          <td>w/o context  w/o tuning</td>
          <td>81.4 84.4 873</td>
          <td>81.4</td>
      </tr>
      <tr>
          <td>w/o tuning</td>
          <td>1.0, 1.0, 1.0</td>
          <td>81.4 84.4 873</td>
          <td>81.4</td>
      </tr>
      <tr>
          <td>w/ tuning</td>
          <td>0.6, 0.3, 0.1</td>
          <td>81.4 84.4 873</td>
          <td>81.4</td>
      </tr>
      <tr>
          <td>w/o context  w/o tuning  w/ tuning</td>
          <td>-</td>
          <td>77.2</td>
          <td>77.2</td>
      </tr>
      <tr>
          <td>w/o tuning</td>
          <td>1.0, 1.0, 1.0</td>
          <td>77.2</td>
          <td>77.2</td>
      </tr>
      <tr>
          <td>w/ tuning  /</td>
          <td>0.5, 0.3, 0.2</td>
          <td>79.7</td>
          <td>77.2</td>
      </tr>
      <tr>
          <td>w/o context</td>
          <td>-</td>
          <td>73.1</td>
          <td>73.1 0 73.8</td>
      </tr>
      <tr>
          <td>w/o tuning</td>
          <td>1.0, 1.0, 1.0</td>
          <td>73.1</td>
          <td>73.1 0 73.8</td>
      </tr>
      <tr>
          <td>w/ tuning</td>
          <td>74.5</td>
          <td>73.1</td>
          <td>73.1 0 73.8</td>
      </tr>
  </tbody>
</table>

<h2 class="relative group">B. Additional Quantitative Evaluation
    <div id="b-additional-quantitative-evaluation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-additional-quantitative-evaluation" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">B.1. Segment length and FPS
    <div id="b1-segment-length-and-fps" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b1-segment-length-and-fps" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Table S2 presents the performance comparison and FPS based on different segment lengths. The baseline segment length was set to 1. It was observed that deriving anomaly scores at the segment level yields superior performance compared to the baseline, which relies on a single frame. The highest AUC performance was achieved when the segment length is set to 24, reaching 85.72% and 90.27% for C-ShT and C-Ave, respectively. However, excessively long segment length introduces irrelevant information into the temporal context, leading to a decrease in accuracy. Fur- thermore, performing VAD at the segment level resulted in a 594% improvement in the FPS compared with the baseline.</p>

<h2 class="relative group">B.2. Hyperparameter Tuning
    <div id="b2-hyperparameter-tuning" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b2-hyperparameter-tuning" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We tuned the three hyperparameters γ1, γ2, andγ3 used for the final anomaly score calculation for each VAD dataset. Each hyperparameter controls the influence of the anomaly score derived from the frame, position, and temporal contexts. As shown in Table S3, the optimal hyperparameter values vary across datasets owing to differences in object sizes and abnormal events. Additionally, comparing w/o context, which does not utilize context information, and w/o tuning, where all hyperparameters were set to the same value, we observed performance improvements of 3.0%, 2.2%, and 0.7%, even without hyperparameter tuning. In contrast, the performance differences owing to hyperparameter tuning were 2.9%, 0.3%, and 0.7%, respectively. This demonstrates the effectiveness of our proposed approach in utilizing context information in VAD and proves that it achieves a strong generalization performance even without hyperparameter tuning.</p>

<h2 class="relative group">B.3. Diverse LVLM Comparison
    <div id="b3-diverse-lvlm-comparison" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b3-diverse-lvlm-comparison" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Table S4 presents the results for C-ShT and C-Ave when using various LVLMs. We evaluated the performances of four SOTA LVLMs: Chat-UniVi [13], MiniGPT-4 [41], MiniCPM-V [38], and LLAVA++ [26]. All experiments</p>
<p>Table S4. Comparison of diverse LVLMs. The model highlighted in blue represents the most efficient model for the C-VAD task, while the one highlighted in purple indicates the most effective model.</p>
<table>
  <thead>
      <tr>
          <th>LVLM</th>
          <th>Pre-trained</th>
          <th>C-ShT</th>
          <th>C-ShT</th>
          <th>C-Ave</th>
          <th>C-Ave</th>
          <th>FPS</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>LVLM</td>
          <td>Pre-trained</td>
          <td>w/o context</td>
          <td>Proposed</td>
          <td>w/o context</td>
          <td>Proposed</td>
          <td></td>
      </tr>
      <tr>
          <td>Chat-UniVi[13]</td>
          <td>Chat-UniVi-7B</td>
          <td>77.5</td>
          <td>85.7</td>
          <td>78.3</td>
          <td>90.3</td>
          <td>6.67</td>
      </tr>
      <tr>
          <td>MiniGPT-4[41]</td>
          <td>LLaMA-2 Chat 7B</td>
          <td>54.0</td>
          <td>67.4</td>
          <td>53.9</td>
          <td>55.3</td>
          <td>1.26</td>
      </tr>
      <tr>
          <td>MiniCPM-V[38]</td>
          <td>MiniCPM-Llama3-V-2 5 (8B)</td>
          <td>87.7</td>
          <td>90.1</td>
          <td>86.3</td>
          <td>91.0</td>
          <td>1.36</td>
      </tr>
      <tr>
          <td>LLAVA++[26]</td>
          <td>LLaVA-Meta-Llama-3-8B-Instruct-FT</td>
          <td>73.3</td>
          <td>82.8</td>
          <td>59.0</td>
          <td>69.4</td>
          <td>7.25</td>
      </tr>
  </tbody>
</table>
<p>Figure S2. Example of complementarity between position and temporal context. The first example highlights the importance of position context and the second example emphasizes the importance of temporal context.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000011_642df7a5c0b0fcc0600652d51093c52863fa3b3b498e1398bc4bf842ac8f529f.png"
    ></figure>
<p>were conducted using the default settings, and &lsquo;Pre-trained&rsquo; refers to the names of the pre-trained model weights. The experimental results demonstrate that incorporating the proposed context-aware VQA improves the performance of all LVLMs. Specifically, the use context-aware VQA leads to improvements ranging from 2.6% to 24.8%. Notably, even MiniCPM, which achieved the best performance without context-aware VQA, and showed additional improvements of 2.7% and 5.4% for C-ShT and C-Ave, respectively, when context-aware VQA was applied. This confirms that leveraging the proposed context-aware VQA is effective for C-VAD. Additionally, we observed that ChatUniVi, with an FPS of 6.67, was the most efficient model, whereas MiniCPM-V achieved the highest performance on both datasets, scoring 90.1% and 91.0%, respectively. Therefore, as mentioned in Appendix A.1, Chat-UniVi was used for the C-VAD experiments and MiniCPM-V was used for the VAD dataset experiments.</p>

<h2 class="relative group">C. Additional Qualitative Evaluation
    <div id="c-additional-qualitative-evaluation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-additional-qualitative-evaluation" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">C.1. Context Complementarity
    <div id="c1-context-complementarity" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c1-context-complementarity" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In this section, we explain the complementarity between P C and T C in context-aware VQA. Figure S2 visualizes the key frame of a specific segment along with the images generated using WA and GIG for of P C and T C. We also present the results of a context-aware VQA that utilizes these contexts.</p>
<p>In the first row, when the text input was &lsquo;bicycle&rsquo;, P C successfully identified the bicycle via WA, yielding a score of 1.0. However, the temporal context suffers from a cropping effect due to motion over time, resulting in a lower score of 0.5. In the second row, when the text input is &lsquo;jumping,&rsquo; the attention result from WA fails to accurately locate the &lsquo;jumping&rsquo; person. Additionally, because of the lack of temporal information, P C was unable to recognize the jumping action, resulting in a score of 0.0. In contrast, T C captured the entire jumping action over time, achieving a score of 0.9.</p>
<p>These results demonstrate that the proposed P C, which focuses on the object appearance, and T C, which leverages temporal information, are complementary. By integrating</p>
<p>Figure S3. Anomaly detection in diverse scenarios. Various abnormal events can emerge over time.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000012_2611dc632b7ff4f844100e528628d7f52f83aadd741bc63e4138294f454296b4.png"
    ></figure>
<p>both approaches, we enable an effective generalization of the VAD.</p>

<h2 class="relative group">C.2. Anomaly Detection in Diverse scenarios
    <div id="c2-anomaly-detection-in-diverse-scenarios" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c2-anomaly-detection-in-diverse-scenarios" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure S3 visualizes the results of VAD performed on videos containing multiple abnormal classes. The captions in each figure indicate the abnormal classes used in the corresponding video. We input the user-defined abnormal keywords as text individually to obtain the scores, and assigned the highest score as the anomaly score for the corresponding segment. As shown in the visualization results, the proposed AnyAnomaly enables VAD across various types of abnormal events. This demonstrates that AnyAnomaly can be effectively utilized even when the user aims to simultaneously detect multiple abnormal types.</p>

<h2 class="relative group">C.3. Anomaly Detection in Complex scenarios
    <div id="c3-anomaly-detection-in-complex-scenarios" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c3-anomaly-detection-in-complex-scenarios" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure S4 presents the visualization results of AnyAnomaly on complex scenarios. &lsquo;Key Frame&rsquo;, &lsquo;Position Context&rsquo;, and &lsquo;Temporal Context&rsquo; visualize ˆ k , P C, and T C, respectively. The text below each figure represents the LVLM output. These visualization results demonstrate that the proposed context-aware VQA, which utilizes P C and T C, is effective and contributes to improving VAD performance.</p>
<p>Additionally, in Figure S4d, we observe that the model can detect certain frames of &ldquo;walking drunk&rdquo; even without utilizing context information. This suggests that the strong visual reasoning capabilities of the LVLM enable VAD in complex scenarios. However, as shown in Figure S4a–S4c , relying solely on individual frames is insufficient for fully leveraging these reasoning abilities. Therefore, the proposed context-aware VQA approach is essential for effective VAD.</p>

<h2 class="relative group">D. Discussion
    <div id="d-discussion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-discussion" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">D.1. Comparison with traditional VAD
    <div id="d1-comparison-with-traditional-vad" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d1-comparison-with-traditional-vad" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Traditional VAD methods and our zero-shot C-VAD each have distinct strengths and limitations. Traditional VAD detects anomalies as deviations from learned normal patterns, requiring no prior knowledge of specific anomaly types and delivering strong performance within the trained environment. However, it often exhibits poor generalization to unseen environments and typically necessitates retraining. In contrast, C-VAD requires prior knowledge of anomaly types but removes the need for retraining or additional data collection even when the definition of &ldquo;normal&rdquo; varies across users or environments. This makes it a practical and cost-effective solution for real-world applications. We anticipate that, with continued advances in LVLM technology, the proposed C-VAD will become even more effective in the future.</p>

<h2 class="relative group">D.2. Limitation
    <div id="d2-limitation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d2-limitation" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Efficiency is crucial in VAD; therefore, we utilized the most lightweight model among the SOTA LVLMs and adopted a segment-level approach to significantly reduce the latency. However, our method still requires three inputs per segment (key frame, position context, and temporal context) and involves a reasoning process, which makes real-time analysis more challenging. Furthermore, when multiple abnormal events occur simultaneously, each event must be processed independently, which leads to a substantial increase in latency. Hence, our future studies will aim to enhance to the efficiency of the C-VAD in handling multiple abnormal events simultaneously.</p>

<h2 class="relative group">(a) Anomaly event: jaywalking
    <div id="a-anomaly-event-jaywalking" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-anomaly-event-jaywalking" aria-label="Anchor">#</a>
    </span>
    
</h2>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000013_b9f09ed545b08c722962e294a834d04fea9db3fd3a4b924ce40ed6203240a837.png"
    ></figure>

<h2 class="relative group">(b) Anomaly event: driving outside lane
    <div id="b-anomaly-event-driving-outside-lane" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-anomaly-event-driving-outside-lane" aria-label="Anchor">#</a>
    </span>
    
</h2>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000014_4150a43cbe781e6a398cde205a7f13bc50a2ace0158fbdf943f5d07d89f2c3d1.png"
    ></figure>

<h2 class="relative group">(c) Anomaly event: people and car accident
    <div id="c-anomaly-event-people-and-car-accident" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-anomaly-event-people-and-car-accident" aria-label="Anchor">#</a>
    </span>
    
</h2>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000015_ec30d45c1d1a4625d9aafd7b44c4840507a23293954e5c9d817503642aee9313.png"
    ></figure>

<h2 class="relative group">(d) Anomaly event: walking drunk
    <div id="d-anomaly-event-walking-drunk" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-anomaly-event-walking-drunk" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure S4. Anomaly detection in complex scenarios. Results with and without the inclusion of context are presented.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000016_93aa5b67431ebe2eee20f450924cdb6de28506564c588701a12125144587d014.png"
    ></figure>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/AnyAnomaly Zero-Shot Customizable Video Anomaly Detection with LVLM.md"
          data-oid-likes="likes_papers/AnyAnomaly Zero-Shot Customizable Video Anomaly Detection with LVLM.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/assistpda-an-online-video-surveillance-assistant-for-video-anomaly-prediction-detection-and-analysis/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/anomaly-led_prompting_learning_caption_generating_model_and_benchmark/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
