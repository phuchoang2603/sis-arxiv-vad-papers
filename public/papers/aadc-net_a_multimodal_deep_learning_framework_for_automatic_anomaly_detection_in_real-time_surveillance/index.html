<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/aadc-net_a_multimodal_deep_learning_framework_for_automatic_anomaly_detection_in_real-time_surveillance/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/aadc-net_a_multimodal_deep_learning_framework_for_automatic_anomaly_detection_in_real-time_surveillance/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/aadc-net_a_multimodal_deep_learning_framework_for_automatic_anomaly_detection_in_real-time_surveillance\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "10163"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>10163 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">48 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">AADC-Net: A Multimodal Deep Learning Framework for Automatic Anomaly Detection in Real-Time Surveillance
    <div id="aadc-net-a-multimodal-deep-learning-framework-for-automatic-anomaly-detection-in-real-time-surveillance" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#aadc-net-a-multimodal-deep-learning-framework-for-automatic-anomaly-detection-in-real-time-surveillance" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Duc Tri Phan , Member, IEEE, Vu Hoang Minh Doan , Jaeyeop Choi , Byeongil Lee , and Junghwan Oh , Senior Member, IEEE</p>
<p>Abstract— Automatic anomaly detection (AAD) has emerged as an advanced vision-based measurement method with diverse applications in healthcare and security. However, current AAD methods still face challenges related to data limitations and labeled-data imbalances, which limit the accuracy and reliability of AAD in real-life applications. Additionally, labeling and training large datasets for video anomaly detection (VAD) is computationally demanding and time-consuming. To address these challenges, this work introduces AADC-Net, a multimodal deep neural network for automated abnormal event detection and categorization. The key contributions of this research are as follows: 1) AADC-Net leverages pretrained large language models (LLMs) and vision-language models (VLMs) to mitigate VAD dataset limitations and imbalances; 2) a pretrained object detection model [DEtection TRansformer (DETR)] is integrated for visual feature extraction, eliminating the need for bounding box supervision; 3) the experimental results demonstrate the state-of-the-art (SOTA) performance of the proposed AADC-Net with an area under the curve (AUC) of 83.2% and an average precision (AP) of 83.8% on the public UCF-Crime and XDViolence datasets, respectively; and 4) additionally, AADC-Net can be integrated into existing video surveillance systems, such as those in smart gyms and healthcare facilities, to automatically detect anomalies in real time with minimal supervision, enhancing security, monitoring, and reducing labor costs while minimizing human error. In summary, our results demonstrate that AADC-Net not only achieves high accuracy in anomaly detection but also provides a practical solution for real-world surveillance applications.</p>
<p>Received 30 December 2024; revised 17 February 2025; accepted 25 February 2025. Date of publication 31 March 2025; date of current version 16 April 2025. This work was supported by the National Research Foundation of Korea (NRF) Grant funded by the Korean Government (MSIT) under Grant 2022R1A5A8023404. The Associate Editor coordinating the review process was Dr. Sudao He. (Duc Tri Phan and Vu Hoang Minh Doan contributed equally to this work.) (Corresponding authors: Byeongil Lee; Junghwan Oh.)</p>
<p>Duc Tri Phan was with the Department of Biomedical Engineering, Pukyong National University, Busan 48513, South Korea. He is now with the Early Mental Potential and Wellbeing Research Centre (EMPOWER), Nanyang Technological University, Singapore 639798 (e-mail: ductri.phan@ ntu.edu.sg).</p>
<p>Vu Hoang Minh Doan and Jaeyeop Choi are with the Smart GymBased Translational Research Center for Active Senior&rsquo;s Healthcare, Pukyong National University, Busan 48513, South Korea (e-mail: doanvuhoangminh@ gmail.com; <a
  href="mailto:jaeyeopchoi@pknu.ac.kr">jaeyeopchoi@pknu.ac.kr</a>).</p>
<p>Byeongil Lee is with the Digital Healthcare Research Center, Institute of Information Technology and Convergence, Pukyong National University, Busan 48513, South Korea (e-mail: <a
  href="mailto:bilee@pknu.ac.kr">bilee@pknu.ac.kr</a>).</p>
<p>Junghwan Oh is with Ohlabs Corporation, Busan 48513, South Korea, and also with Industry 4.0 Convergence Bionics Engineering and the Department of Biomedical Engineering, Pukyong National University, Busan 48513, South Korea (e-mail: <a
  href="mailto:jungoh@pknu.ac.kr">jungoh@pknu.ac.kr</a>).</p>
<p>Digital Object Identifier 10.1109/TIM.2025.3551832</p>
<p>Index Terms— Automatic anomaly detection (AAD), real-time surveillance, video anomaly detection (VAD), vision-based methods, vision-language model (VLM).</p>

<h2 class="relative group">I. INTRODUCTION
    <div id="i-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#i-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>V IDEO anomaly detection (VAD) is a computational tool used to detect unusual or abnormal events within video sequences, contributing to automated monitoring systems [1] . These abnormal patterns could include unusual movements, behaviors, or any irregular and unexpected activities [2]. More specifically, VAD aims to determine if each given video frame contains any anomalies [3]. In real life, monitoring abnormal activities manually is a challenging task because they rarely occur compared to normal ones. Therefore, developing automated VAD techniques is essential for minimizing unnecessary workload and increasing the efficiency of real-time surveillance and security systems.</p>
<p>VAD has been extensively studied and employed in real-world scenarios due to its practical benefits [4] , [5] , [6] , [7]. In surveillance and security, VAD can help detect intruders in restricted areas and identify suspicious activities in public places, such as violent or terrorist acts [8] , [9] , [10] . In healthcare, it can monitor patients for abnormal movements or behaviors and detect falls or medical emergencies in patient care services, thereby reducing the workload of nurses and doctors [11] , [12]. VAD is also utilized in industrial and manufacturing settings for detecting equipment malfunctions and identifying safety violations [13] , [14] , [15] , [16]. Current research on VAD in smart gyms primarily focuses on monitoring workout routines, such as weight lifting, to ensure proper form [17]. Despite its limitations in implementation within workouts, VAD shows several promising applications for enhancing personalized training and feedback, safety monitoring, and identifying abnormal activities.</p>
<p>Over the past few years, deep learning approaches have been widely used for VAD applications. Convolutional neural networks (CNNs) and long short-term memory networks (LSTMs) excel in learning complex patterns and hierarchical features from raw video data, making them ideal for detecting anomalies [18] , [19]. Their ability to describe strongly nonlinear spatial and temporal relationships without manual feature engineering is a significant advantage, allowing them to adapt to various scenarios [20]. However, their limitations include the requirement of large datasets for training and significant</p>
<p>1557-9662 © 2025 IEEE. All rights reserved, including rights for text and data mining, and training of artificial intelligence and similar technologies. Personal use is permitted, but republication/redistribution requires IEEE permission.</p>
<p>See <a
  href="https://www.ieee.org/publications/rights/index.html"
    target="_blank"
  >https://www.ieee.org/publications/rights/index.html</a> for more information.</p>
<p>Fig. 1. Prediction scores from baseline VAD models and video descriptions using VLM and LLM models are employed for workout categorization and abnormal detection. On the score curve, the red dashed lines denote anomaly thresholds. The bottom shows the answers from our proposed video-LLM model for the VAD video description.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_7a858b5e43214e2f9633df05cdd7ee19b6f01fe0c917a3c0cdd6ac42fa51b292.png"
    ></figure>
<p>computational resources [21]. They can be challenging to interpret, often acting as black boxes without clear insight into how decisions are made [22]. Spatiotemporal feature-based methods, such as dynamic texture models, treat videos as a linear dynamic system over time [23] , [24]. These methods are well-known for detecting anomalies in dynamic scenes, such as abnormal crowd activities [25]. However, dynamic texture models can only effectively handle videos with smooth movements, are sensitive to noise, and require precise feature extraction [26] .</p>
<p>Despite various meaningful and innovative studies on VAD, several technical challenges remain in this field.</p>
<ul>
<li>
<ol>
<li>The effectiveness of VAD is highly dependent on the complexity of real-world scenes, such as weather conditions, lighting variations, and background activities [27] , [28] .</li>
</ol>
</li>
<li>
<ol start="2">
<li>Data scarcity is another challenge for VAD systems. Training data might not cover all possible abnormal events in social behaviors, leading to potential mispredictions by classical supervised models for detecting anomalies [29] .</li>
</ol>
</li>
<li>
<ol start="3">
<li>Moreover, the rarity of anomalies in daily life compared to normal activities causes an imbalance between positive and negative classes in the training dataset, resulting in low precision for minority class predictions [30] .</li>
</ol>
</li>
<li>
<ol start="4">
<li>Besides that, the data collection and labeling process in VAD is time-consuming, and deploying trained models for real-time video data prediction can be computationally intensive [31] .</li>
</ol>
</li>
</ul>
<p>To address these challenges, we proposed a deep learning model that integrates large language models (LLMs) and vision-language models (VLMs) for VAD applications.</p>
<p>LLMs and VLMs are renowned for their ability to process and comprehend complex data across a wide range of applications. LLMs provide transformative capabilities by enhancing how systems interact with video data in VAD [32] . VLMs enable a more comprehensive analysis of videos by integrating visual data from video frames with contextual information from associated textual data [33]. This multimodal approach combines visual signals from video frames with contextual information from textual data, enabling the detection of context-specific anomalies that might be missed by visual data alone. The large pretrained dataset of VLMs effectively targets the data scarcity and imbalance challenges of VAD applications. The proposed unsupervised models eliminate the need for time-consuming data labeling processes. LLMs and VLMs thus support more intuitive and effective anomaly detection by incorporating both visual and textual data to analyze and respond to complex situations in real time. Furthermore, by incorporating advanced object detection models and the Top-K selection mechanism, the impact of environmental changes on detection performance is minimized.</p>
<p>In summary, we propose a multimodal network based on LLM and VLM models for automatic abnormal event detection and categorization. As illustrated in Fig. 1, our anomaly detector aims to generate frame-level anomaly confidences, categories, localization, and video descriptions with only video-level annotations provided. The main contributions and advantages of the research are outlined as follows.</p>
<ul>
<li>
<ol>
<li>We present a novel unsupervised framework, namely AADC-Net, for abnormal event detection and categorization. The AADC-Net is based on LLM and VLM models to address the data limitations of VAD, resulting in enhanced accuracy for abnormal detection and classification.</li>
</ol>
</li>
<li>
<ol start="2">
<li>The pretrained object detection model [DEtection TRansformer (DETR)] was implemented for visual feature extraction and bounding box detection without supervision. Moreover, we employed a Top-K selection mechanism for abnormal detection in video frames and utilized the multi-instance learning (MIL)-Align mechanism to extend our approach beyond binary classification to handle multiple classes. We leveraged a pretrained LLM model to provide detailed video descriptions.</li>
</ol>
</li>
<li>
<ol start="3">
<li>The performance of our AADC-Net model was evaluated using two widely recognized metrics and compared against current state-of-the-art (SOTA) methods. Our model achieved an area under the curve (AUC) of 83.2% on the UCF-Crime dataset and an average precision (AP) of 83.8% on the XD-Violence dataset, demonstrating its effective classification capabilities.</li>
</ol>
</li>
<li>
<ol start="4">
<li>Finally, we present a framework utilizing our AADC-Net model as a video processing node. This framework is applied to a smart gym surveillance system to demonstrate the AADC-Net model&rsquo;s applicability in real-world scenarios.</li>
</ol>
</li>
</ul>
<p>The rest of this article is structured as follows. Section II provides a review of related work on VAD, focusing on the video-based VLM (VVLM) and video-based LLM (VLLM). Section III introduces the AADC network. Section IV presents experiments, results, an ablation study on public benchmark datasets, and the deployment of AADC-Net. Finally, Section V concludes and discusses the study.</p>

<h2 class="relative group">II. RELATED WORK
    <div id="ii-related-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#ii-related-work" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">A. Video Anomaly Detection
    <div id="a-video-anomaly-detection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-video-anomaly-detection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>VAD is a promising research area with various applications in security, surveillance, and beyond [3] , [6]. However, a major challenge remains in the limited availability of anomalous data and labels [30]. Recent reviews and surveys have highlighted advancements in VAD methodologies, emphasizing the evolution of techniques to address these challenges [34] , [35] , [36]. Researchers have recently explored weakly supervised VAD (WSVAD) techniques to overcome the limitations of traditional VAD methods. These WSVAD methods leverage videos with only normal or abnormal labels, relying on weak annotations at the video level. Sultani et al. [1] first established a large benchmark and introduced a lightweight network utilizing MIL mechanisms. A higher order context model combined with a margin-based MIL loss function proposed by Lv et al. [37] has further improved anomaly localization. Zhong et al. [38] employed graph convolutional networks (GCNs) to capture frame-level similarities and temporal relationships, while self-attention mechanisms utilized by Tian et al. [39] have demonstrated effectiveness in modeling global temporal contexts. Zhang et al. [40] explored the completeness and uncertainty of pseudo-labels.</p>
<p>Li et al. [41] proposed a transformer-based multisequence learning framework, while Huang et al. [42] introduced a transformer-based framework for temporal representation aggregation. Fine-grained anomaly detection techniques have emerged, distinguishing between various types of anomalous frames [43], and multihead network models have been designed to disentangle anomaly representations, with each component specializing in specific anomaly types [44] .</p>

<h2 class="relative group">B. Video-Based Large Language Models
    <div id="b-video-based-large-language-models" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-video-based-large-language-models" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>LLMs are making a remarkable impact in the field of video understanding [45]. VLLMs explore how LLMs can tackle multimodal problems, where information comes in various forms such as text, images, and video [45]. One key work is WebVid, a large dataset of short videos with corresponding text descriptions, introduced by Bain et al. [46] . Based on this dataset, Li et al. [47] improved image encoders to enable large models to grasp the visual content within videos. Su et al. [48] proposed multimodal encoders that allow models to handle multiple modalities. Zhang et al. [49] took a different approach, focusing on training fundamental models to comprehend both the visual and auditory aspects of videos. Ning et al. [50] proposed a benchmark system called Video-Bench to evaluate the capabilities of video-LLMs. Additionally, Wang et al. [51] introduced VidiL, a new model for creating VLMs that can handle various video-to-text tasks with minimal training data. For processing long videos, Weng et al. [52] offer a novel approach called LongVLM. It breaks down long videos into smaller segments, enabling the LLM to analyze the details of each part. Maaz et al. [53] introduced Video-ChatGPT, a conversational AI that can understand and discuss video content. Jin et al. [54] tackled the challenge of integrating image and video understanding into conversational LLMs with ChatUniVi. This method utilizes a dynamic visual token system and multiscale representation for efficient comprehension of both broad concepts and fine-grained details within videos.</p>

<h2 class="relative group">C. Video-Based Vision Language Models
    <div id="c-video-based-vision-language-models" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-video-based-vision-language-models" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>VVLMs have emerged as a trend in recent years [55]. This technique focuses on learning the connection between visual information and language by pretraining models on large-scale datasets [55]. One recent model attracting significant attention in the VAD applications is CLIP, a contrastive languageimage pretraining model [56]. CLIP4Clip by Luo et al. [57] demonstrates its effectiveness in video-text retrieval. Several studies, including those by Wang et al. [58], Lin et al. [59] , and Ni et al. [60], explore adopting CLIP for video recognition tasks. Lv et al. [37] built upon CLIP&rsquo;s visual features to develop a new framework unbiased multiple instance learning (UMIL) for unbiased anomaly detection, leading to improved WSVAD performance. Joo et al. [61] propose using CLIP&rsquo;s visual features to extract key representations from videos. They then leverage temporal self-attention (TSA) to analyze both short-term and long-term temporal dependencies and identify relevant video snippets. The application of CLIP extends further to the more complex tasks of video action localization, as demonstrated by</p>
<p>Nag et al. [62] and Ju et al. [63]. Ju et al. [63] even propose a foundational approach for efficiently adapting pretrained image-based CLIP models to general video understanding, highlighting its potential for broader video analysis applications.</p>

<h2 class="relative group">D. Vision-Language Models in Anomaly Detection
    <div id="d-vision-language-models-in-anomaly-detection" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-vision-language-models-in-anomaly-detection" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Recent advancements in VLMs have enhanced VAD by integrating visual and textual representations. Methods such as CLIP-TSA [61] and UMIL-CLIP [37] utilize CLIP-based embeddings for weakly supervised anomaly detection, while approaches like Video-ChatGPT [53] and VidIL [51] employ LLM-guided reasoning for video understanding. Despite the recent success of VLM-based approaches, these existing methods often lack fine-grained frame-level supervision or require extensive video-text annotations. Transformer-based models such as UniVi [54] and LongVLM [52] further improve temporal anomaly detection but require extensive computational resources. In contrast, AADC-Net introduces a novel multimodal fusion approach that integrates DETR-based object detection, CLIP features, and LLM-generated descriptions, enabling fine-grained anomaly categorization beyond binary classification. Unlike existing methods, AADC-Net utilizes LLMs for context-aware anomaly reasoning, reducing false positives and enhancing interpretability. Its efficient fusion mechanism ensures scalability, making it more suitable for real-time surveillance applications than resource-intensive transformer-based models.</p>

<h2 class="relative group">III. METHOD
    <div id="iii-method" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#iii-method" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">A. Problem Statement
    <div id="a-problem-statement" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-problem-statement" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>The defined issue can be formally described as follows. Given a collection of training videos V = {Vn Vn, Va Va }, where Vn Vn = {v i n |i = 1 , . . . , Nn Nn } represents the set of normal videos and Nn Nn is the total number of normal videos in the dataset. Similarly, Va Va = {v j a | j = 1 , . . . , Ma Ma } represents the set of abnormal videos and Ma Ma is the number of abnormal videos. For each video v ∈ Vn Vn ∪Va Va , it has a corresponding video-level category label c, where c ∈ L nor ∪ Labn. Here, L nor represents the set of normal categories, and Labn is the set of abnormal categories. Specifically, the model aims to predict an anomaly confidence score for each frame and identify the anomaly category along with workout activities in the input videos.</p>

<h2 class="relative group">B. Overall Framework
    <div id="b-overall-framework" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-overall-framework" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>AADC-Net leverages the capabilities of a pretrained VLM to excel at learning rich, high-dimensional representations of data [55]. The CLIP models are pretrained on vast multimodal datasets containing diverse visual and textual information, enabling them to develop robust feature representations [56] . These representations are capable of capturing subtle patterns that distinguish normal from anomalous instances, making them particularly effective for VAD tasks [62]. Furthermore, traditional vision-based methods often rely on pixel-level features or handcrafted rules, which struggle to capture the complex semantic context of events [61]. In contrast, AADCNet incorporates contextual learning from VLM, which allows it to associate visual features with textual concepts. This integration enables the system to infer whether an activity is anomalous based on a broader understanding of normal behavior patterns [51]. As a result, AADC-Net can detect subtle anomalies, such as unusual social interactions or contextual inconsistencies, which are often missed by traditional approaches.</p>
<p>Fig. 2 illustrates the AADC-Net framework for abnormal detection and categorization. Given a set of training videos V , AADC-Net first processes each video v ∈ V by splitting it into f frames. These input frames f ∈ R C×H×W , where C represents the number of channels, H stands for the height of the input frame, and W denotes the width of the input frame, are then fed into a DETR encoder to extract DETR features (FDETR) and perform novel bounding box detection. Subsequently, the input frames are cropped into smaller patches (Pbbox) based on the bounding box coordinates. These cropped patches are then processed by the CLIP image encoder 8CLIP-V to obtain visual CLIP features (F v ).</p>
<p>Next, the AADC-Net framework combines the DETR features with the visual CLIP features to obtain frame-level features fc fc with dimensions n × c, where n represents the number of video frames and c is the feature dimension. This combined feature representation is then passed through a multiscale temporal network (MTN) to effectively capture temporal dependencies at different scales. The resulting multiscale visual features are fed into an anomaly predictor to generate frame-level anomaly confidence scores. This pipeline is primarily used for the abnormal detection task.</p>
<p>For workout categorization, the visual CLIP features are fused with the anomaly confidence features to create videolevel features. AADC-Net then leverages the CLIP text encoder to generate textual features. By calculating the alignments between the video-level features and the textual features, the framework estimates the anomaly category. Additionally, AADC-Net incorporates a pretrained LLM model to generate informative video descriptions.</p>

<h2 class="relative group">C. Multiscale Temporal Network
    <div id="c-multiscale-temporal-network" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-multiscale-temporal-network" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>While CLIP is trained on massive image–text pairs, its direct application to videos presents challenges due to the temporal gap between the image and video domains. To address this, we introduce the MTN, designed to effectively capture long-range and short-range temporal dependencies for VAD.</p>
<p>MTN consists of two key components: 1) a three-layer pyramid dilated convolution (PDC) block for multiresolution feature extraction and 2) a TSA module for long-range temporal dependency modeling. The learned multiscale temporal features are then integrated with DETR and CLIP-extracted features, ensuring robust spatiotemporal anomaly detection.</p>
<p>The PDC block employs dilated convolutions to capture temporal variations at different scales, ensuring that both short-term and long-term dependencies are considered.</p>
<p>Given the feature fδ ∈ R T , the 1-D dilated convolution operation with kernel W
( (λ) W
(κ,δ) ∈ R W is defined as follows: κ ∈ {1 , . . . , D/4} , δ ∈ {1 , . . . , D} , λ ∈ {PDC1 , PDC 2, PDC 3 }, and W denotes the filter size</p>
<!-- formula-not-decoded -->
<p>Fig. 2. AADC-Net framework analyzes abnormal videos by identifying anomalies using DETR and CLIP features. It further categorizes workout types and describes abnormal events through text generated by LLMs.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_22cfe3f056d79d14acdc2cc063931c83f6125546fcb7e6d4e1638a70811d33c4.png"
    ></figure>
<p>Here, ∗ denotes the dilated convolution operator indexed by λ. The term f
k (λ) f
k ∈ R T represents the output features obtained after applying the dilated convolution along the temporal dimension. The dilation factors that correspond to {PDC1 , PDC 2, PDC 3 } are {1 , 2 , 4} .</p>
<p>We modified the TSA block to analyze the video sequence along the time dimension. This allows us to produce an attention map M ∈ R T×T to capture long-range interactions and relationships within the video sequence. The attention map is derived from a self-attention mechanism within the TSA module. The TSA module starts by applying a 1 × 1 convolution to reduce the spatial dimension of X ∈ R T×D to X
(c) ∈ RT×D/4
, where X
( c ) = Conv 1×1 (X), with T being the number of frames in the video sequence and D the feature dimension.</p>
<p>Next, three distinct 1 × 1 convolutional layers are applied to X
(c)
to generate X
( c 1) , X
(c2)
, and X
( c 3) ∈ R T×D/4 , where X
(ci) = Conv 1×1 (X
( c ) ) for i ∈ {1 , 2 , 3}. An attention map is then constructed as M = (X
( c 1) )(X
( c 2) ) T , which is used to produce X (c4) = Conv 1×1 (M X( c 3) ). A skip connection is added after this final 1 × 1 convolutional layer, as follows:</p>
<!-- formula-not-decoded -->
<p>The final output of the MTN is obtained by concatenating the outputs from both the PDC and MTN modules, resulting in X ¯ = [X
( λ) ]λ ∈L ∈ R T×D , where L = {PDC1 , PDC 2, PDC 3, TSA}. The final output of the MTN module, X ¯ , is combined with the original input features (denoted as X) via a skip connection to obtain F. This is done by</p>
<!-- formula-not-decoded -->
<p>Here, sθ (X) denotes the function that combines the concatenated outputs of the PDC and TSA modules (i.e., X ¯ ) with the original input features X . F is the output of the function sθ , which includes both the MTN module&rsquo;s processed features and the original features.</p>
<p>In anomaly detection, we input F into a binary classifier comprising a feed-forward network (FFN) layer, a fully connected (FC) layer, and a sigmoid activation function to compute the anomaly confidence score Sa Sa ∈ R n×1</p>
<!-- formula-not-decoded -->

<h2 class="relative group">D. Learnable Prompt
    <div id="d-learnable-prompt" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-learnable-prompt" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>In standard VLMs like CLIP, text labels typically consist of single words or short phrases (e.g., &ldquo;treadmill workout,&rdquo; &ldquo;bike workout,&rdquo; and &ldquo;bench press&rdquo;). However, such labels often lack the contextual depth needed to describe complex anomalous events in surveillance video data.</p>
<p>To bridge this gap and enhance the transferability of text embeddings, AADC-Net incorporates a learnable prompt mechanism that dynamically refines text embeddings by adding adaptive context tokens.</p>
<p>First, AADC-Net transforms the original text labels (e.g., &ldquo;treadmill workout,&rdquo; &ldquo;bike workout,&rdquo; and &ldquo;bench press&rdquo;) into class tokens using the CLIP tokenizer. This can be expressed as tinit = Tokenizer(Label), where Label denotes the discrete text label. Second, a learnable prompt, denoted by {c1 , . . . , c m } , is created. This prompt comprises m context tokens that provide additional context for the class token. In sentence token formation, the class token t init is strategically placed in the middle of the prompt sequence, forming a complete sentence token t p = {c1 , . . . , t init , . . . , cl}. This positioning aims to</p>
<p>leverage the surrounding context for a more comprehensive representation.</p>
<p>The sentence token t p is further enhanced by adding positional embeddings. These embeddings encode the order of words within the sentence, providing crucial information for the text encoder. Finally, the CLIP text encoder takes the enriched sentence token t p as input and generates a more robust text embedding t out ∈ R d .</p>
<p>Traditional CLIP-based models use fixed text embeddings, which may struggle to distinguish between normal and anomalous activities in complex video scenes. AADC-Net&rsquo;s learnable prompt mechanism enhances text embeddings by providing richer context, improving alignment with anomalies, bridging the text-video domain gap, and reducing reliance on manually labeled descriptions, enabling the model to learn anomalous characteristics autonomously.</p>

<h2 class="relative group">E. Detection Module
    <div id="e-detection-module" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#e-detection-module" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>DETR is an end-to-end object detector based on a transformer encoder–decoder architecture [64]. Unlike traditional detection frameworks, DETR eliminates the need for anchor boxes and nonmaximum suppression. Instead, it directly predicts the set of object-bounding boxes and class labels by utilizing a transformer-based model that processes the image as a whole. The pretrained object detection network processes the input image and generates a set of object queries corresponding to potential objects in the image. A key advantage of DETR is its use of a bipartite matching algorithm that associates the object queries with ground-truth objects during training. The query-based detection approach easily obtains the features corresponding to each detected instance, enabling efficient extraction of pairwise representations for interaction recognition.</p>
<p>The detection process in the DETR framework begins by passing the input image through the visual DETR encoder, which serves as the backbone for extracting feature representations. The encoder processes the input image and outputs two key types of features: bounding box features, b = (x , y, w, h) , where (x , y) represent the center coordinates of the bounding box and (w, h) represent its width and height, as well as the DETR features FD FDETR.</p>
<p>Using the bounding box coordinates, we extract smaller image patches, denoted as Pbbox. These image patches are then passed through a pretrained VLM CLIP to generate image embeddings, as shown in Fig. 2 .</p>
<p>The list of image patches is denoted as Lbbox, and the computation for extracting the image embeddings is summarized as follows:</p>
<!-- formula-not-decoded -->
<p>Next, we fuse the extracted visual features Fvis with the DETR features. This is done by performing an elementwise addition between the CLIP image features and the DETR embedding features. An FC layer, denoted by fFC, is applied to reduce the dimension of the visual features to match the dimensionality of the DETR features, and the two are then fused</p>
<!-- formula-not-decoded -->
<p>Here, X c ∈ R dt dtxt represents the final fused features, where dt dtxt is the target dimensionality of the combined feature vector, typically aligning with the text feature dimensionality in multimodal fusion tasks. The parameter δ refers to the weights of the FC layer fFC .</p>
<p>The fused features X c are then used in subsequent downstream tasks, such as interaction recognition, activity detection, and other multimodal recognition tasks that require both visual and semantic understanding of the input scene. This fusion process enables the system to harness the complementary strengths of both DETR&rsquo;s spatial reasoning capabilities and CLIP&rsquo;s rich, semantic image understanding, making it highly effective for tasks requiring both detailed visual information and semantic context.</p>

<h2 class="relative group">F. Large Language Model
    <div id="f-large-language-model" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#f-large-language-model" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This work explores the use of LLaMA, an LLM, for generating video descriptions [65]. These descriptions typically follow a question-and-answer format with a template structure. Here is an example demonstrating how AADC-Net leverages LLaMA to create video descriptions.</p>
<p>Question:</p>

<h2 class="relative group">### Human:
    <div id="-human" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#-human" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>&lt;Video&gt; [Video Tokens]</p>
<p>&lt;Video&gt; [Can you describe this video?]</p>

<h2 class="relative group">### AADC-Net:
    <div id="-aadc-net" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#-aadc-net" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>&lt;Normal/Abnormal&gt;</p>
<p>&lt;Workout /Abnormal Activities&gt;</p>

<h2 class="relative group">Answer:
    <div id="answer" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#answer" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">### Assistant:
    <div id="-assistant" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#-assistant" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>&lt;Yes, the video shows normal activities at the gym from 00:00 to 02:18 with participants in a dumbbell press session.&gt;</p>
<p>The question is first converted into textual embeddings using a pretrained LLM. These textual embeddings are then combined with the features extracted by AADC-Net. This combined representation provides LLaMA with a richer understanding of the video content. LLaMA utilizes this combined input to generate a textual description as the answer. This description conveys information about whether the video is normal or abnormal, the specific activities it showcases, and their duration. By incorporating LLaMA, AADC-Net gains the ability to generate informative video descriptions that complement its core functionalities of abnormal classification and workout activity categorization.</p>

<h2 class="relative group">G. Objective Function
    <div id="g-objective-function" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#g-objective-function" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>For abnormal detection, we build upon prior VAD works by employing a Top-K selection mechanism. This method identifies the K frames with the highest anomaly scores in both normal and abnormal videos. We then calculate the average anomaly score for both sets, denoted by Anormal and A abnormal , respectively. These averages are then fed into a sigmoid function σ to obtain video-level anomaly</p>
<p>predictions yˆ ˆ</p>
<!-- formula-not-decoded -->
<p>Finally, binary cross-entropy loss (Lbce) is computed between these predictions and the ground-truth labels (y) for classification</p>
<!-- formula-not-decoded -->
<p>where N represents the total number of videos.</p>
<p>In regard to class-specific categorization, we propose the MIL-Align mechanism to extend our approach beyond binary classification to handle multiple classes. This method utilizes aggregated video-level features (X v ) and textual category embeddings (E c = {e1 , . . . , e m }) to determine video-level classification. For each video, we choose the Top-K similarities between the video features and category embeddings using a distance metric. The average of these Top-K similarities represents the alignment score (si) between the video and the ith class. This process results in a vector S = {s1 , . . . , s m } , where each element represents the similarity between the video and a specific class.</p>
<p>Our goal is to maximize the similarity score between the video and its corresponding textual label (E y ∗ , where y ∗ denotes the ground-truth class) compared to other classes. To accomplish this, the multiclass prediction (φi) with respect to the ith class is first computed using a softmax function</p>
<!-- formula-not-decoded -->
<p>where γ represents the hyperparameter used for scaling. Finally, the multiclass classification loss L nce is computed using cross-entropy</p>
<!-- formula-not-decoded -->
<p>During training, we combine both loss functions Lbce , L nce to optimize the overall model performance. The total loss function (L) is simply the sum of these individual losses as follows:</p>
<!-- formula-not-decoded -->
<p>where λ is a hyperparameter that controls the weight of the classification loss.</p>

<h2 class="relative group">H. Compared Methods
    <div id="h-compared-methods" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#h-compared-methods" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We compared our AADC-Net with various advanced VAD approaches for a comprehensive analysis. Supervised and weakly supervised methods include Sultani et al. [1], which employ MIL for anomaly detection, MIST [66] with multipleinstance self-training, and Bayesian network + spatial-visual pattern (BN-SVP) [67] using submodular video partitioning for anomaly localization. Graph-based and self-attention methods such as GCN [38] utilize GCNs for spatial–temporal modeling, while RTFM [39] enhances anomaly detection through Robust Temporal Feature Magnitude learning. Contrastive and transformer-based approaches include generative one-shot detection of anomalies (GODS) [68] with The UCF-Crime dataset was initially designed for weakly supervised anomaly detection tasks. Our proposed AADC-Net method addresses this challenge and surpasses all unsupervised and supervised methods on UCF-Crime. Specifically, AADC-Net achieves an improvement of 24.39% in AUC compared to the unsupervised baselines (GODS and FSCN). While AADC-Net demonstrates slight improvements across all evaluation metrics when compared to existing VAD methAuthorized licensed use limited to: University of South Florida. Downloaded on September 01,2025 at 19:58:57 UTC from IEEE Xplore. Restrictions apply.</p>
<p>TABLE I FRAME-LEVEL AUC RESULTS ON UCF-CRIME DATASET</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>One-Class Discriminative Subspaces, fully self-contained network (FSCN) [69] using sparse coding networks for robust feature learning, and dual memory units for weakly-supervised anomaly detection (DMU) [70] integrating dual memory units for uncertainty regulation. Graph temporal learning for anomaly detection (GTL) [71] employs a generative Transformer model for long-range dependency modeling without labeled data. A diffusion-based feature prediction model future-prediction-based dual memory network (FPDM) [72] refines video representations to learn normal distributions. Finally, vision-language and multimodal methods such as CLIP-TSA [61] incorporate CLIP-assisted TSA for anomaly detection, while consistency-based self-supervised learning for temporal anomaly localization (CSL-TAL) [73] refines anomaly scores via self-supervised contrastive learning.</p>

<h2 class="relative group">IV. RESULTS
    <div id="iv-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#iv-results" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">A. Comparison With SOTA Methods
    <div id="a-comparison-with-sota-methods" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-comparison-with-sota-methods" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>This section presents frame-level AUC results on the UCFCrime [74] and XD-Violence datasets [75]. The UCF-Crime dataset is a real-world surveillance video dataset with nearly 1900 video clips collected from various online sources, encompassing 13 different types of real-world anomalies alongside some normal videos. The XD-Violence dataset is a large-scale collection containing 2800 video clips that focus specifically on violent actions and violence-related scenarios. We introduced a new VAD dataset, AN-Workout, constructed specifically to capture abnormal actions within gym scenarios. AN-Workout comprises 830 real-world surveillance videos from smart gyms, featuring 11 types of realistic anomalies, including actions like fighting, using a phone, smoking, talking, and drinking water. It also includes 14 normal workout activities such as treadmill use, dumbbell press, and deadlifting. The results are detailed in Tables I and II, respectively.</p>
<p>TABLE II FRAME-LEVEL AP RESULTS ON XD-VIOLENCE DATASET</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>TABLE III mAP UNDER IOU RESULTS ON XD-VIOLENCE DATASET</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>TABLE IV mAP UNDER IOU RESULTS ON UCF-CRIME DATASET</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>ods on both datasets, it surpasses them in achieving new SOTA performance. AADC-Net reaches 83.8% AP and 83.2% AUC on the XD-Violence and UCF-Crime datasets, respectively. Notably, AADC-Net outperforms the best competing methods, CLIP-TSA and DMU, by an increase of 2.1% and 1.7% in AP on XD-Violence. Similarly, it demonstrates better performance in AUC, achieving increases of 0.8% and 1.7% over CLIP-TSA and DMU on XD-Violence and UCF-Crime, respectively.</p>
<p>These results highlight the effectiveness of AADC-Net in frame-level anomaly detection and classification tasks on the UCF-Crime and XD-Violence datasets.</p>
<p>Tables III and IV explore AADC-Net&rsquo;s performance in anomaly classification tasks. Anomaly classification is more challenging compared to anomaly detection. It requires not only the identification of anomalies but also their accurate categorization while maintaining continuity in the detected segments. This requirement for both detection and classification introduces additional complexity.</p>
<p>Despite this challenge, AADC-Net surpasses even the SOTA&rsquo;s previous works on both the XD-Violence and UCFCrime datasets. AADC-Net achieves an improvement of 23.8% and 113.6% in AP on XD-Violence compared to AVVD and Sultani et al., respectively. This trend is consistent on the UCF-Crime dataset, where AADC-Net outperforms AVVD and Sultani et al. by 11.4% and 112.5% in AP, respectively.</p>
<p>TABLE V ABLATION STUDIES WITH DIFFERENT DESIGNED MODULES ON UCF-CRIME FOR ABNORMAL DETECTION</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>TABLE VI ABLATION STUDIES WITH DIFFERENT DESIGNED MODULES ON XD-VIOLENCE FOR ABNORMAL DETECTION</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>TABLE VII ABLATION STUDIES WITH DIFFERENT DESIGNED MODULES ON AN-WORKOUT FOR ABNORMAL DETECTION</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>These results demonstrate AADC-Net&rsquo;s effectiveness in handling the complexities of anomaly classification tasks. AADC-Net achieves superior performance in both anomaly detection and classification tasks, surpassing existing methods on the XD-Violence and UCF-Crime benchmark datasets.</p>

<h2 class="relative group">B. Ablation Studies
    <div id="b-ablation-studies" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-ablation-studies" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ol>
<li>
<p>Effectiveness of DETR Features: This section investigates the effectiveness of DERT features in improving anomaly detection performance. As detailed in Tables V– VII, DERT features consistently enhance detection across all datasets, regardless of whether the multi-scale temporal network (MSTN) module is included in the model. The inclusion of DERT features leads to performance improvements. Compared to models without DERT features, we observe an increase of 2.2% in AUC for the UCF-Crime dataset, 2.3% in AP for the XD-Violence dataset, and 3% in AUC and 2.3% in AP for the AN-Workout dataset. Furthermore, incorporating DERT features within the visual CLIP features used by AADC-Net leads to a consistent improvement in performance. This integration strengthens the AADC-Net model&rsquo;s overall anomaly detection capabilities.</p>
</li>
<li>
<p>Effectiveness of Multiscale Temporal Network: As discussed earlier, the MSTN module is designed to capture temporal relationships within video data, thereby enhancing class-agnostic anomaly detection capabilities. To evaluate its effectiveness, we conducted experiments and presented ablation studies in Tables V – VII. The results demonstrate that incorporating the MSTN module improves performance across various datasets (UCF-Crime, XD-Violence, and AN-Workout) and evaluation metrics (AUC and AP). Without the temporal modeling offered by MSTN, the baseline model achieves only 80.4% AP in the XD-Violence dataset and 86.2% AUC in UCF-Crime dataset. Incorporating the MSTN module results in clear improvements: a 2.3% increase in AUC for UCF-</p>
</li>
</ol>
<p>Authorized licensed use limited to: University of South Florida. Downloaded on September 01,2025 at 19:58:57 UTC from IEEE Xplore. Restrictions apply.</p>
<p>Fig. 3. Qualitative results of our AADC-Net model on the AN-Workout dataset. The blue curves represent the anomaly scores, while the red regions indicate detected abnormal temporal events.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_2f02ccd313e45b54864a3140e88d51fde9814247917d396345fcb10adbb6a94e.png"
    ></figure>
<p>Crime, a 4.2% increase in AP for XD-Violence, and a 3.79% increase in AUC and a 3.94% increase in AP for AN-Workout.</p>
<p>Overall, the combination of DETR features and the MSTN module boosts performance in terms of AP and AUC. This improvement can be attributed to two key factors.</p>
<ul>
<li>
<ol>
<li>DETR features capture object-level details and relationships within a frame, while the MSTN module focuses on capturing temporal information across video sequences.</li>
</ol>
</li>
<li>
<ol start="2">
<li>By incorporating both DETR features and the MSTN module, the model learns a more comprehensive feature representation for anomaly detection. This richer representation allows the model to make more distinctions between normal and abnormal events.</li>
</ol>
</li>
<li>
<ol start="3">
<li>Analysis of Cross-Dataset Ability: To assess the zero-shot learning capabilities of AADC-Net, we conducted experiments using a cross-dataset setup. We trained the model on one dataset and evaluated its performance on another. Specifically, we employed UCF-Crime and XD-Violence datasets, which share some categories but originate from entirely different sources. The evaluation results in Table VIII reveal two key findings.</li>
</ol>
</li>
<li>
<ol>
<li>The model achieves better performance when trained on all available data within the source dataset.</li>
</ol>
</li>
<li>
<ol start="2">
<li>The AADC-Net model achieves competitive performance even when evaluated on a dataset it was not trained on. This demonstrates the model&rsquo;s ability to learn patterns from one dataset and effectively apply them to categorize events from a different source.</li>
</ol>
</li>
</ul>
<p>TABLE VIII CROSS-DATA RESULTS ON UCF-CRIME AND XD-VIOLENCE</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>AP is 83.8%, and the AN-Workout AUC and AP reach 79.4% and 81.7%, respectively. Increasing K beyond 7 leads to marginal performance drops, likely due to the inclusion of less relevant frames, which introduce noise. Conversely, selecting a lower K value may exclude important anomaly frames, leading to reduced alignment accuracy and potentially missing key discriminative features.</p>
<p>We tested γ values of 0.5, 1.0, and 2.0 to observe their effects on video-level predictions. The optimal performance was achieved at γ = 1 . 0, where all datasets reached their peak performance metrics. Lower values (γ = 0 . 5) resulted in smoother probability distributions, reducing prediction confidence and promoting more uniform probabilities across classes. This smoothness could encourage more generalization but reduces the ability to distinguish between classes. Higher values (γ = 2 . 0) sharpened the distribution, increasing confidence for top predictions but potentially causing numerical instability or overfitting by making the model overly confident in its predictions.</p>
<p>For this work, optimal results were observed with K = 5 and γ = 1 . 0. This configuration ensures a balanced trade-off among prediction confidence, computational efficiency, and model stability, providing effective performance for practical deployment.</p>
<ul>
<li>
<ol start="4">
<li>Effectiveness of Hyperparameters K and γ : We conducted experiments by varying the K value among 3, 5, and 7, evaluating its impact on performance metrics across the UCFCrime, XD-Violence, and AN-Workout datasets. The results in Table IX indicate that the model achieves the best performance at K = 5, where the UCF-Crime AUC is 88.2%, XD-Violence C. Qualitative Results Fig. 3 presents a qualitative visualization of the AADC-Net model&rsquo;s performance on the AN-Workout dataset. The blue Authorized licensed use limited to: University of South Florida. Downloaded on September 01,2025 at 19:58:57 UTC from IEEE Xplore. Restrictions apply.</li>
</ol>
</li>
</ul>
<p>Fig. 4. Example of AADC-Net with video description. The orange boxes are the questions from humans. The blue boxes are the answers from the Video-LaMA and our AADC-Net.</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>TABLE IX ANALYSIS OF HYPERPARAMETERS K AND γ ACROSS DATASETS</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_025845cca42dcede0ca541ea0bff777610a2bff440670e633f69e277bdaa3242.png"
    ></figure>
<p>curves represent the anomaly scores, with red regions indicating detected abnormal temporal events. AADC-Net effectively identifies and classifies anomalies with high confidence while generating minimal false positives for normal events. Furthermore, the distinction between normal and abnormal regions is relatively clear.</p>
<p>Fig. 4 showcases an example of AADC-Net integrated with LLaMA for identifying normal and abnormal events alongside textual descriptions. This integration enhances AADC&rsquo;s capabilities, enabling a richer understanding of video content beyond anomaly detection and classification. AADC-Net leverages the pretrained LLaMA model to analyze video content in conjunction with feature extraction from the anomaly detection and classification branches. This combined approach facilitates a better comprehension of the video. As illustrated in Fig. 4, AADC-Net-LLaMA effectively detects anomalies (e.g., &ldquo;drinking water&rdquo;) and precisely locates their occurrences (e.g., &ldquo;from 00:15 to 00:40&rdquo;). Additionally, it provides detailed descriptions with durations. The integration of LLaMA enables AADC-Net to generate informative video descriptions, complementing its core functionalities of anomaly classification and workout activity categorization. This capability supports</p>
<p>Fig. 5. Confusion matrices of normal/abnormal categorization.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000004_59ca1dd21a257ffba0c85a770c75ee53c658a68fb2f2d6102e555672c896f23e.png"
    ></figure>
<p>various applications, including video summarization, activity understanding, and interactive human–computer dialogs.</p>
<p>Fig. 5 presents confusion matrices for normal/abnormal event classification on the AN-Workout dataset. While AADCNet&rsquo;s overall performance is promising, as demonstrated in Section IV, these confusion matrices highlight some limitations in identifying certain anomaly categories. This observation emphasizes the inherent challenges associated with one-class versus one-class anomaly detection (OVVAD) approaches. OVVAD models typically learn a representation of normal data and classify any significant deviation from this norm as an anomaly. While this strategy can be effective for general anomaly detection, it may struggle with accurately classifying specific types of anomalies, especially when normal and abnormal events share some characteristics. Further investigation into these limitations and potential improvements for fine-grained anomaly classification within AADC-Net is an ongoing focus of our research efforts.</p>

<h2 class="relative group">D. Model Deployment
    <div id="d-model-deployment" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-model-deployment" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We have deployed our AADC-Net in an automatic surveillance system for the smart gym application. The deployment framework is illustrated in Fig. 6. In this setup, a camera serves as the video source, streaming data through WebRTC clients to a Python-based WebRTC server that manages peerto-peer connections. The video streams are processed by our AADC-Net model to detect anomalies. Detected anomalies are logged with timestamps and detailed descriptions generated by our model. The entire system is integrated with a user-friendly Streamlit frontend, which provides a web-based interface for real-time monitoring and interaction, ensuring efficient and accurate anomaly detection for gym administrators.</p>
<p>Fig. 6. Model deployment in an automatic surveillance system for smart gym applications.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000005_8d4e8f02f03b8f12dd316b6305a0518c2842c8e7a5e59ece2385b8395f21a856.png"
    ></figure>
<p>To assess the practical feasibility of AADC-Net, we evaluated its inference speed, memory consumption, and computational resource requirements on both GPU and CPU setups. On an NVIDIA RTX 3090 (24-GB VRAM, CUDA 11.8), AADC-Net achieves 35 frames per second (frames/s) in batch processing, making it suitable for real-time surveillance at 30 frames/s, with a processing time of approximately 28.6 ms per frame and a memory usage of around 6-GB GPU memory, ensuring deployability on consumer-grade GPUs. On a CPU setup (Intel Core i912900K, 16 cores, 24 threads, 32-MB L3 cache), the model achieves 8 frames/s, which remains practical for offline batch analysis, with a processing time of approximately 125 ms per frame and a memory usage of around 14-GB RAM due to the absence of GPU acceleration. These analyses demonstrate that AADC-Net is highly feasible for real-time anomaly detection on GPU-powered systems and remains practical for offline analysis on high-performance CPUs, making it adaptable for surveillance and monitoring applications.</p>

<h2 class="relative group">V. CONCLUSION
    <div id="v-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#v-conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>One of the fundamental challenges in VAD is the scarcity of labeled data, as anomalies are rare and labor-intensive to annotate in real-world scenarios. VLMs help mitigate this issue by leveraging contrastive learning objectives, such as those used in CLIP. AADC-Net utilizes these objectives to discover meaningful features without explicit supervision, reducing its reliance on labeled training data. Additionally, in real-world detection tasks, the number of normal samples far exceeds the number of abnormal ones, leading to class imbalance issues that can bias supervised learning approaches. Pretrained VLMs act as a regularization mechanism, preventing the model from overfitting to the majority class (normal events). By aligning video frames with pretrained textual embeddings, the model learns robust, class-agnostic representations of anomalies.</p>
<p>This research introduces AADC-Net, a novel multimodal deep neural framework that addresses the challenges of data scarcity, imbalance, and computational intensity inherent in video anomaly detection (VAD). By integrating pretrained VLMs and LLMs, AADC-Net leverages their rich, pretrained knowledge to extract meaningful feature representations, reducing reliance on extensively labeled VAD data. This integration of VLMs and LLMs enhances the model&rsquo;s ability to discriminate between normal and anomalous events, improving both anomaly detection and categorization, especially in imbalanced datasets. Furthermore, AADC-Net incorporates the pretrained object detection model DETR to efficiently extract visual features without requiring manual bounding box annotations, making it robust in data-scarce scenarios. To refine anomaly detection, a Top-K selection mechanism identifies the most relevant video frames, while the MIL-Align mechanism enables multiclass anomaly categorization. Experimental results on public benchmarks (UCF-Crime and XD-Violence) and our AN-Workout dataset demonstrate AADC-Net&rsquo;s superior performance compared to SOTA methods. The model generates frame-level anomaly confidences, categories, localization, and video descriptions with minimal supervision.</p>
<p>One challenge is that the AADC-Net model is trained on datasets captured in controlled environments with fixed camera alignments, limiting its applicability in real-world scenarios with varying camera placements and angles. To address this, future work will focus on training and evaluating AADC-Net on datasets that include diverse camera angles, lighting conditions, and occlusions. We will also investigate domain adaptation techniques and incorporate spatial attention mechanisms to enhance the model&rsquo;s robustness and real-time performance in real-world settings. Additionally, we will evaluate its performance using metrics like recall at low intersection over union (IoU) thresholds and detection consistency across multiple views. In the long-term vision, the integration of vision-based methods into automatic anomaly detection (AAD) has the potential to develop intelligent monitoring systems by increasing efficiency and reducing the burden on human operators. In summary, AADC-Net provides a robust solution for overcoming the current challenges in VAD, showcasing superior performance with promising applications in smart gym and healthcare monitoring in the future.</p>

<h2 class="relative group">REFERENCES
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>
<p>[1] W. Sultani, C. Chen, and M. Shah, &ldquo;Real-world anomaly detection in surveillance videos,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 6479–6488.</p>
</li>
<li>
<p>[2] M. Zhang, T. Li, Y. Yu, Y. Li, P. Hui, and Y. Zheng, &ldquo;Urban anomaly analytics: Description, detection, and prediction,&rdquo; IEEE Trans. Big Data , vol. 8, no. 3, pp. 809–826, Jun. 2022.</p>
</li>
<li>
<p>[3] R. Nayak, U. C. Pati, and S. K. Das, &ldquo;A comprehensive review on deep learning-based methods for video anomaly detection,&rdquo; Image Vis. Comput., vol. 106, Jan. 2021, Art. no. 104078.</p>
</li>
<li>
<p>[4] U. A. Usmani, A. Happonen, and J. Watada, &ldquo;A review of unsupervised machine learning frameworks for anomaly detection in industrial applications,&rdquo; in Proc. Sci. Inf. Conf. Cham, Switzerland: Springer, Jan. 2022, pp. 158–189.</p>
</li>
<li>
<p>[5] X. Li, J. Jing, J. Bao, P. Lu, Y. Xie, and Y. An, &ldquo;OTB-AAE: Semisupervised anomaly detection on industrial images based on adversarial autoencoder with output-turn-back structure,&rdquo; IEEE Trans. Instrum. Meas., vol. 72, pp. 1–14, 2023.</p>
</li>
<li>
<p>[6] Y. Cui, Z. Liu, and S. Lian, &ldquo;A survey on unsupervised anomaly detection algorithms for industrial images,&rdquo; IEEE Access, vol. 11, pp. 55297–55315, 2023.</p>
</li>
<li>
<p>[7] A. Yang, X. Xu, Y. Wu, and H. Liu, &ldquo;Reverse distillation for continuous anomaly detection,&rdquo; IEEE Trans. Instrum. Meas., vol. 73, pp. 1–13, 2024.</p>
</li>
<li>
<p>[8] H. Yao and X. Hu, &ldquo;A survey of video violence detection,&rdquo; Cyber-Phys. Syst., vol. 9, no. 1, pp. 1–24, Jan. 2023.</p>
</li>
<li>
<p>[9] T. Saba, &ldquo;Real time anomalies detection in crowd using convolutional long short-term memory network,&rdquo; J. Inf. Sci., vol. 49, no. 5, pp. 1145–1152, Oct. 2023.</p>
</li>
<li>
<p>[10] K. Rezaee, S. M. Rezakhani, M. R. Khosravi, and M. K. Moghimi, &ldquo;A survey on deep learning-based real-time crowd anomaly detection for secure distributed video surveillance,&rdquo; Pers. Ubiquitous Comput. , vol. 28, no. 1, pp. 135–151, Feb. 2024.</p>
</li>
<li>
<p>[11] M. Kavitha, P. V. V. S. Srinivas, P. S. L. Kalyampudi, and S. Srinivasulu, &ldquo;Machine learning techniques for anomaly detection in smart healthcare,&rdquo; in Proc. 3rd Int. Conf. Inventive Res. Comput. Appl. (ICIRCA) , Sep. 2021, pp. 1350–1356.</p>
</li>
<li>
<p>[12] Y. Yang, Y. Xian, Z. Fu, and S. M. Naqvi, &ldquo;Video anomaly detection for surveillance based on effective frame area,&rdquo; in Proc. IEEE 24th Int. Conf. Inf. Fusion (FUSION), Nov. 2021, pp. 1–5.</p>
</li>
<li>
<p>[13] H. Yao et al., &ldquo;Scalable industrial visual anomaly detection with partial semantics aggregation vision transformer,&rdquo; IEEE Trans. Instrum. Meas. , vol. 73, pp. 1–17, 2024.</p>
</li>
<li>
<p>[14] K. Xiao, J. Cao, Z. Zeng, and W.-K. Ling, &ldquo;Graph-based active learning with uncertainty and representativeness for industrial anomaly detection,&rdquo; IEEE Trans. Instrum. Meas., vol. 72, pp. 1–14, 2023.</p>
</li>
<li>
<p>[15] J. Zhu, P. Yan, J. Jiang, Y. Cui, and X. Xu, &ldquo;Asymmetric teacher–student feature pyramid matching for industrial anomaly detection,&rdquo; IEEE Trans. Instrum. Meas., vol. 73, pp. 1–13, 2024.</p>
</li>
<li>
<p>[16] M. Carratù et al., &ldquo;A novel methodology for unsupervised anomaly detection in industrial electrical systems,&rdquo; IEEE Trans. Instrum. Meas. , vol. 72, pp. 1–12, 2023.</p>
</li>
<li>
<p>[17] M. S. Sapwan, Z. Ibrahim, Z. Mabni, and N. L. Adam, &ldquo;Detection and classification of weightlifting form anomalies using deep learning,&rdquo; J. Positive School Psychol., vol. 6, no. 3, pp. 8530–8537, 2022.</p>
</li>
<li>
<p>[18] Y. He, H. Yang, and Z. Yin, &ldquo;Adaptive context-aware distillation for industrial image anomaly detection,&rdquo; IEEE Trans. Instrum. Meas. , vol. 73, pp. 1–15, 2024.</p>
</li>
<li>
<p>[19] R. Sharma and A. Sungheetha, &ldquo;An efficient dimension reduction based fusion of CNN and SVM model for detection of abnormal incident in video surveillance,&rdquo; J. Soft Comput. Paradigm, vol. 3, no. 2, pp. 55–69, May 2021.</p>
</li>
<li>
<p>[20] S. Fadl, Q. Han, and Q. Li, &ldquo;CNN spatiotemporal features and fusion for surveillance video forgery detection,&rdquo; Signal Process., Image Commun. , vol. 90, Jan. 2021, Art. no. 116066.</p>
</li>
<li>
<p>[21] L. Alzubaidi et al., &ldquo;Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions,&rdquo; J. Big Data, vol. 8, no. 1, pp. 1–74, Mar. 2021.</p>
</li>
<li>
<p>[22] Y. Liang, S. Li, C. Yan, M. Li, and C. Jiang, &ldquo;Explaining the blackbox model: A survey of local interpretation methods for deep neural networks,&rdquo; Neurocomputing, vol. 419, pp. 168–182, Jan. 2021.</p>
</li>
<li>
<p>[23] P. Wu, W. Wang, F. Chang, C. Liu, and B. Wang, &ldquo;DSS-Net: Dynamic self-supervised network for video anomaly detection,&rdquo; IEEE Trans. Multimedia, vol. 26, pp. 1–13, 2023.</p>
</li>
<li>
<p>[24] M. Lovanshi and V. Tiwari, &ldquo;Human skeleton pose and spatio-temporal feature-based activity recognition using ST-GCN,&rdquo; Multimedia Tools Appl., vol. 83, no. 5, pp. 12705–12730, Jun. 2023.</p>
</li>
<li>
<p>[25] M. George, B. R. Jose, and J. Mathew, &ldquo;Abnormal activity detection using shear transformed spatio-temporal regions at the surveillance network edge,&rdquo; Multimedia Tools Appl., vol. 79, nos. 37–38, pp. 27511–27532, Oct. 2020.</p>
</li>
<li>
<p>[26] J. Wang, Y. Zhao, K. Zhang, Q. Wang, and X. Li, &ldquo;Spatio-temporal online matrix factorization for multi-scale moving objects detection,&rdquo; IEEE Trans. Circuits Syst. Video Technol., vol. 32, no. 2, pp. 743–757, Feb. 2022.</p>
</li>
<li>
<p>[27] M. Mathieu, C. Couprie, and Y. LeCun, &ldquo;Deep multi-scale video prediction beyond mean square error,&rdquo; 2015, arXiv:1511.05440 .</p>
</li>
<li>
<p>[28] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther, &ldquo;Autoencoding beyond pixels using a learned similarity metric,&rdquo; in Proc. 33rd Int. Conf. Mach. Learn., vol. 48, Feb. 2016, pp. 1558–1566.</p>
</li>
<li>
<p>[29] N. C. Tay, T. Connie, T. S. Ong, A. B. J. Teoh, and P. S. Teh, &ldquo;A review of abnormal behavior detection in activities of daily living,&rdquo; IEEE Access, vol. 11, pp. 5069–5088, 2023.</p>
</li>
<li>
<p>[30] B. Ramachandra, M. J. Jones, and R. R. Vatsavai, &ldquo;A survey of singlescene video anomaly detection,&rdquo; IEEE Trans. Pattern Anal. Mach. Intell. , vol. 44, no. 5, pp. 2293–2312, May 2022.</p>
</li>
<li>
<p>[31] H. Wang, X. Jiang, H. Ren, Y. Hu, and S. Bai, &ldquo;SwiftNet: Real-time video object segmentation,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 1296–1305.</p>
</li>
<li>
<p>[32] H. Lv and Q. Sun, &ldquo;Video anomaly detection and explanation via large language models,&rdquo; 2024, arXiv:2401.05702 .</p>
</li>
<li>
<p>[33] Z. Yang, J. Liu, and P. Wu, &ldquo;Text prompt with normality guidance for weakly supervised video anomaly detection,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2024, pp. 18899–18908.</p>
</li>
<li>
<p>[34] H.-T. Duong, V.-T. Le, and V. T. Hoang, &ldquo;Deep learning-based anomaly detection in video surveillance: A survey,&rdquo; Sensors, vol. 23, no. 11, p. 5024, May 2023.</p>
</li>
<li>
<p>[35] R. Raja, P. C. Sharma, M. R. Mahmood, and D. K. Saini, &ldquo;Analysis of anomaly detection in surveillance video: Recent trends and future vision,&rdquo; Multimedia Tools Appl., vol. 82, no. 8, pp. 12635–12651, Mar. 2023.</p>
</li>
<li>
<p>[36] M. Baradaran and R. Bergevin, &ldquo;A critical study on the recent deep learning based semi-supervised video anomaly detection methods,&rdquo; Multimedia Tools Appl., vol. 83, no. 9, pp. 27761–27807, Aug. 2023.</p>
</li>
<li>
<p>[37] H. Lv, Z. Yue, Q. Sun, B. Luo, Z. Cui, and H. Zhang, &ldquo;Unbiased multiple instance learning for weakly supervised video anomaly detection,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2023, pp. 8022–8031.</p>
</li>
<li>
<p>[38] J.-X. Zhong, N. Li, W. Kong, S. Liu, T. H. Li, and G. Li, &ldquo;Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 1237–1246.</p>
</li>
<li>
<p>[39] Y. Tian, G. Pang, Y. Chen, R. Singh, J. W. Verjans, and G. Carneiro, &ldquo;Weakly-supervised video anomaly detection with robust temporal feature magnitude learning,&rdquo; in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 4975–4986.</p>
</li>
<li>
<p>[40] C. Zhang et al., &ldquo;Exploiting completeness and uncertainty of pseudo labels for weakly supervised video anomaly detection,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2023, pp. 16271–16280.</p>
</li>
<li>
<p>[41] S. Li, F. Liu, and L. Jiao, &ldquo;Self-training multi-sequence learning with transformer for weakly supervised video anomaly detection,&rdquo; in Proc. AAAI Conf. Artif. Intell., vol. 36, no. 2, 2022, pp. 1395–1403.</p>
</li>
<li>
<p>[42] C. Huang et al., &ldquo;Weakly supervised video anomaly detection via self-guided temporal discriminative transformer,&rdquo; IEEE Trans. Cybern. , vol. 54, no. 5, pp. 3197–3210, May 2022.</p>
</li>
<li>
<p>[43] P. Wu, X. Liu, and J. Liu, &ldquo;Weakly supervised audio-visual violence detection,&rdquo; IEEE Trans. Multimedia, vol. 25, pp. 1674–1685, 2022.</p>
</li>
<li>
<p>[44] C. Ding, G. Pang, and C. Shen, &ldquo;Catching both gray and black swans: Open-set supervised anomaly detection,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 7388–7398.</p>
</li>
<li>
<p>[45] Y. Zhao, I. Misra, P. Krähenbühl, and R. Girdhar, &ldquo;Learning video representations from large language models,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2023, pp. 6586–6597.</p>
</li>
<li>
<p>[46] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, &ldquo;Frozen in time: A joint video and image encoder for end-to-end retrieval,&rdquo; in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 1728–1738.</p>
</li>
<li>
<p>[47] K. Li et al., &ldquo;VideoChat: Chat-centric video understanding,&rdquo; 2023, arXiv:2305.06355 .</p>
</li>
<li>
<p>[48] Y. Su, T. Lan, H. Li, J. Xu, Y. Wang, and D. Cai, &ldquo;PandaGPT: One model to instruction-follow them all,&rdquo; 2023, arXiv:2305.16355 .</p>
</li>
<li>
<p>[49] H. Zhang, X. Li, and L. Bing, &ldquo;Video-LLaMA: An instructiontuned audio-visual language model for video understanding,&rdquo; 2023, arXiv:2306.02858 .</p>
</li>
<li>
<p>[50] M. Ning et al., &ldquo;Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models,&rdquo; 2023, arXiv:2311.16103 .</p>
</li>
<li>
<p>[51] Z. Wang et al., &ldquo;Language models with image descriptors are strong few-shot video-language learners,&rdquo; in Proc. Adv. Neural Inf. Process. Syst., vol. 35, Jan. 2022, pp. 8483–8497.</p>
</li>
<li>
<p>[52] Y. Weng, M. Han, H. He, X. Chang, and B. Zhuang, &ldquo;LongVLM: Efficient long video understanding via large language models,&rdquo; 2024, arXiv:2404.03384 .</p>
</li>
<li>
<p>[53] M. Maaz, H. Rasheed, S. Khan, and F. Khan, &ldquo;VideoGPT+: Integrating image and video encoders for enhanced video understanding,&rdquo; 2024, arXiv:2406.09418 .</p>
</li>
<li>
<p>[54] P. Jin, R. Takanobu, W. Zhang, X. Cao, and L. Yuan, &ldquo;Chat-UniVi: Unified visual representation empowers large language models with image and video understanding,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2024, pp. 13700–13710.</p>
</li>
<li>
<p>[55] S. Uppal et al., &ldquo;Multimodal research in vision and language: A review of current and emerging trends,&rdquo; Inf. Fusion, vol. 77, pp. 149–171, Jan. 2022.</p>
</li>
<li>
<p>[56] A. Radford et al., &ldquo;Learning transferable visual models from natural language supervision,&rdquo; in Proc. Int. Conf. Mach. Learn., vol. 139, 2021, pp. 8748–8763.</p>
</li>
<li>
<p>[57] H. Luo et al., &ldquo;CLIP4Clip: An empirical study of CLIP for end to end video clip retrieval and captioning,&rdquo; Neurocomputing, vol. 508, pp. 293–304, Oct. 2022.</p>
</li>
<li>
<p>[58] J. Wang, H. Wang, J. Deng, W. Wu, and D. Zhang, &ldquo;EfficientCLIP: Efficient cross-modal pre-training by ensemble confident learning and language modeling,&rdquo; 2021, arXiv:2109.04699 .</p>
</li>
<li>
<p>[59] Z. Lin et al., &ldquo;Frozen clip models are efficient video learners,&rdquo; in Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2022, pp. 388–404.</p>
</li>
<li>
<p>[60] B. Ni et al., &ldquo;Expanding language-image pretrained models for general video recognition,&rdquo; in Proc. Eur. Conf. Comput. Vis. (ECCV). Cham, Switzerland: Springer, 2022, pp. 1–18.</p>
</li>
<li>
<p>[61] H. K. Joo, K. Vo, K. Yamazaki, and N. Le, &ldquo;CLIP-TSA: Clipassisted temporal self-attention for weakly-supervised video anomaly detection,&rdquo; in Proc. IEEE Int. Conf. Image Process. (ICIP), Oct. 2023, pp. 3230–3234.</p>
</li>
<li>
<p>[62] S. Nag, X. Zhu, Y.-Z. Song, and T. Xiang, &ldquo;Zero-shot temporal action detection via vision-language prompting,&rdquo; in Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, Jan. 2022, pp. 681–697.</p>
</li>
<li>
<p>[63] C. Ju, T. Han, K. Zheng, Y. Zhang, and W. Xie, &ldquo;Prompting visual-language models for efficient video understanding,&rdquo; in Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2022, pp. 105–124.</p>
</li>
<li>
<p>[64] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, &ldquo;End-to-end object detection with transformers,&rdquo; in Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2020, pp. 213–229.</p>
</li>
<li>
<p>[65] H. Touvron et al., &ldquo;LLaMA: Open and efficient foundation language models,&rdquo; 2023, arXiv:2302.13971 .</p>
</li>
<li>
<p>[66] J.-C. Feng, F.-T. Hong, and W.-S. Zheng, &ldquo;MIST: Multiple instance self-training framework for video anomaly detection,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021, pp. 14009–14018.</p>
</li>
<li>
<p>[67] H. Sapkota and Q. Yu, &ldquo;Bayesian nonparametric submodular video partition for robust anomaly detection,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2022, pp. 3212–3221.</p>
</li>
<li>
<p>[68] J. Wang and A. Cherian, &ldquo;GODS: Generalized one-class discriminative subspaces for anomaly detection,&rdquo; in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 8200–8210.</p>
</li>
<li>
<p>[69] P. Wu, J. Liu, M. Li, Y. Sun, and F. Shen, &ldquo;Fast sparse coding networks for anomaly detection in videos,&rdquo; Pattern Recognit., vol. 107, Jun. 2020, Art. no. 107515.</p>
</li>
<li>
<p>[70] H. Zhou, J. Yu, and W. Yang, &ldquo;Dual memory units with uncertainty regulation for weakly supervised video anomaly detection,&rdquo; in Proc. AAAI Conf. Artif. Intell., vol. 37, Jun. 2023, pp. 3769–3777.</p>
</li>
<li>
<p>[71] M. A. Hafeez, S. Javed, M. Madden, and I. Ullah, &ldquo;Unsupervised end-toend transformer based approach for video anomaly detection,&rdquo; in Proc. 38th Int. Conf. Image Vis. Comput. New Zealand (IVCNZ), Nov. 2023, pp. 1–7.</p>
</li>
<li>
<p>[72] C. Yan, S. Zhang, Y. Liu, G. Pang, and W. Wang, &ldquo;Feature prediction diffusion model for video anomaly detection,&rdquo; in Proc. IEEE/CVF Int. Conf. Comput. Vis., Oct. 2023, pp. 5527–5537.</p>
</li>
<li>
<p>[73] A. Panariello, A. Porrello, S. Calderara, and R. Cucchiara, &ldquo;Consistencybased self-supervised learning for temporal anomaly localization,&rdquo; in Proc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, Jan. 2022, pp. 338–349.</p>
</li>
<li>
<p>[74] T. Yuan et al., &ldquo;Towards surveillance video-and-language understanding: New dataset, baselines, and challenges,&rdquo; 2023, arXiv:2309.13925 .</p>
</li>
<li>
<p>[75] P. Wu et al., &ldquo;Not only look, but also listen: Learning multimodal violence detection under weak supervision,&rdquo; in Proc. Eur. Conf. Comput. Vis. (ECCV), Glasgow, U.K. Cham, Switzerland: Springer, Aug. 2020, pp. 322–339.</p>
</li>
<li>
<p>[76] P. Wu and J. Liu, &ldquo;Learning causal temporal relation and feature discrimination for anomaly detection,&rdquo; IEEE Trans. Image Process. , vol. 30, pp. 3513–3527, 2021.</p>
</li>
<li>
<p>[77] G. Pang, C. Yan, C. Shen, A. Van Den Hengel, and X. Bai, &ldquo;Selftrained deep ordinal regression for end-to-end video anomaly detection,&rdquo; in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 12173–12182.</p>
</li>
</ul>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000006_9963fc5ba2f6c15869de6d9257a3ba6151a5b907c23b29ad28c3a4a950f36e1e.png"
    ></figure>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000007_c6869972e49e7b5fe2653ac86e693df0c8d3ba5287369ea273d30b333aaba2d6.png"
    ></figure>
<p>Duc Tri Phan (Member, IEEE) received the B.S. degree from the Mechanics Department, Back Khoa University, Ho Chi Minh City, Vietnam, in 2017, and the M.S. and Ph.D. degrees from the Biomedical Engineering Department, Pukyong National University, Busan, South Korea, in 2020 and 2023, respectively.</p>
<p>He is currently a Post-Doctoral Fellow with Nanyang Technological University, Singapore. His research interests include smart healthcare and artificial intelligence applications.</p>
<p>Vu Hoang Minh Doan received the B.S. degree from the Mechatronics Engineering Department, Back Khoa University, Ho Chi Minh City, Vietnam, in 2016, and the M.S. and Ph.D. degrees from the Biomedical Engineering Department, Pukyong National University, Busan, South Korea, in 2019 and 2022, respectively.</p>
<p>He is currently a Research Professor with the Smart Gym-Based Translational Research Center for Active Senior&rsquo;s Healthcare, Pukyong National University. His research interests include artificial intelligence applications in biomedical engineering and automation control systems.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000008_bbffe2a7f2db289f0f88c1d928a319ba3437cee0c44c32d43257c84869e790d2.png"
    ></figure>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000009_01116987796de1f20f0a499189f4ef90bd0218a956d8effc0a23dcda1990beff.png"
    ></figure>
<p>Jaeyeop Choi received the B.S. degree in biomedical engineering and the M.S. and Ph.D. degrees from Pukyong National University, Busan, South Korea, in 2017, 2019, and 2022, respectively.</p>
<p>He is currently a Research Professor with the Smart Gym-Based Translational Research Center for Active Senior&rsquo;s Healthcare, Pukyong National University. His research interests include scanning acoustic microscopy and fabrication of highfrequency transducers.</p>
<p>Byeongil Lee received the master&rsquo;s and Ph.D. degrees in computer engineering, with a specialization in medical image analysis, from Inje University, Gimhae, South Korea, in 1999 and 2004, respectively.</p>
<p>In 2005, he commenced his career at the Department of Nuclear Medicine, Chonnam National University Hospital, Gwangju, South Korea. He expanded his research horizons in 2011 by joining Korea Photonics Technology Institute, Gwangju, focusing on medical photonics.</p>
<p>His academic journey led him to Pukyong National University, Busan, South Korea, in 2021, where he currently holds a professorship with the Department of Smart Healthcare. His research interests are currently centered on medical photonics, molecular imaging, and the advancement of digital healthcare systems.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000010_dd5796f376ea287f109b568e593a448b15c3f41367a92b610ea975801192f18a.png"
    ></figure>
<p>Junghwan Oh (Senior Member, IEEE) received the B.S. degree in mechanical engineering from Pukyong National University, Busan, South Korea, in 1992, and the M.S. and Ph.D. degrees in biomedical engineering from The University of Texas at Austin, Austin, TX, USA, in 2003 and 2007, respectively.</p>
<p>He is currently a Professor with Industry 4.0 Convergence Bionics Engineering, Department of Biomedical Engineering, Pukyong National University. His research interests include the development of ultrasonic-based diagnostic imaging modalities for biomedical engineering applications.</p>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/AADC-Net_A_Multimodal_Deep_Learning_Framework_for_Automatic_Anomaly_Detection_in_Real-Time_Surveillance.md"
          data-oid-likes="likes_papers/AADC-Net_A_Multimodal_Deep_Learning_Framework_for_Automatic_Anomaly_Detection_in_Real-Time_Surveillance.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/action-hints-semantic-typicality-and-context-uniqueness-for/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/a-vlm-based-method-for-visual-anomaly-detection-in-robotic-scientific-laboratories/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
