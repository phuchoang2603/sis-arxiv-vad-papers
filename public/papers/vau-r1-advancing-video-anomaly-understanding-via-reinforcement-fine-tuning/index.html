<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="en">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title> &middot; Blowfish</title>
    <meta name="title" content=" &middot; Blowfish">
  

  
  
  
  
  
  <link rel="canonical" href="http://localhost:1313/papers/vau-r1-advancing-video-anomaly-understanding-via-reinforcement-fine-tuning/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/papers/vau-r1-advancing-video-anomaly-understanding-via-reinforcement-fine-tuning/">
  <meta property="og:site_name" content="Blowfish">
  <meta property="og:title" content="Blowfish">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Blowfish">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.7946532fa78157bafb1b4340d4074792811f18177af35790b3a0cb90566ce1cda3edd782eb92e134849964d14b6d9185c9d4ded2a12dcf5b706cb6a275c0cce0.css"
    integrity="sha512-eUZTL6eBV7r7G0NA1AdHkoEfGBd681eQs6DLkFZs4c2j7deC65LhNISZZNFLbZGFydTe0qEtz1twbLaidcDM4A==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9df4fc14d50efcc9aa4cfc2b6f348e365f421f5ad491278f8f48c0360cf2f93f08882fda6da162d7ace8e5add57c2df4ac46bd3861306b1d4c452cd31f448d64.js"
      integrity="sha512-nfT8FNUO/MmqTPwrbzSONl9CH1rUkSePj0jANgzy&#43;T8IiC/abaFi16zo5a3VfC30rEa9OGEwax1MRSzTH0SNZA=="
      data-copy="Copy"
      data-copied="Copied"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Papers",
    "name": "",
    "headline": "",
    
    
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/papers\/vau-r1-advancing-video-anomaly-understanding-via-reinforcement-fine-tuning\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    
    
    
    
    
    
    
    
    "mainEntityOfPage": "true",
    "wordCount": "9444"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Skip to main content
      </a>
    </div>
    
    
      













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          Blowfish
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/papers/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="papers"
  title="Papers">
  
  
    <p class="text-base font-medium">
      papers
    </p>
  
</a>



      
        
  <a
  href="/test/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="test"
  title="Tests">
  
  
    <p class="text-base font-medium">
      test
    </p>
  
</a>



      
        
  <a
  href=""
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  
  title="">
  
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Search (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/papers/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="papers"
    title="Papers">
    
    
      <p class="text-bg font-bg">
        papers
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/test/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="test"
    title="Tests">
    
    
      <p class="text-bg font-bg">
        test
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href=""
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    
    title="">
    
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>





    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time><span class="px-2 text-primary-500">&middot;</span><span>9444 words</span><span class="px-2 text-primary-500">&middot;</span><span title="Reading time">45 mins</span>
    

    
    
  </div>

  

  
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning
    <div id="vau-r1-advancing-video-anomaly-understanding-via-reinforcement-fine-tuning" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#vau-r1-advancing-video-anomaly-understanding-via-reinforcement-fine-tuning" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Liyun Zhu 1 , 2 , ∗ Qixiang Chen 1 Xi Shen 3 Xiaodong Cun 2 , †</p>
<p>1 Australian National University 2 GVC Lab, Great Bay University 3 Intellindust AI Lab</p>
<p>{liyun.zhu, <a
  href="mailto:u7227010%7d@anu.edu.au">u7227010}@anu.edu.au</a>, <a
  href="mailto:shenxiluc@gmail.com">shenxiluc@gmail.com</a>, <a
  href="mailto:cun@gbu.edu.cn">cun@gbu.edu.cn</a></p>

<h2 class="relative group">Abstract
    <div id="abstract" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#abstract" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAUBench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at <a
  href="https://github.com/GVCLab/VAU-R1"
    target="_blank"
  >https://github.com/GVCLab/VAU-R1</a> .</p>
<p>Figure 1: Effectiveness of Reinforcement Fine-Tuning. We compare QA accuracy and temporal anomaly grounding performance across different models. VAU-R1, trained via Reinforcement Fine-Tuning (RFT), consistently outperforms its Supervised Fine-Tuning (SFT) counterpart. This demonstrates that RFT enhances both reasoning and temporal localization capabilities in VAU tasks.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000000_b00c208704cb5acb1a71feb703e229a7c67277c3416d419fcf814e59ca78eea5.png"
    ></figure>

<h2 class="relative group">1 Introduction
    <div id="1-introduction" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-introduction" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Anomalies are events or behaviors that deviate from regular patterns or expected activities in a given context. In surveillance settings, these may include incidents such as fighting, theft, or</p>
<p>∗ Work done while the author was a visiting student at GVC Lab, Great Bay University.</p>
<p>† Corresponding Author</p>
<p>Figure 2: Overview of VAU-R1. VAU-R1 leverages Reinforcement Fine-Tuning to enhance the reasoning ability of MLLMs for video anomaly understanding. Specifically, we adopt Group Relative Policy Optimization (GRPO) to optimize the model with task-specific rewards, such as answer format, accuracy, and temporal Intersection-over-Union (IoU). We decompose the VAU task into four complementary tasks to facilitate comprehensive reasoning: multiple-choice QA, temporal anomaly grounding, anomaly reasoning, and anomaly classification.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000001_2542ea5f1df8f57d1377dce0335c86b90cf4322d134fcd56ddc6fe07cbf88330.png"
    ></figure>
<p>traffic violations, etc. Video Anomaly Understanding (VAU) aims to detect and interpret such irregular events in unstructured, real-world video streams [22]. The task is challenging due to scene complexity, context dependence, varying camera viewpoints, and diverse anomaly types [27 , 44 , 56]. Early approaches only focuses on detecting anomalies, which typically framed the task as binary classification, assigning normal or abnormal labels to individual frames and identifying the temporal boundaries of anomalous events [5 , 6 , 11 , 16 , 21 , 33 , 36 , 47 , 55]. While effective for localization, these methods offer limited interpretability and provide little insight into the underlying causes of anomalies [7 , 8 , 49]. Recent advances in Multi-modal Large Language Models (MLLMs) have introduced the ability to generate textual descriptions of anomalous events [9 , 48 , 51 , 52 , 53], improving model transparency to some extent. However, current methods still face three key limitations: (i) they lack the ability to generate coherent, multi-step reasoning chains; (ii) no comprehensive benchmark provides rich annotations to support detailed causal reasoning; and (iii) evaluation protocols for reasoning quality remain underdeveloped.</p>
<p>To move beyond shallow classification and toward deeper understanding, we decompose VAU into four progressive stages: (i) Perception — identifying the scene and relevant objects, either through free-text descriptions or guided multiple-choice questions; (ii) Grounding — localizing the precise temporal segment where the anomaly occurs; (iii) Reasoning — explaining the event by analyzing causal factors, temporal dynamics, and contextual cues; and (iv) Conclusion — summarizing the event with a final decision, such as assigning it to a specific category (e.g., fighting vs. robbery). This structured formulation enables models to progressively build semantic understanding and supports more interpretable and task-aligned evaluation.</p>
<p>To implement this four-stage formulation, we introduce VAU-R1, a Reinforcement Fine-Tuning (RFT) framework designed to improve the reasoning capabilities of MLLMs on the VAU task. Our method builds on Group Relative Policy Optimization (GRPO) [31], incorporating task-specific reward signals based on answer format correctness, question-answer accuracy, and temporal grounding alignment. The framework is data-efficient and can be applied in low-resource settings, making it practical for real-world deployments. To support training and evaluation, we also construct VAUBench, a new benchmark that spans diverse scenarios and provides rich annotations across the four reasoning stages, including multiple-choice QA pairs, detailed event descriptions, temporal groundings, and step-by-step rationales. Finally, we propose a set of evaluation metrics—QA</p>
<p>accuracy, temporal Intersection-over-Union (IoU), GPT-based reasoning score, and classification accuracy—to quantitatively assess model performance across perception, grounding, reasoning, and conclusion. Together, VAU-R1 and VAU-Bench offer a scalable and unified framework for advancing structured video anomaly understanding. Our contribution can be summarized as follows:</p>
<ul>
<li>We propose VAU-R1, a data-efficient Reinforcement Fine-Tuning framework that improves the reasoning ability of MLLMs for video anomaly understanding. It outperforms standard supervised fine-tuning on reasoning-intensive tasks.</li>
<li>We present VAU-Bench, the first large-scale benchmark with Chain-of-Thought annotations designed for video anomaly reasoning. It contains a diverse collection of videos, QA pairs, temporal labels, and detailed rationales spanning a wide range of real-world scenarios.</li>
<li>We design a unified evaluation protocol that measures model performance across four reasoning stages, jointly considering reasoning quality, answer correctness, and temporal localization to capture both interpretability and detection precision.</li>
</ul>

<h2 class="relative group">2 Related Works
    <div id="2-related-works" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-related-works" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>From Detection to Understanding. Early efforts in Video Anomaly Detection (VAD) can be broadly categorized into self-supervised and weakly-supervised paradigms. Self-supervised methods rely solely on normal video samples, learning the distribution of normal behavior and flagging deviations as anomalies [11 , 21 , 25]. In contrast, weakly-supervised methods are trained with both normal and anomalous videos using coarse video-level labels rather than fine-grained frame-level annotations [5 , 16 , 33 , 36 , 45 , 47 , 55]. These approaches typically adopt a top-k selection strategy to identify the most likely anomalous segments. While effective for localizing anomaly boundaries, they often rely heavily on motion cues [56], operating under the assumption that rapid or irregular motion is indicative of anomalous behavior. However, this assumption does not hold for subtle or semantically complex anomalies, leading to poor interpretability. To address these limitations, recent work has turned to video anomaly understanding, leveraging MLLMs to provide more semantically grounded and interpretable reasoning [26].</p>
<p>Prompt-Based vs. Learning-Based Approaches for VAU. Building on the shift toward semantic understanding, recent approaches to VAU fall into two main categories: prompt-based and learningbased methods. Prompt-based methods typically use MLLMs as anomaly scorers [30 , 51], or as reasoning agents via rule-based few-shot prompting [48] or learned question templates [49]. While these methods avoid computationally expensive training, their generalization ability is often limited due to the absence of task-specific adaptation. On the other hand, pretraining [8] and finetuning [52 , 53] approaches aim to learn anomaly-aware representations by incorporating video captions and causal reasoning signals (e.g., cause and effect). Despite this progress, existing methods remain constrained to improving anomaly description and fail to capture the full logical chain of an anomaly. To overcome these limitations, we leverage reinforcement fine-tuning to enhance the model&rsquo;s reasoning ability, enabling end-to-end identification of both when and why anomalies occur.</p>
<p>Reinforcement Learning in MLLMs. With the rise of powerful models such as OpenAI-o1 [15] and DeepSeek-R1 [12], reinforcement learning has been increasingly adopted in the post-training stage of MLLMs to enhance their reasoning capabilities [3 , 10 , 14 , 42 , 54]. While effective, this process often demands substantial computational resources and large-scale datasets, making it less practical for targeted downstream tasks [34]. To address these challenges, Visual-RFT [23] introduces Reinforcement Fine-Tuning (RFT) for visual tasks, demonstrating improved data efficiency and stronger performance compared to Supervised Fine-Tuning (SFT). Building on this idea, VideoChatR1 [17] extends RFT to video domains, achieving promising results in tasks such as question answering, temporal grounding, and object tracking. Yet, these tasks remain fragmented and have not been unified under the video anomaly understanding setting. To bridge this gap, we propose a framework that jointly addresses multiple tasks, aiming to advance comprehensive and interpretable anomaly reasoning.</p>

<h2 class="relative group">3 Methodology
    <div id="3-methodology" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-methodology" aria-label="Anchor">#</a>
    </span>
    
</h2>

<h2 class="relative group">3.1 Preliminary: Reinforcement Learning via Group Relative Policy Optimization
    <div id="31-preliminary-reinforcement-learning-via-group-relative-policy-optimization" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#31-preliminary-reinforcement-learning-via-group-relative-policy-optimization" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Group Relative Policy Optimization (GRPO) [31] is a reinforcement learning framework that optimizes a policy πθ using preference-based feedback and multi-aspect reward signals. Given a question x, GRPO generates M candidate outputs O = {o1, o2, . . . , oM} from the old policy πθ old , each output oj assigned a reward Rj computed as a weighted sum of K task-specific components:</p>
<!-- formula-not-decoded -->
<p>where R (k) j is the k-th task-specific reward (e.g., accuracy, IoU, format compliance) and λk is its weight. To measure the relative quality of the j-th output, we calculate the normalized reward R ˜ j for each output oj with the mean µR and standard deviation σR across M candidates:</p>
<!-- formula-not-decoded -->
<p>GRPO maximises the following objective while keeping the update close to the original MLLM parameters πref through a KL penalty term DKL(· || ·):</p>
<!-- formula-not-decoded -->
<p>where β is a regularization coefficient. This formulation allows GRPO to incorporate diverse reward signals while retaining training stability through KL regularization.</p>

<h2 class="relative group">3.2 VAU-R1
    <div id="32-vau-r1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#32-vau-r1" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>As shown in Figure 2, VAU-R1 is a data-efficient reinforcement fine-tuning framework designed for the four VAU tasks, including Multi-choice QA, Temporal Anomaly Grounding, Anomaly Reasoning, and Anomaly Classification. Given videos and task-specific questions, we fine-tune a pre-trained MLLM to improve its multi-step reasoning ability across different tasks. The model generates multiple candidate responses for each input, which are then scored using task-specific reward functions (e.g., accuracy, temporal IoU, or format compliance). We employ Group Relative Policy Optimization (GRPO) to optimize the model, which maximizes reward-weighted likelihood while constraining divergence from the reference model via KL regularization. Our reinforcement-based approach outperforms supervised fine-tuning (SFT) in both reasoning capability and generalization to unseen scenarios. The design of task-specific reward functions is further detailed in Section 3.3 .</p>

<h2 class="relative group">3.3 Reward Rules
    <div id="33-reward-rules" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#33-reward-rules" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We adopt the general idea of GRPO-based RFT to optimize the VAU model by designing task-specific reward functions for different VAU components. Below, we detail each reward definition.</p>
<p>Format Reward. For multiple-choice QA and anomaly classification tasks, we instruct the model to enclose its reasoning within &lt;think&gt;&hellip;&lt;/think&gt; tags and the answer within &lt;answer&gt;&hellip;&lt;/answer&gt; tags. For the temporal anomaly grounding task, we additionally require &lt;glue&gt;&hellip;&lt;/glue&gt; tags to indicate the predicted time span in seconds. The reward is defined as:</p>
<!-- formula-not-decoded -->
<p>We apply a format reward to VAU tasks to enforce structured outputs and discourage format violations.</p>
<p>Accuracy Reward. We also define an accuracy reward R acc to measure the correctness of the model&rsquo;s answer. In our experiments, this reward is given by:</p>
<!-- formula-not-decoded -->
<p>Figure 3: Statistics of our VAU-Bench. (a) Distribution of main anomaly types. (b) Distribution of video durations (top) and the proportion of anomalous segments within each video (bottom). (c) The evaluation criteria for four VAU tasks.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000002_ec45de56454cb0aff1939e5296e47771920cfc88932523a32f165d2437eec726.png"
    ></figure>
<p>This simple accuracy reward encourages the model to choose the right answer during training.</p>
<p>Temporal IoU Reward. To encourage precise temporal grounding, we introduce a temporal Intersection-over-Union (IoU) reward RtIoU, which measures the alignment between the predicted and ground truth anomaly intervals. The reward is defined as:</p>
<!-- formula-not-decoded -->
<!-- formula-not-decoded -->
<p>(6)</p>
<p>Here, [s1, s2] denotes the predicted temporal span of the anomaly, while [s ∗ ∗
1
, s ∗ 2 ] is the ground truth interval. The temporal IoU quantifies the degree of overlap between these intervals, and serves as a fine-grained reward signal to guide the model toward more accurate temporal localization.</p>
<p>Task-specific Reward Formulations. We apply task-specific combinations of the reward components mentioned above. For the multiple-choice QA task, we use a combination of format and accuracy rewards: R QA = R format + R acc . For temporal anomaly grounding, we further include a temporal IoU term to evaluate localization quality: RTAG = Rformat + R acc + RtIoU. For anomaly classification, we adopt a similar reward design as QA: RCLS = Rformat + R acc.</p>

<h2 class="relative group">3.4 VAU-Bench
    <div id="34-vau-bench" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#34-vau-bench" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Task Definition. We decompose the VAU task into four stages: perception, grounding, reasoning, and conclusion. These stages address four core questions respectively: &ldquo;What happens in this video?&rdquo;, &quot; When does the anomaly occur?&quot;, &ldquo;Why does the anomaly happen?&rdquo;, and &ldquo;What is our overall judgment of the anomaly?&rdquo;. Corresponding to these stages, we define four VAU tasks:</p>
<ul>
<li>Multiple-Choice QA: Targets event perception by answering questions about videos.</li>
<li>Temporal Grounding: Localizes anomalous segments in the video timeline.</li>
<li>Anomaly Reasoning: Explores causal relationships to explain why an anomaly arises.</li>
</ul>
<p>Table 1: Comparison of performance on MSAD and UCF-Crime datasets on multiple-choice QA task and anomaly reasoning task. Accw/o think and Accw/ think refer to the multiple-choice question accuracy without and with thinking, respectively. For the anomaly reasoning task, CLS , KM , FLU , INF, and FAC represent VAU-Eval scores generated by DeepSeek-V3, measuring classification accuracy, key concept alignment, linguistic fluency, informativeness, and factual consistency, respectively. Each dimension is scored on a 10-point scale. Total denotes the aggregated score over five dimensions.</p>
<table>
  <thead>
      <tr>
          <th>Dataset</th>
          <th>Model</th>
          <th>QA Accuracy</th>
          <th>QA Accuracy</th>
          <th>VAU-Eval KM↑ FLU↑ INF↑ FAC↑ Total↑</th>
          <th>VAU-Eval KM↑ FLU↑ INF↑ FAC↑ Total↑</th>
          <th>VAU-Eval KM↑ FLU↑ INF↑ FAC↑ Total↑</th>
          <th>VAU-Eval KM↑ FLU↑ INF↑ FAC↑ Total↑</th>
          <th>VAU-Eval KM↑ FLU↑ INF↑ FAC↑ Total↑</th>
          <th>VAU-Eval KM↑ FLU↑ INF↑ FAC↑ Total↑</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Dataset</td>
          <td>Model</td>
          <td>Accw/o think</td>
          <td>Accw/ think</td>
          <td>CLS↑</td>
          <td>KM↑</td>
          <td>FLU↑</td>
          <td>INF↑</td>
          <td>FAC↑</td>
          <td>Total↑</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>InternVL2.5-2B</td>
          <td>76.67</td>
          <td>72.08</td>
          <td>6.84</td>
          <td>6.23</td>
          <td>8.55</td>
          <td>6.64</td>
          <td>6.64</td>
          <td>34.90</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>Qwen2.5-VL-7B</td>
          <td>84.58</td>
          <td>83.33</td>
          <td>6.75</td>
          <td>6.41</td>
          <td>9.27</td>
          <td>7.74</td>
          <td>6.92</td>
          <td>37.08</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>InternVL2.5-8B-MPO</td>
          <td>82.50</td>
          <td>84.17</td>
          <td>6.83</td>
          <td>6.33</td>
          <td>8.32</td>
          <td>6.37</td>
          <td>6.86</td>
          <td>34.72</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>Qwen2-VL-2B</td>
          <td>77.08</td>
          <td>72.50</td>
          <td>5.94</td>
          <td>5.43</td>
          <td>8.77</td>
          <td>6.29</td>
          <td>5.90</td>
          <td>32.25</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>+SFT</td>
          <td>82.92</td>
          <td>85.83</td>
          <td>6.04</td>
          <td>5.43</td>
          <td>8.89</td>
          <td>6.55</td>
          <td>5.93</td>
          <td>32.84</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>+RFT</td>
          <td>82.92 (↑5.84</td>
          <td>83.75 (↑11.25)</td>
          <td>6.05(↑)</td>
          <td>5.49(↑)</td>
          <td>8.89(↑)</td>
          <td>6.50(↑)</td>
          <td>6.05(↑)</td>
          <td>32.98(↑)</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>Qwen2.5-VL-3B</td>
          <td>85.83</td>
          <td>82.50</td>
          <td>5.77</td>
          <td>5.24</td>
          <td>9.02</td>
          <td>6.74</td>
          <td>(↑ 570</td>
          <td>32.47</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>+SFT</td>
          <td>86.25</td>
          <td>84.58</td>
          <td>2.89</td>
          <td>2.22</td>
          <td>9.02  489</td>
          <td>3.52</td>
          <td>2.44</td>
          <td>15.96</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>+RFT</td>
          <td>88.33</td>
          <td>87.08 (↑4.</td>
          <td>2.89  597(↑</td>
          <td>2.22</td>
          <td>4.89</td>
          <td>6.84(↑</td>
          <td>2.44  603(</td>
          <td>15.96 3338(</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>+RFT</td>
          <td>88.33 (↑</td>
          <td>87.08 (↑4.58)</td>
          <td>5.97(↑)</td>
          <td>5.49(↑)</td>
          <td>9.05(↑)</td>
          <td>6.84(↑)</td>
          <td>6.03(↑)</td>
          <td>33.38(↑)</td>
      </tr>
      <tr>
          <td></td>
          <td>InternVL2.5-2B</td>
          <td>84.86</td>
          <td>68.13</td>
          <td>4.40</td>
          <td>3.08</td>
          <td>8.09</td>
          <td>5.69</td>
          <td>3.47</td>
          <td>24.74</td>
      </tr>
      <tr>
          <td></td>
          <td>Qwen2.5-VL-7B</td>
          <td>92.03</td>
          <td>89.64</td>
          <td>4.80</td>
          <td>3.73</td>
          <td>8.95</td>
          <td>7.05</td>
          <td>4.25</td>
          <td>28.78</td>
      </tr>
      <tr>
          <td></td>
          <td>InternVL 2.5 8B-MPO</td>
          <td>89.64</td>
          <td>90.44</td>
          <td>3.79</td>
          <td>3.20</td>
          <td>8.23</td>
          <td>5.77</td>
          <td>3.48</td>
          <td>24.47</td>
      </tr>
      <tr>
          <td></td>
          <td>Qwen2-VL-2B</td>
          <td>87.25</td>
          <td>83.67</td>
          <td>3.47</td>
          <td>2.48</td>
          <td>7.75</td>
          <td>4.49</td>
          <td>2.82</td>
          <td>21.02</td>
      </tr>
      <tr>
          <td></td>
          <td>+SFT</td>
          <td>83.67</td>
          <td>86.06</td>
          <td>3.61</td>
          <td>2.26</td>
          <td>7.30</td>
          <td>4.79</td>
          <td>2.70</td>
          <td>20.66</td>
      </tr>
      <tr>
          <td></td>
          <td>+RFT</td>
          <td>88.45 (↑1.20)</td>
          <td>88.05 (↑4.38)</td>
          <td>4.04(↑)</td>
          <td>2.75(↑)</td>
          <td>7.72(↓)</td>
          <td>4.89(↑)</td>
          <td>3.11(↑)</td>
          <td>22.52(↑)</td>
      </tr>
      <tr>
          <td></td>
          <td>Qwen2.5-VL-3B</td>
          <td>91.63</td>
          <td>83.27</td>
          <td>4.31</td>
          <td>2.88</td>
          <td>8.70</td>
          <td>5.95</td>
          <td>3.27</td>
          <td>25.10</td>
      </tr>
      <tr>
          <td></td>
          <td>+SFT  +RFT</td>
          <td>92.03 (↑0.40)</td>
          <td>91.63 (↑8.36)</td>
          <td>1.80</td>
          <td>1.01</td>
          <td>4.15</td>
          <td>2.82</td>
          <td>1.11</td>
          <td>10.89</td>
      </tr>
      <tr>
          <td></td>
          <td>+RFT</td>
          <td>92.03 (↑0.40)</td>
          <td>91.63 (↑8.36)</td>
          <td>4.42(↑)</td>
          <td>2.98(↑)</td>
          <td>8.71(↑)</td>
          <td>5.98(↑)</td>
          <td>3.39(↑)</td>
          <td>25.49(↑)</td>
      </tr>
  </tbody>
</table>
<ul>
<li>Anomaly Classification: Assigns the anomaly to its corresponding category.</li>
</ul>
<p>This structured decomposition provides a clear framework for systematically addressing different perspectives of VAU, with each task rigorously evaluated using domain-specific metrics.</p>
<p>Dataset Construction and Annotation. Existing video anomaly datasets typically provide only frame-level labels [1 , 33 , 56] or sparse descriptions [8 , 9 , 50], limiting their usefulness for reasoningbased tasks. To address this, we construct VAU-Bench, a unified benchmark built from MSAD [56], UCF-Crime [33], and ECVA [8], enriched with Chain-of-Thought (CoT) annotations, including: (i) video descriptions, (ii) temporal boundaries, (iii) multiple-choice QA, and (iv) reasoning rationales. We apply a cleaning pipeline to remove corrupted or overly long videos and merge overlapping anomaly types. For UCF-Crime and ECVA, we use DeepSeek-V3 [18] to generate video-level summaries, QA pairs, and reasoning chains. For MSAD, CoT annotations are produced through a two-stage pipeline: we first apply InternVL-8B-MPO [42] to generate initial captions and analyses, which are then verified and refined using DeepSeek-V3 to obtain more accurate QA pairs and coherent reasoning rationales. We also give further construction and annotation details in the Appendix.</p>
<p>Dataset Statistics. Figure 3 presents an overview of VAU-Bench, the first VAU benchmark designed for Chain-of-Thought reasoning. Our dataset contains 4,602 videos covering 19 major anomaly types, with a total duration of 169.1 hours. It includes over 1.5 million words of fine-grained textual annotations, averaging 337 words per video, encompassing detailed descriptions, reasoning rationales, and multiple-choice questions. The dataset is split into 2,939 training, 734 validation, and 929 test videos. Additionally, we provide 3,700 temporal annotations to support the anomaly grounding task. Figure 3a shows the distribution of the main anomaly categories, while Figure 3b illustrates the diversity in video duration and anomaly sparsity. The evaluation protocols and metrics used for different tasks are summarized in Figure 3c, and we give more dataset statistics in the Appendix.</p>
<p>Reasoning Evaluation Metric: VAU-Eval. For VAU tasks, prior work has adopted BLEU and ROUGE [8 , 35 , 53] to evaluate semantic content. However, such n-gram-based metrics often fall short in capturing reasoning quality and deeper relational understanding. To better assess anomaly reasoning, we propose VAU-Eval, a GPT-based metric that compares model-generated descriptions and analyses with ground truth annotations. As illustrated in Figure 3c, we evaluate each response along five dimensions using DeepSeek-V3 [18] as the judge: classification accuracy, key concept</p>
<p>Table 2: Comparison of temporal anomaly grounding performance on the three datasets. For each dataset, we present results for the base models, followed by SFT and RFT variants. w/o think and w/ think refer to the inference prompt without and with thinking, respectively. Rows highlighted in light yellow denote the results on the UCF-Crime dataset, serving as an out-of-distribution test for cross-dataset evaluation.</p>
<table>
  <thead>
      <tr>
          <th>Dataset</th>
          <th>Model</th>
          <th>w/o think</th>
          <th>w/o think</th>
          <th>w/o think</th>
          <th>w/o think</th>
          <th>w/ think @03 @0@0</th>
          <th>w/ think @03 @0@0</th>
          <th>w/ think @03 @0@0</th>
          <th>w/ think @03 @0@0</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Dataset</td>
          <td>Model</td>
          <td>mIoU</td>
          <td><a
  href="mailto:R@0.3">R@0.3</a></td>
          <td><a
  href="mailto:R@0.5">R@0.5</a></td>
          <td><a
  href="mailto:R@0.7">R@0.7</a></td>
          <td>mIoU</td>
          <td><a
  href="mailto:R@0.3">R@0.3</a></td>
          <td><a
  href="mailto:R@0.5">R@0.5</a></td>
          <td><a
  href="mailto:R@0.7">R@0.7</a></td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>Qwen2-VL-2B</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>Qwen2.5-VL-7B</td>
          <td>45.90</td>
          <td>70.83</td>
          <td>45.83</td>
          <td>21.67</td>
          <td>17.57</td>
          <td>26.67</td>
          <td>11.67</td>
          <td>3.33</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>Qwen2.5-VL-3B</td>
          <td>21.27</td>
          <td>30.00</td>
          <td>10.83</td>
          <td>4.17</td>
          <td>13.00</td>
          <td>16.67</td>
          <td>5.83</td>
          <td>1.67</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>+ SFT</td>
          <td>30.65</td>
          <td>47.50</td>
          <td>30.00</td>
          <td>9.17</td>
          <td>35.17</td>
          <td>50.83</td>
          <td>34.17</td>
          <td>15.00</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>+ RFT</td>
          <td>35.77 (↑14.50)</td>
          <td>53.33</td>
          <td>34.17</td>
          <td>15.83</td>
          <td>30.70 (↑17.7</td>
          <td>48.33</td>
          <td>29.17</td>
          <td>12.50</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>Qwen2-VL-2B</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.17</td>
          <td>0.30</td>
          <td>0.00</td>
          <td>0.00</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>Qwen2.5-VL-7B</td>
          <td>19.85</td>
          <td>25.87</td>
          <td>15.17</td>
          <td>9.70</td>
          <td>5.71</td>
          <td>7.96</td>
          <td>4.73</td>
          <td>2.99</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>Qwen2.5-VL-3B</td>
          <td>14.21</td>
          <td>17.16</td>
          <td>6.47</td>
          <td>3.23</td>
          <td>6.35</td>
          <td>7.21</td>
          <td>1.99</td>
          <td>0.50</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>+ SFT</td>
          <td>45.30</td>
          <td>66.67</td>
          <td>49.75</td>
          <td>24.13</td>
          <td>45.96</td>
          <td>65.67</td>
          <td>51.00</td>
          <td>26.12</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>+ RFT</td>
          <td>35.09 (↑20.88)</td>
          <td>49.00</td>
          <td>28.86</td>
          <td>19.40</td>
          <td>33.25 (↑26.90)</td>
          <td>48.51</td>
          <td>30.60</td>
          <td>18.41</td>
      </tr>
      <tr>
          <td>UCF-Crime</td>
          <td>Qwen2-VL-2B</td>
          <td>2.74</td>
          <td>4.84</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.12</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
      </tr>
      <tr>
          <td>UCF-Crime</td>
          <td>Qwen2.5-VL-7B</td>
          <td>22.72</td>
          <td>33.87</td>
          <td>16.13</td>
          <td>8.06</td>
          <td>4.89</td>
          <td>8.06</td>
          <td>1.61</td>
          <td>0.00</td>
      </tr>
      <tr>
          <td>UCF-Crime</td>
          <td>Qwen2.5-VL-3B</td>
          <td>10.91</td>
          <td>15.32</td>
          <td>6.45</td>
          <td>3.23</td>
          <td>7.68</td>
          <td>10.48</td>
          <td>4.84</td>
          <td>1.61</td>
      </tr>
      <tr>
          <td>UCF-Crime</td>
          <td>+ SFT</td>
          <td>4.98</td>
          <td>3.23</td>
          <td>0.81</td>
          <td>0.00</td>
          <td>5.76</td>
          <td>5.65</td>
          <td>0.81</td>
          <td>0.81</td>
      </tr>
      <tr>
          <td>UCF-Crime</td>
          <td>+ RFT</td>
          <td>16.80 (↑5.89)</td>
          <td>23.39</td>
          <td>8.06</td>
          <td>4.03</td>
          <td>9.21 (↑1.53)</td>
          <td>9.68</td>
          <td>4.03</td>
          <td>1.61</td>
      </tr>
  </tbody>
</table>
<p>alignment, fluency, informativeness, and factual consistency. Each dimension is scored on a 10-point scale to provide fine-grained assessment of reasoning quality.</p>

<h2 class="relative group">4 Experiment
    <div id="4-experiment" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#4-experiment" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Implementation Details. Our main experiments are conducted using the Qwen2-VL-2B-Instruct [41] and Qwen2.5-VL-3B-Instruct [2] models. We apply full-parameter fine-tuning without adapters or LoRA, using 2 NVIDIA H20 GPUs for training. During the RFT training process, we adopt a structured prompting strategy that guides the model to generate intermediate reasoning and final answers in a standardized format. Specifically, each prompt instructs the model to enclose its reasoning process within &lt;think&gt;&hellip;&lt;/think&gt; tags and its final answer within &lt;answer&gt;&hellip;&lt;/answer&gt; tags. This format ensures consistency across different tasks. During inference, for Qwen-VL models, we sample frames at 1 FPS. For InternVL models, we uniformly sample 16 frames per video.</p>

<h2 class="relative group">4.1 Evaluation of VAU-R1
    <div id="41-evaluation-of-vau-r1" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#41-evaluation-of-vau-r1" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Evaluation Protocol. We report results separately on the MSAD, ECVA, and UCF-Crime datasets rather than using a single aggregated benchmark, as these datasets differ substantially in anomaly types, video durations, and scene contexts. All evaluation metrics for our four tasks are summarized in Figure 3c. For the QA task, we report multiple-choice accuracy. Temporal anomaly grounding is evaluated using temporal mean Intersection over Union (mIoU), as well as recall at different IoU thresholds: <a
  href="mailto:R@0.3">R@0.3</a>, <a
  href="mailto:R@0.5">R@0.5</a>, and <a
  href="mailto:R@0.7">R@0.7</a>. For anomaly reasoning, we adopt the GPT-based VAU-Eval introduced in Section 3.4. Finally, binary and multi-class classification accuracy are used for evaluating the anomaly classification task.</p>
<p>Evaluation on QA-Guided Reasoning. As shown in Table 1, we evaluate the reasoning capabilities of VAU-R1 on MSAD and UCF-Crime using multiple-choice QA accuracy and GPT-based VAU-Eval scores. We highlight two key observations. First, base models often perform worse when generating answers with reasoning (Accw/think) compared to without (Accw/o think) reasoning, indicating that naive Chain-of-Thought generation may introduce hallucination. In contrast, reinforcement fine-tuning (RFT) improves both QA accuracy with reasoning (e.g., +11.25 on MSAD) and overall reasoning quality. Second, RFT leads to consistent gains across five VAU-Eval dimensions—classification, demonstrating its ability to strengthen structured reasoning. For instance, on MSAD, Qwen2.5-VL3B+RFT achieves the highest total VAU-Eval score (33.38), showing substantial improvement over</p>
<p>Table 3: Ablation study on task co-training for anomaly classification. Bin. Acc. denotes binary classification accuracy (normal vs. abnormal), and Multi Acc. denotes multi-class accuracy over 19 anomaly types plus the normal class. Results are reported with and without think prompting.</p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>w/o think</th>
          <th>w/o think</th>
          <th>w/ think</th>
          <th>w/ think</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Model</td>
          <td>Bin. Acc.</td>
          <td>Multi Acc.</td>
          <td>Bin. Acc</td>
          <td>Multi Acc.</td>
      </tr>
      <tr>
          <td>Baseline (Qwen2.5-VL-3B-Instruct)</td>
          <td>62.77</td>
          <td>47.96</td>
          <td>59.33</td>
          <td>39.06</td>
      </tr>
      <tr>
          <td>+SFT w/ CLS</td>
          <td>81.12</td>
          <td>29.08</td>
          <td>83.37</td>
          <td>32.19</td>
      </tr>
      <tr>
          <td>+RFT w/ CLS</td>
          <td>60.30</td>
          <td>46.14</td>
          <td>59.01</td>
          <td>42.27</td>
      </tr>
      <tr>
          <td>+RFT w/ QA</td>
          <td>59.01</td>
          <td>46.14</td>
          <td>58.91</td>
          <td>41.95</td>
      </tr>
      <tr>
          <td>+RFT w/ TAG</td>
          <td>67.81</td>
          <td>49.46</td>
          <td>74.14</td>
          <td>46.14</td>
      </tr>
      <tr>
          <td>+RFT w/ QA-TAG</td>
          <td>65.77</td>
          <td>47.53</td>
          <td>67.60</td>
          <td>45.06</td>
      </tr>
      <tr>
          <td>+RFT w/ QA-TAG-CLS</td>
          <td>64.70</td>
          <td>48.61</td>
          <td>65.02</td>
          <td>45.60</td>
      </tr>
  </tbody>
</table>
<p>its SFT counterpart. These results confirm that RFT not only enhances answer correctness but also fosters robust and generalizable multimodal reasoning under the VAU setting.</p>
<p>Evaluation on Temporal Anomaly Grounding. As shown in Table 2, we evaluate the temporal anomaly grounding performance across three datasets. Note that all models are trained only on MSAD and ECVA, while UCF-Crime serves as an out of distribution test set. We observe several key findings. First, across both inference settings (w/ and w/o think), RFT consistently outperforms the corresponding base models, demonstrating its effectiveness in improving temporal localization. Notably, the RFT-finetuned 3B model achieves higher mIoU than the larger 7B base model on ECVA. Second, similar to our observations in QA-guided reasoning, Chain-of-Thought prompting does not necessarily enhance grounding performance. In some cases, adding reasoning leads to degraded localization accuracy. Third, RFT shows significantly better generalization compared to SFT. In cross-dataset evaluation (e.g., UCF-Crime as an out-of-distribution test), SFT demonstrates limited generalization, whereas RFT maintains strong performance across unseen scenarios. While SFT occasionally outperforms RFT in isolated cases, we observe that its direct predictions are opaque and lack interpretability, often yielding repetitive, non-discriminative outputs across videos (see Figure 4). These results highlight the advantages of RFT for enhancing generalization in VAU tasks.</p>
<p>Ablation Study. For VAU, the core objective is to make accurate high-level judgments about anomaly categories (e.g., distinguishing a fight from a robbery). To explore effective task formulations, we train models with different combinations of VAU tasks—multiple-choice QA, temporal anomaly grounding (TAG), and multi-class classification (CLS)—to assess their impact on reasoning. As shown in Table 3, RFT models trained with TAG alone achieve the highest binary accuracy (74.14) and strong multi-class performance (46.14) under the think setting, highlighting the benefit of temporal grounding for perception and category discrimination. Combining QA and TAG also improves performance but is slightly less effective than TAG alone. In contrast, SFT tends to over-predict anomalies, yielding high binary accuracy but poor multi-class results, suggesting overfitting. Overall, grounding-based tasks are more effective for anomaly classification, and jointly optimizing tasks via reinforcement learning yields complementary gains in both accuracy and reasoning.</p>
<p>Case Study. Figure 4 illustrates two representative examples from the QA and TAG tasks, comparing SFT and our VAU-R1 under the same Chain-of-Thought (CoT) prompt. In the QA example, SFT incorrectly selects a normal explanation based on surface cues, while VAU-R1 correctly infers a people-falling anomaly by identifying posture and behavioral irregularities. In the TAG example, SFT outputs a coarse anomaly span without rationale, whereas VAU-R1 localizes the anomaly more precisely (0.0–13.6s) and provides an interpretable causal chain. These cases highlight VAU-R1&rsquo;s superior reasoning and interpretability in both classification and localization settings. More qualitative case studies are provided in the Appendix.</p>

<h2 class="relative group">4.2 Discussion
    <div id="42-discussion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#42-discussion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>RFT Enhances Generalization and Interpretability. Our experiments demonstrate that RFT consistently outperforms SFT across multiple VAU tasks, offering improved interpretability (Table 1) and better generalization (Table 2). In contrast, SFT tends to memorize task-specific patterns and suffers from poor generalization to unseen scenarios. This suggests that SFT-trained models are more prone to overfitting, especially when trained on limited or narrowly defined tasks.</p>
<p>Is Chain-of-Thought Reasoning Necessary for VAU? Our findings suggest that Chain-of-Thought (CoT) reasoning does not always lead to better performance in visual understanding tasks. However,</p>
<p>Figure 4: Qualitative case of the QA (top) and TAG (bottom) task. All ground-truths and correct answers are highlighted in orange. Both SFT and RFT perform inference using the same CoT prompt. RFT&rsquo;s explicit chain-of-thought yields precise, interpretable QA choice and anomaly interval, whereas SFT&rsquo;s output is less informative and tends to produce inaccurate responses.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000003_ac79e7a124ac332369b814df7e69f9ae31a6576a8240b4e4b62adad5a8ee0bd2.png"
    ></figure>
<p>it significantly enhances interpretability by providing structured justifications. Unlike mathematical or logical tasks, where reasoning is more deterministic, visual understanding involves inherently diverse reasoning paths. Therefore, designing simpler sub-tasks with well-defined reward signals to guide reasoning effectively remains underexplored. Directly applying complex tasks (e.g., multi-class anomaly classification) without task co-training often leads to suboptimal results (Table 3).</p>
<p>Rethinking Anomaly Understanding in Multimodal Contexts. VAU calls for constructing a coherent reasoning chain that bridges spatial-temporal localization and causal inference. Yet, leveraging diverse cues such as keyframes, salient objects, and even additional modalities (e.g., audio) to support unified reasoning remains underexplored. We envision that future work could benefit from integrating these multimodal signals into a structured reasoning framework, enabling more robust and interpretable anomaly understanding. Our method and benchmark take a step in this direction by proposing a unified evaluation protocol across perception, localization, and reasoning dimensions, ultimately guiding models toward accurate and justifiable anomaly judgments.</p>

<h2 class="relative group">5 Conclusion
    <div id="5-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#5-conclusion" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We present VAU-R1, an advanced and unified Video Anomaly Understanding framework focusing on four VAU tasks: multi-choice QA, temporal grounding, anomaly reasoning, and classification. VAU-R1 leverages a multimodal large language model (MLLM) and, notably, employs reinforcement fine-tuning to enhance anomaly reasoning and explainability via carefully designed GRPO reward functions for each task. To facilitate the training and evaluation of this framework, we also introduce VAU-Bench, the first chain-of-thought benchmark designed to train and evaluate VAU tasks at the</p>
<p>reasoning level. The experiments on different tasks prove the strong performance of the proposed method than baselines.</p>

<h2 class="relative group">References
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#references" aria-label="Anchor">#</a>
    </span>
    
</h2>
<ul>
<li>
<p>[1] A. Acsintoae, A. Florescu, M.-I. Georgescu, T. Mare, P. Sumedrea, R. T. Ionescu, F. S. Khan, and M. Shah. Ubnormal: New benchmark for supervised open-set video anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20143–20153, 2022.</p>
</li>
<li>
<p>[2] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.</p>
</li>
<li>
<p>[3] J. Bi, S. Liang, X. Zhou, P. Liu, J. Guo, Y. Tang, L. Song, C. Huang, G. Sun, J. He, et al. Why reasoning matters? a survey of advancements in multimodal reasoning (v1). arXiv preprint arXiv:2504.03151, 2025.</p>
</li>
<li>
<p>[4] C. Cao, Y. Lu, P. Wang, and Y. Zhang. A new comprehensive benchmark for semi-supervised video anomaly detection and anticipation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20392–20401, 2023.</p>
</li>
<li>
<p>[5] Y. Chen, Z. Liu, B. Zhang, W. Fok, X. Qi, and Y.-C. Wu. Mgfn: Magnitude-contrastive glanceand-focus network for weakly-supervised video anomaly detection. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 387–395, 2023.</p>
</li>
<li>
<p>[6] D. Ding, L. Wang, L. Zhu, T. Gedeon, and P. Koniusz. Lego: Learnable expansion of graph operators for multi-modal feature fusion. arXiv preprint arXiv:2410.01506, 2024.</p>
</li>
<li>
<p>[7] K. Doshi and Y. Yilmaz. Towards interpretable video anomaly detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2655–2664, 2023.</p>
</li>
<li>
<p>[8] H. Du, G. Nan, J. Qian, W. Wu, W. Deng, H. Mu, Z. Chen, P. Mao, X. Tao, and J. Liu. Exploring what why and how: A multifaceted benchmark for causation understanding of video anomaly. arXiv preprint arXiv:2412.07183, 2024.</p>
</li>
<li>
<p>[9] H. Du, S. Zhang, B. Xie, G. Nan, J. Zhang, J. Xu, H. Liu, S. Leng, J. Liu, H. Fan, et al. Uncovering what why and how: A comprehensive benchmark for causation understanding of video anomaly. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18793–18803, 2024.</p>
</li>
<li>
<p>[10] K. Feng, K. Gong, B. Li, Z. Guo, Y. Wang, T. Peng, B. Wang, and X. Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025.</p>
</li>
<li>
<p>[11] D. Gong, L. Liu, V. Le, B. Saha, M. R. Mansour, S. Venkatesh, and A. v. d. Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1705–1714, 2019.</p>
</li>
<li>
<p>[12] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.</p>
</li>
<li>
<p>[13] K. Hara, H. Kataoka, and Y. Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 6546–6555, 2018.</p>
</li>
<li>
<p>[14] W. Huang, B. Jia, Z. Zhai, S. Cao, Z. Ye, F. Zhao, Z. Xu, Y. Hu, and S. Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749 , 2025.</p>
</li>
<li>
<p>[15] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.</p>
</li>
<li>
<p>[16] J. Leng, Z. Wu, M. Tan, Y. Liu, J. Gan, H. Chen, and X. Gao. Beyond euclidean: Dual-space representation learning for weakly supervised video violence detection. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.</p>
</li>
<li>
<p>[17] X. Li, Z. Yan, D. Meng, L. Dong, X. Zeng, Y. He, Y. Wang, Y. Qiao, Y. Wang, and L. Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025.</p>
</li>
<li>
<p>[18] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.</p>
</li>
<li>
<p>[19] K. Liu, W. Liu, C. Gan, M. Tan, and H. Ma. T-c3d: Temporal convolutional 3d network for real-time action recognition. In Proceedings of the AAAI conference on artificial intelligence , volume 32, 2018.</p>
</li>
<li>
<p>[20] K. Liu and H. Ma. Exploring background-bias for anomaly detection in surveillance videos. In Proceedings of the 27th ACM International Conference on Multimedia, pages 1490–1499, 2019.</p>
</li>
<li>
<p>[21] W. Liu, W. Luo, D. Lian, and S. Gao. Future frame prediction for anomaly detection–a new baseline. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6536–6545, 2018.</p>
</li>
<li>
<p>[22] Y. Liu, D. Yang, Y. Wang, J. Liu, J. Liu, A. Boukerche, P. Sun, and L. Song. Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models. ACM Computing Surveys, 56(7):1–38, 2024.</p>
</li>
<li>
<p>[23] Z. Liu, Z. Sun, Y. Zang, X. Dong, Y. Cao, H. Duan, D. Lin, and J. Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025.</p>
</li>
<li>
<p>[24] C. Lu, J. Shi, and J. Jia. Abnormal event detection at 150 fps in matlab. In Proceedings of the IEEE international conference on computer vision, pages 2720–2727, 2013.</p>
</li>
<li>
<p>[25] Y. Lu, F. Yu, M. K. K. Reddy, and Y. Wang. Few-shot scene-adaptive anomaly detection. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16, pages 125–141. Springer, 2020.</p>
</li>
<li>
<p>[26] H. Lv and Q. Sun. Video anomaly detection and explanation via large language models. arXiv preprint arXiv:2401.05702, 2024.</p>
</li>
<li>
<p>[27] G. Pang, C. Shen, L. Cao, and A. V. D. Hengel. Deep learning for anomaly detection: A review. ACM computing surveys (CSUR), 54(2):1–38, 2021.</p>
</li>
<li>
<p>[28] B. Ramachandra and M. Jones. Street scene: A new dataset and evaluation protocol for video anomaly detection. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2569–2578, 2020.</p>
</li>
<li>
<p>[29] R. Rodrigues, N. Bhargava, R. Velmurugan, and S. Chaudhuri. Multi-timescale trajectory prediction for abnormal human activity detection. In The IEEE Winter Conference on Applications of Computer Vision (WACV), March 2020.</p>
</li>
<li>
<p>[30] Y. Shao, H. He, S. Li, S. Chen, X. Long, F. Zeng, Y. Fan, M. Zhang, Z. Yan, A. Ma, et al. Eventvad: Training-free event-aware video anomaly detection. arXiv preprint arXiv:2504.13092 , 2025.</p>
</li>
<li>
<p>[31] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.</p>
</li>
<li>
<p>[32] K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. Advances in neural information processing systems, 27, 2014.</p>
</li>
<li>
<p>[33] W. Sultani, C. Chen, and M. Shah. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6479–6488, 2018.</p>
</li>
<li>
<p>[34] H. Tan, Y. Ji, X. Hao, M. Lin, P. Wang, Z. Wang, and S. Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025.</p>
</li>
<li>
<p>[35] J. Tang, H. Lu, R. Wu, X. Xu, K. Ma, C. Fang, B. Guo, J. Lu, Q. Chen, and Y. Chen. Hawk: Learning to understand open-world video anomalies. Advances in Neural Information Processing Systems, 37:139751–139785, 2024.</p>
</li>
<li>
<p>[36] Y. Tian, G. Pang, Y. Chen, R. Singh, J. W. Verjans, and G. Carneiro. Weakly-supervised video anomaly detection with robust temporal feature magnitude learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4975–4986, 2021.</p>
</li>
<li>
<p>[37] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 4489–4497, 2015.</p>
</li>
<li>
<p>[38] M. Vijay, W.-X. LI, B. Viral, and V. Nuno. Anomaly detection in crowded scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1975–1981, 2010.</p>
</li>
<li>
<p>[39] L. Wang, W. Li, W. Li, and L. Van Gool. Appearance-and-relation networks for video classification. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1430–1439, 2018.</p>
</li>
<li>
<p>[40] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European conference on computer vision, pages 20–36. Springer, 2016.</p>
</li>
<li>
<p>[41] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin. Qwen2-vl: Enhancing vision-language model&rsquo;s perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024.</p>
</li>
<li>
<p>[42] W. Wang, Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, J. Zhu, X. Zhu, L. Lu, Y. Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024.</p>
</li>
<li>
<p>[43] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7794–7803, 2018.</p>
</li>
<li>
<p>[44] P. Wu, C. Pan, Y. Yan, G. Pang, P. Wang, and Y. Zhang. Deep learning for video anomaly detection: A review. arXiv preprint arXiv:2409.05383, 2024.</p>
</li>
<li>
<p>[45] P. Wu, X. Zhou, G. Pang, Y. Sun, J. Liu, P. Wang, and Y. Zhang. Open-vocabulary video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18297–18307, 2024.</p>
</li>
<li>
<p>[46] P. Wu, X. Zhou, G. Pang, Z. Yang, Q. Yan, P. Wang, and Y. Zhang. Weakly supervised video anomaly detection and localization with spatio-temporal prompts. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 9301–9310, 2024.</p>
</li>
<li>
<p>[47] P. Wu, X. Zhou, G. Pang, L. Zhou, Q. Yan, P. Wang, and Y. Zhang. Vadclip: Adapting visionlanguage models for weakly supervised video anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 6074–6082, 2024.</p>
</li>
<li>
<p>[48] Y. Yang, K. Lee, B. Dariush, Y. Cao, and S.-Y. Lo. Follow the rules: reasoning for video anomaly detection with large language models. In European Conference on Computer Vision , pages 304–322. Springer, 2024.</p>
</li>
<li>
<p>[49] M. Ye, W. Liu, and P. He. Vera: Explainable video anomaly detection via verbalized learning of vision-language models. arXiv preprint arXiv:2412.01095, 2024.</p>
</li>
<li>
<p>[50] T. Yuan, X. Zhang, K. Liu, B. Liu, C. Chen, J. Jin, and Z. Jiao. Towards surveillance videoand-language understanding: New dataset baselines and challenges. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22052–22061, 2024.</p>
</li>
<li>
<p>[51] L. Zanella, W. Menapace, M. Mancini, Y. Wang, and E. Ricci. Harnessing large language models for training-free video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18527–18536, 2024.</p>
</li>
<li>
<p>[52] H. Zhang, X. Xu, X. Wang, J. Zuo, C. Han, X. Huang, C. Gao, Y. Wang, and N. Sang. Holmesvad: Towards unbiased and explainable video anomaly detection via multi-modal llm. arXiv preprint arXiv:2406.12235, 2024.</p>
</li>
<li>
<p>[53] H. Zhang, X. Xu, X. Wang, J. Zuo, X. Huang, C. Gao, S. Zhang, L. Yu, and N. Sang. Holmesvau: Towards long-term video anomaly understanding at any granularity. arXiv preprint arXiv:2412.06171, 2024.</p>
</li>
<li>
<p>[54] H. Zhou, X. Li, R. Wang, M. Cheng, T. Zhou, and C.-J. Hsieh. R1-zero&rsquo;s&quot; aha moment&quot; in visual reasoning on a 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025.</p>
</li>
<li>
<p>[55] H. Zhou, J. Yu, and W. Yang. Dual memory units with uncertainty regulation for weakly supervised video anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 3769–3777, 2023.</p>
</li>
<li>
<p>[56] L. Zhu, L. Wang, A. Raj, T. Gedeon, and C. Chen. Advancing video anomaly detection: A concise review and a new dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024.</p>
</li>
</ul>

<h2 class="relative group">A Further Dataset Details
    <div id="a-further-dataset-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#a-further-dataset-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure 5: More dataset statistics of our VAU-Bench. (a) Distribution of training, validation, and test splits across the four tasks included in VAU-Bench. (b) Word cloud visualization of frequent terms appearing in the multiple-choice questions and choices.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000004_7ff91fbe07e5d749b92fcaef0f22f51c0864591063f0a002a3e69f02ab97b1c9.png"
    ></figure>
<p>Dataset Annotation. VAU-Bench is constructed from three datasets: UCF-Crime, ECVA, and MSAD. While UCF-Crime [33] and ECVA [8] provide basic scene-level descriptions, they lack the structured annotations necessary for fine-grained reasoning. To address this, we leverage DeepSeekV3 [18], a powerful large language model, to enrich the existing annotations from HIVAU-70K (which includes UCF-Crime) [53] and ECVA [8]. We use prompt-based instruction to guide the model in extracting key events, causal relationships, and anomalous behaviors, thereby producing reasoning-oriented annotations suitable for causal understanding. The detailed prompt design is provided in the blue-colored box below.</p>

<h2 class="relative group">Video Understanding Prompt.
    <div id="video-understanding-prompt" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#video-understanding-prompt" aria-label="Anchor">#</a>
    </span>
    
</h2>
<pre tabindex="0"><code>You are an expert in video understanding and reasoning. I will give you structured metadata for a surveillance or behavior-related video. Your task is twofold: Please analyze the entire video description, including anomaly labels, events, and all textual summaries. Based on this, generate a comprehensive summary of what happens in the video in the following JSON structure:
</code></pre><pre tabindex="0"><code>{ &#34;judgement&#34;: &#34;Does this video depict an anomaly? If yes, what is it called?&#34;, &#34;description&#34;: &#34;Chronological and factual summary of what happened in the video&#34;, &#34;analysis&#34;: { &#34;Specific Anomaly Type&#34;: &#34;Select from the [Anomaly Type]&#34;, &#34;Location&#34;: &#34;Where the event occurs: indoor/outdoor/specific&#34;, &#34;Key Evidence&#34;: &#34;Key actions or objects that support classification&#34;, &#34;Detailed Explanation&#34;: &#34;Why these events are normal/anomalous&#34;, &#34;Cause and Effect&#34;: &#34;What led to the event and its outcome&#34;, &#34;Conclusion&#34;: &#34;Wrap-up reasoning with final conclusion about the event&#34; } }
</code></pre>
<h2 class="relative group">Generating QA Pair Prompt.
    <div id="generating-qa-pair-prompt" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#generating-qa-pair-prompt" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>You are an expert in reasoning-focused QA generation for surveillance analysis videos. You will be given a structured video summary, including: (i) A judgement (whether the video is anomalous or normal). (ii) A chronological description of what happens in the video. (iii) A multi-part analysis that breaks down the event&rsquo;s anomaly type, location, key evidence, explanation, causes, and conclusion. Please generate a single multiple-choice question-answer pair in JSON format.</p>
<p>For the MSAD [56] dataset, which lacks textual annotations, we design a structured Chain-of-Thought (CoT) annotation pipeline. We first use InternVL2.5-8B-MPO [42] as the Vision-Language Model (VLM) to generate initial annotations that include detailed descriptions, step-by-step reasoning, and anomaly classification. To further improve the quality of these annotations, we apply DeepSeek-V3 in a secondary refinement stage, which enhances the coherence and clarity of the generated descriptions, QA pairs, and reasoning chains. The overall annotation pipeline consists of the following stages:</p>
<ul>
<li>Task Definition: The VLM is instructed to act as an anomaly detector.</li>
<li>Video Description: The VLM generates a detailed description of the video content.</li>
<li>Step-by-Step Reasoning: The VLM performs multi-step reasoning to analyze the presence and nature of anomalies.</li>
<li>Verification: Given the ground-truth anomaly type, the VLM verifies whether its prediction aligns with it. If not, it regenerates both the description and reasoning.</li>
<li>Key Object Summarization: The VLM identifies key visual objects or cues relevant to the anomaly, expressed in 1–3 words.</li>
<li>QA Generation: The VLM constructs multiple-choice questions by generating and shuffling plausible anomaly-related answer options.</li>
<li>Quality Enhancement: We use DeepSeek-V3 to validate and refine the generated QA pairs, descriptions, and reasoning chains.</li>
</ul>
<p>After completing the CoT annotation for the entire VAU-Bench, we perform a manual review to ensure the accuracy and consistency of all generated annotations.</p>
<p>More Dataset Statistics. Table 4 presents a detailed comparison of our VAU-Bench and existing video anomaly datasets. Compared to previous datasets, our benchmark offers a longer total video duration, a more diverse set of primary anomaly types (with similar categories merged), diverse multi-choice QA pairs, and richer Chain-of-Thought reasoning annotations. Figure 5a shows the dataset splits across four tasks. Each task contains a balanced number of training, validation, and test samples, supporting robust evaluation. Figure 5b presents a word cloud of frequent phrases extracted from the multiple-choice questions and answers in VAU-Bench. Notably, the presence of phrases such as &ldquo;best describes&rdquo; , &ldquo;plausible explanation&rdquo;, and &ldquo;behavioral clue&rdquo; highlights the variety of question formulations, encouraging models to engage in fine-grained interpretation. In addition, keywords such as robbery , man action, and scene indicate that our questions are intentionally crafted to guide models toward recognizing specific objects and anomaly types in complex real-world scenarios.</p>
<p>Dataset Examples. We present representative examples from our VAU-Bench, each annotated to support four core tasks of video anomaly understanding. As illustrated in Figure 7, each example is richly labeled with a question-answer pair, key visual evidence, anomaly type, temporal annotation, and a multi-part reasoning chain that includes location, cause and effect, and a high-level conclusion. This annotation format enables models not only to detect and classify anomalies, but also to explain them in a structured, interpretable manner. Figure 7 and Figure 8 show challenging anomaly scenarios, while Figure 9 depicts a normal scene, included to test model robustness and reduce false positives. These examples demonstrate the breadth and depth of our annotations, enabling holistic evaluation across perception and reasoning dimensions.</p>

<h2 class="relative group">B Experiment Details
    <div id="b-experiment-details" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#b-experiment-details" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Training Details. We use the Adam optimizer with a learning rate of 2 × 10 − 5 . The supervised fine-tuning (SFT) stage runs for less steps (e.g. 200) to avoid overfitting, while the Reinforcement Fine-Tuning (RFT) stage takes approximately 15 hours for 1.5k steps. We set the hyperparameter β in the KL divergence term of the GRPO to 0.04, using M = 4 candidate outputs per prompt. The maximum response length is capped at 1024 tokens.</p>
<p>Table 4: Comparison of video anomaly detection benchmarks. We compare VAU-Bench with existing datasets in terms of size, annotation granularity, and reasoning capabilities. VAU-Bench is the first benchmark to support structured reasoning via multiple-choice questions and Chain-of-Thought (CoT) annotations. Columns indicate whether each dataset provides QA pairs, free-text descriptions (Descrip.), anomaly judgement (Judge.), reasoning (Reason.), and full CoT rationales.</p>
<table>
  <thead>
      <tr>
          <th>Dataset</th>
          <th>Year #Videos Total Len. #Type</th>
          <th>Year #Videos Total Len. #Type</th>
          <th>Year #Videos Total Len. #Type</th>
          <th>Year #Videos Total Len. #Type</th>
          <th>Annotation QA Pairs Descrip. Judge. Reason. CoT</th>
          <th>Annotation QA Pairs Descrip. Judge. Reason. CoT</th>
          <th>Annotation QA Pairs Descrip. Judge. Reason. CoT</th>
          <th>Annotation QA Pairs Descrip. Judge. Reason. CoT</th>
          <th>Annotation QA Pairs Descrip. Judge. Reason. CoT</th>
          <th>Annotation QA Pairs Descrip. Judge. Reason. CoT</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>UCSD Ped1 [38]</td>
          <td>2010</td>
          <td>70</td>
          <td>0.1h</td>
          <td>5</td>
          <td>Bounding-box</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
      </tr>
      <tr>
          <td>UCSD Ped2 [38]</td>
          <td>2010</td>
          <td>28</td>
          <td>0.1h</td>
          <td>5</td>
          <td>Bounding-box</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
      </tr>
      <tr>
          <td>CUHK Avenue [24]</td>
          <td>2013</td>
          <td>35</td>
          <td>0.5h</td>
          <td>5</td>
          <td>Bounding-box</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
      </tr>
      <tr>
          <td>ShanghaiTech [21]</td>
          <td>2017</td>
          <td>437</td>
          <td>3.5h</td>
          <td>13</td>
          <td>Bounding-box</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
      </tr>
      <tr>
          <td>UCF-Crime [33]</td>
          <td>2018</td>
          <td>1900</td>
          <td>128.0h</td>
          <td>13</td>
          <td>Frame</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
      </tr>
      <tr>
          <td>Street Scene [28]</td>
          <td>2020</td>
          <td>81</td>
          <td>3.8h</td>
          <td>17</td>
          <td>Bounding-box</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%  %</td>
          <td>%</td>
      </tr>
      <tr>
          <td>IITB Corridor [29]</td>
          <td>2020</td>
          <td>358</td>
          <td>2.0h</td>
          <td>10</td>
          <td>Frame</td>
          <td>%</td>
          <td>%</td>
          <td>%  %</td>
          <td>%  %</td>
          <td>% %</td>
      </tr>
      <tr>
          <td>UBNormal [1]</td>
          <td>2022</td>
          <td>543</td>
          <td>2.2h</td>
          <td>22</td>
          <td>Frame</td>
          <td>%</td>
          <td>%</td>
          <td>%  %</td>
          <td>%</td>
          <td>%</td>
      </tr>
      <tr>
          <td>NWPU [4]</td>
          <td>2023</td>
          <td>547</td>
          <td>16.3h</td>
          <td>43</td>
          <td>Frame</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
      </tr>
      <tr>
          <td>MSAD [56]</td>
          <td>2024</td>
          <td>720</td>
          <td>4.1h</td>
          <td>11</td>
          <td>Frame</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
      </tr>
      <tr>
          <td>UCA [50]</td>
          <td>2024</td>
          <td>1854</td>
          <td>121.9h</td>
          <td>13 Time Duration</td>
          <td>13 Time Duration</td>
          <td>%</td>
          <td>!</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
      </tr>
      <tr>
          <td>CUVA [9]</td>
          <td>2024</td>
          <td>1000</td>
          <td>32.5h</td>
          <td>11 Time Duration</td>
          <td>11 Time Duration</td>
          <td>!</td>
          <td>!</td>
          <td>!</td>
          <td>!</td>
          <td>%</td>
      </tr>
      <tr>
          <td>ECVA [8]</td>
          <td>2024</td>
          <td>2240</td>
          <td>88.2h</td>
          <td>21 Time Duration</td>
          <td>21 Time Duration</td>
          <td>!</td>
          <td>!</td>
          <td>!</td>
          <td>!</td>
          <td>%</td>
      </tr>
      <tr>
          <td>HIVAU-70K [53]</td>
          <td>2025</td>
          <td>5443</td>
          <td>NA</td>
          <td>NA Time Duration</td>
          <td>NA Time Duration</td>
          <td>!</td>
          <td>!</td>
          <td>%</td>
          <td>%</td>
          <td>%</td>
      </tr>
      <tr>
          <td>VAU–Bench (Ours) 2025</td>
          <td>VAU–Bench (Ours) 2025</td>
          <td>4596</td>
          <td>169.1h</td>
          <td>9 Time</td>
          <td>9 Time Duration</td>
          <td>!</td>
          <td>!</td>
          <td>!</td>
          <td>!</td>
          <td>!</td>
      </tr>
  </tbody>
</table>

<h2 class="relative group">VAU-Eval Prompt.
    <div id="vau-eval-prompt" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#vau-eval-prompt" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Below is a ground-truth description and analysis, followed by a model-generated description and analysis. Please evaluate the model&rsquo;s outputs from the following aspects:</p>
<ol>
<li>Classification Correctness (10 pts)</li>
<li>Key Object and Action Matching (10 pts)</li>
<li>Fluency and Coherence (10 pts)</li>
<li>Informativeness and Domain Awareness (10 pts)</li>
<li>Factual Consistency (10 pts)</li>
</ol>
<p>Evaluation Details for Anomaly Reasoning. To evaluate the alignment between model-generated outputs and our annotated ground truth in video anomaly understanding, we introduce VAU-Eval, a GPT-based evaluation protocol. The evaluation is structured as a multi-turn interaction, where the model first generates a description of the video and then performs reasoning to determine whether the video contains an anomaly. We then use DeepSeek-V3 [18] to assess the similarity between the predicted answers and the ground truth across five aspects: classification correctness, key object and action matching, fluency and coherence, informativeness and domain awareness, and factual consistency. Each aspect is scored out of 10 points, yielding a total of 50 points per sample. To better reflect the model&rsquo;s actual reasoning capabilities, we do not fine-tune the model on any reasoning-style description or analysis. Instead, we directly test models that are trained solely on the multiple-choice QA task, thus ensuring that their descriptive reasoning is not memorized but inferred. The detailed evaluation prompt used in this process is shown in the blue box above.</p>

<h2 class="relative group">C Further Evaluations
    <div id="c-further-evaluations" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#c-further-evaluations" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>More Evaluations. As shown in Table 5, we conduct experiments on the ECVA dataset. Compared to UCF-Crime and MSAD, ECVA poses greater challenges across both recognition and reasoning tasks. All models consistently achieve lower VAU-Eval reasoning scores on ECVA, indicating that its longer videos, more camera movements, viewpoint shifts and richer anomaly diversity make fine-grained understanding more difficult. While our RFT-enhanced models achieve consistent improvements in multiple-choice QA accuracy, their VAU-Eval reasoning scores does not always improve. This suggests that while RFT helps models better predict the final answer, it does not necessarily enhance the reasoning process. These findings highlight the need for more fine-grained reward signals to guide the generation of high-quality rationales in complex scenarios.</p>
<p>Table 5: Comparison of performance on ECVA datasets on multiple-choice QA task and anomaly reasoning task. Accw/o think and Accw/ think refer to the multiple-choice question accuracy without and with thinking, respectively. For the anomaly reasoning task, CLS , KM , FLU , INF, and FAC represent VAU-Eval scores generated by DeepSeek-V3, measuring classification accuracy, key concept alignment, linguistic fluency, informativeness, and factual consistency, respectively. Each dimension is scored on a 10-point scale. Total denotes the aggregated score over five dimensions.</p>
<table>
  <thead>
      <tr>
          <th>Dataset</th>
          <th>Model</th>
          <th>QA Accuracy</th>
          <th>QA Accuracy</th>
          <th>VAU-Eval</th>
          <th>VAU-Eval</th>
          <th>VAU-Eval</th>
          <th>VAU-Eval</th>
          <th>VAU-Eval</th>
          <th>VAU-Eval</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Dataset</td>
          <td>Model</td>
          <td>Accw/o think</td>
          <td>Accw/ think</td>
          <td>CLS↑</td>
          <td>KM↑</td>
          <td>FLU↑</td>
          <td>INF↑</td>
          <td>FAC↑</td>
          <td>Total↑</td>
      </tr>
      <tr>
          <td></td>
          <td>InternVL2.5-2B</td>
          <td>78.84</td>
          <td>58.84</td>
          <td>2.86</td>
          <td>2.78</td>
          <td>7.57</td>
          <td>4.62</td>
          <td>3.03</td>
          <td>20.86</td>
      </tr>
      <tr>
          <td></td>
          <td>Qwen2.5-VL-7B</td>
          <td>83.02</td>
          <td>86.98</td>
          <td>3.70</td>
          <td>3.67</td>
          <td>8.64</td>
          <td>6.40</td>
          <td>4.04</td>
          <td>26.45</td>
      </tr>
      <tr>
          <td></td>
          <td>InternVL2.5-8B-MPO</td>
          <td>90.00</td>
          <td>83.72</td>
          <td>3.4</td>
          <td>3.31</td>
          <td>7.87</td>
          <td>4.48</td>
          <td>3.47</td>
          <td>22.53</td>
      </tr>
      <tr>
          <td></td>
          <td>Qwen2-VL-2B</td>
          <td>86.98</td>
          <td>83.95</td>
          <td>2.41</td>
          <td>2.36</td>
          <td>7.81</td>
          <td>3.81</td>
          <td>2.57</td>
          <td>18.96</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>+SFT</td>
          <td>84.88</td>
          <td>84.65</td>
          <td>2.20</td>
          <td>2.12</td>
          <td>7.37</td>
          <td>3.99</td>
          <td>2.22</td>
          <td>17.90</td>
      </tr>
      <tr>
          <td></td>
          <td>+RFT</td>
          <td>90.23 (↑3.25</td>
          <td>84.42 (↑0.47</td>
          <td>2.26</td>
          <td>2.28</td>
          <td>7.52</td>
          <td>3.70</td>
          <td>2.40</td>
          <td>18.16</td>
      </tr>
      <tr>
          <td></td>
          <td>Qwen2.5-VL-3B</td>
          <td>85.58</td>
          <td>75.81</td>
          <td>2.21</td>
          <td>2.58</td>
          <td>8.33</td>
          <td>5.02</td>
          <td>2.75</td>
          <td>20.89</td>
      </tr>
      <tr>
          <td></td>
          <td>+SFT</td>
          <td>89.30</td>
          <td>86.98</td>
          <td>1.50</td>
          <td>1.22</td>
          <td>4.37</td>
          <td>2.66</td>
          <td>1.24</td>
          <td>10.99</td>
      </tr>
      <tr>
          <td></td>
          <td>+RFT</td>
          <td>89.53 (↑3.95</td>
          <td>86.51 (↑10.7</td>
          <td>1.45</td>
          <td>2.24</td>
          <td>8.05</td>
          <td>4.32</td>
          <td>2.39</td>
          <td>18.45</td>
      </tr>
  </tbody>
</table>
<p>Table 6: Performance of HolmesVAU 2B [53] and our VAU-R1 2B on multiple-choice QA and anomaly reasoning task.</p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Dataset</th>
          <th>QA Accuracy</th>
          <th>QA Accuracy</th>
          <th>VAU-Eval ↑ ↑ AC↑ ↑</th>
          <th>VAU-Eval ↑ ↑ AC↑ ↑</th>
          <th>VAU-Eval ↑ ↑ AC↑ ↑</th>
          <th>VAU-Eval ↑ ↑ AC↑ ↑</th>
          <th>VAU-Eval ↑ ↑ AC↑ ↑</th>
          <th>VAU-Eval ↑ ↑ AC↑ ↑</th>
          <th>VAU-Eval ↑ ↑ AC↑ ↑</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Model</td>
          <td>Dataset</td>
          <td>Accw/o think</td>
          <td>Accw/ think</td>
          <td>k</td>
          <td>CLS↑</td>
          <td>KM↑</td>
          <td>FLU↑</td>
          <td>INF↑</td>
          <td>FAC↑</td>
          <td>Total↑</td>
      </tr>
      <tr>
          <td>HolmesVAU 2B</td>
          <td>MSAD</td>
          <td>85.00</td>
          <td>86.25</td>
          <td></td>
          <td>2.7</td>
          <td>2.72</td>
          <td>6.82</td>
          <td>3.55</td>
          <td>3.33</td>
          <td>20.15</td>
      </tr>
      <tr>
          <td>HolmesVAU 2B</td>
          <td>UCF-Crime</td>
          <td>86.45</td>
          <td>85.66</td>
          <td></td>
          <td>3.05</td>
          <td>1.97</td>
          <td>6.30</td>
          <td>3.08</td>
          <td>2.39</td>
          <td>16.79</td>
      </tr>
      <tr>
          <td>HolmesVAU 2B</td>
          <td>ECVA</td>
          <td>70.47</td>
          <td>70.70</td>
          <td>70</td>
          <td>2.54</td>
          <td>1.71</td>
          <td>6.26</td>
          <td>2.78</td>
          <td>2.30</td>
          <td>15.59</td>
      </tr>
      <tr>
          <td>VAU-R1 2B</td>
          <td>MSAD</td>
          <td>82.92</td>
          <td>83.75</td>
          <td>5 (↓2.5</td>
          <td>6.05</td>
          <td>5.49</td>
          <td>8.89</td>
          <td>6.50</td>
          <td>6.05</td>
          <td>32.98</td>
      </tr>
      <tr>
          <td>VAU-R1 2B</td>
          <td>UCF-Crime</td>
          <td>88.45 (↑</td>
          <td>8.05 (↑2.</td>
          <td>05 (↑2.39</td>
          <td>4.04</td>
          <td>2.75</td>
          <td>7.72</td>
          <td>4.89</td>
          <td>3.11</td>
          <td>22.52</td>
      </tr>
      <tr>
          <td>VAU-R1 2B</td>
          <td>ECVA</td>
          <td>90.23 (↑19</td>
          <td>84.42 (↑13.7</td>
          <td>42 (↑13.72)</td>
          <td>2.26</td>
          <td>2.28</td>
          <td>7.52</td>
          <td>3.70</td>
          <td>2.40</td>
          <td>18.16</td>
      </tr>
  </tbody>
</table>
<p>Comparison with Prior Work. As shown in Table 6, we evaluate HolmesVAU 2B [53], a recently released baseline for VAU, on our benchmark to assess its reasoning capability in complex scenarios. While HolmesVAU 2B achieves reasonable performance across all datasets, it consistently underperforms compared to our Qwen-based models, particularly on the challenging ECVA dataset. This performance gap is evident in both multiple-choice QA accuracy and VAU-Eval reasoning scores, indicating limitations in HolmesVAU 2B&rsquo;s ability to generalize to diverse and complex scenarios. In contrast, VAU-R1 demonstrates stronger alignment with human-annotated reasoning chains and greater robustness across datasets.</p>
<p>Classification Results. Table 7 presents the binary and multi-class anomaly classification accuracy on three datasets: MSAD, UCF-Crime, and ECVA. We directly apply the RFT strategy to train a multi-class anomaly classification task, which includes 19 different anomaly types as well as the normal class. However, directly training the complex multi-class task with RFT degrades performance, suggesting it is more effective to decompose the task into simpler sub-tasks with structured rewards to better guide learning. We compare multiple models under two settings: w/o think and w/ think . We observe that, for the relatively challenging multi-class anomaly task, incorporating an explicit &ldquo;think&rdquo; reasoning step improves the model&rsquo;s classification accuracy.</p>
<p>Temporal Localization Performance. Table 8 summarizes the temporal localization (mIoU) performance of representative methods, categorized into traditional models, multi-modal approaches, and MLLMs. As expected, early appearance-based methods (e.g., Two-stream [32], TSN [40], C3D [37]) achieve limited performance. Incorporating spatio-temporal modeling via 3D convolutions (T-C3D [19], ARTNet [39], 3DResNet [13]) brings moderate improvements, with Liu et al. [20] reaching a mIoU of 16.40. More recent multi-modal approaches, such as VADClip [47] and STPrompt [46], achieve significantly better performance, with STPrompt reaching 23.90 mIoU. Our MLLM-based methods show promising yet limited temporal grounding capabilities. While Qwen2.5-VL-3B achieves only 10.91 mIoU, reinforcement tuning (+RFT) boosts performance to 16.80, indicating that structured reward learning helps align model outputs with temporal structures.</p>
<p>Table 7: Comparison of anomaly classification accuracy on three datasets. Bin. Acc. denotes binary classification accuracy (normal vs. abnormal), and Multi Acc. denotes multi-class accuracy over 19 anomaly types and the normal class. Results are reported with and without think prompting.</p>
<table>
  <thead>
      <tr>
          <th>Dataset</th>
          <th>Model</th>
          <th>w/o think</th>
          <th>w/o think</th>
          <th>w/ think ccMulti Acc</th>
          <th>w/ think ccMulti Acc</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td></td>
          <td>Bin. Acc.</td>
          <td>Multi Acc.</td>
          <td>Bin. Acc.</td>
          <td>Multi Acc.</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>Qwen2-VL-2B-Instruct  Qwen2.5-VL-7B-Instruct  Qwen2.5-VL-3B-Instruct</td>
          <td>75.00  90.00  79.17</td>
          <td>62.50  70.00  69.58</td>
          <td>60.42  75.00  7333</td>
          <td>52.92 66.67</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>Qwen2.5-VL-7B-Instruct</td>
          <td>90.00</td>
          <td>70.00</td>
          <td>75.00</td>
          <td>66.67</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>+ SFT</td>
          <td>70.83</td>
          <td>28.75</td>
          <td>74.58</td>
          <td>56.67 3333</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>+ RFT</td>
          <td>70.83  208 (↑29</td>
          <td>71.25 (↑1.67)</td>
          <td>74.58  74.58 (↑1.25</td>
          <td>60.83 (↑4.16)</td>
      </tr>
      <tr>
          <td>MSAD</td>
          <td>+ RFT</td>
          <td>82.08 (↑2.91)</td>
          <td>71.25 (↑1.67)</td>
          <td>74.58 (↑1.25</td>
          <td>60.83 (↑4.16)</td>
      </tr>
      <tr>
          <td>UCF-Crime</td>
          <td>Qwen2-VL-2B-Instruct  Qwen2.5-VL-7B-Instruct</td>
          <td>60.56  8685</td>
          <td>53.78  6215</td>
          <td>60.16</td>
          <td>51.79</td>
      </tr>
      <tr>
          <td>UCF-Crime</td>
          <td>Qwen2.5-VL-7B-Instruct</td>
          <td>86.85</td>
          <td>62.15</td>
          <td>70.12</td>
          <td>61.35</td>
      </tr>
      <tr>
          <td>UCF-Crime</td>
          <td>Qwen2.5-VL-3B-Instruct</td>
          <td>64.54</td>
          <td>58.57</td>
          <td>62.55</td>
          <td>52.19</td>
      </tr>
      <tr>
          <td>UCF-Crime</td>
          <td>+ SFT  + RFT</td>
          <td>64.14  62.55 (↓1.99</td>
          <td>28.69  57.77 (↓0.80)</td>
          <td>69.32  62.15 (↓0.40</td>
          <td>37.05</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>Qwen2-VL-2B-Instruct  Qwen25-VL-7B-Instruct</td>
          <td>41.95</td>
          <td>24.72  3288</td>
          <td>32.88  4354</td>
          <td>19.05 2381</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>Qwen2.5-VL-7B-Instruct</td>
          <td>64.85</td>
          <td>32.88</td>
          <td>43.54</td>
          <td>23.81</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>Qwen2.5-VL-3B-Instruct</td>
          <td>52.83</td>
          <td>30.16</td>
          <td>49.89</td>
          <td>22.00</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>+ SFT</td>
          <td>96.37</td>
          <td>29.48</td>
          <td>96.15</td>
          <td>28.80</td>
      </tr>
      <tr>
          <td>ECVA</td>
          <td>+ RFT</td>
          <td>49.66 (↓3.17)</td>
          <td>30.61 (↑0.45)</td>
          <td>55.78 (↑5.89)</td>
          <td>31.07 (↑9.07</td>
      </tr>
  </tbody>
</table>
<p>Table 8: Comparison of temporal localization performance (mIoU) across different methods on UCF-Crime dataset.</p>
<table>
  <thead>
      <tr>
          <th>Category</th>
          <th>Method</th>
          <th>Feature</th>
          <th>mIoU</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Multi-moda</td>
          <td>Two-stream [32]</td>
          <td>2.20</td>
          <td>2.2</td>
      </tr>
      <tr>
          <td>Multi-moda</td>
          <td>TSN [40]</td>
          <td>SN</td>
          <td>2.6</td>
      </tr>
      <tr>
          <td>Multi-moda</td>
          <td>C3D [37]</td>
          <td>C3D</td>
          <td>7.2</td>
      </tr>
      <tr>
          <td>Multi-moda</td>
          <td>T-C3D [19]</td>
          <td>C3D</td>
          <td>10.2</td>
      </tr>
      <tr>
          <td>Multi-moda</td>
          <td>ARTNet [39]</td>
          <td>ARTNets</td>
          <td>11.4</td>
      </tr>
      <tr>
          <td>Multi-moda</td>
          <td>3DResNet [13]</td>
          <td>I3D-ResNe</td>
          <td>10.3</td>
      </tr>
      <tr>
          <td>Multi-moda</td>
          <td>NLN [43]</td>
          <td>I3D-ResNe</td>
          <td>12.2</td>
      </tr>
      <tr>
          <td>Multi-moda</td>
          <td>Liu et al. [20]</td>
          <td>I3D-ResNet</td>
          <td>16.4</td>
      </tr>
      <tr>
          <td>Multi-modal</td>
          <td>VADClip [47]</td>
          <td>22.05</td>
          <td>22.05</td>
      </tr>
      <tr>
          <td>Multi-modal</td>
          <td>p  STPrompt [46]</td>
          <td>CLIP</td>
          <td>23.9</td>
      </tr>
      <tr>
          <td>MLLMs</td>
          <td>Qwen2.5-VL-3B</td>
          <td>10.91</td>
          <td>10.91</td>
      </tr>
      <tr>
          <td>MLLMs</td>
          <td>Qwen2.5-VL-3B + RFT</td>
          <td>ViT</td>
          <td>16.8</td>
      </tr>
      <tr>
          <td>MLLMs</td>
          <td>Qwen2.5-VL-7B</td>
          <td>ViT</td>
          <td>22.72</td>
      </tr>
  </tbody>
</table>
<p>However, even with RFT, MLLMs still underperform compared to specialized temporal models, suggesting that current architectures may lack explicit temporal reasoning modules required for fine-grained localization.</p>
<p>Case Study on Anomaly Reasoning. Figure 6 presents a qualitative comparison between outputs generated by SFT and our proposed VAU-R1 model on anomaly reasoning task. Both models are evaluated using the same Chain-of-Thought (CoT) prompt and scored based on five criteria: classification correctness (CLS), key object matching (KM), fluency (FLU), informativeness (INF), and factual consistency (FAC). The SFT output incorrectly identifies the anomaly as a political argument, which does not match the core issue (an escalator malfunction). It also fails to mention any key visual evidence or relevant location. In contrast, VAU-R1 produces a more contextually appropriate response, identifying an emergency situation at a subway station involving injured individuals and emergency vehicles. While the response focuses on surface-level emergency context rather than the root cause, it demonstrates greater fluency and relevance. The evaluation assigns a higher total score of 22, with solid performance across all dimensions, particularly in fluency and informativeness.</p>

<h2 class="relative group">D Limitation and Future Work
    <div id="d-limitation-and-future-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#d-limitation-and-future-work" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>One limitation of this work is its focus on a constrained set of tasks, namely multiple-choice question answering, temporal grounding, anomaly reasoning, and anomaly classification. While these tasks form a strong foundation for video anomaly understanding, there remains substantial room for extension. Future work could incorporate additional tasks such as spatial localization of key objects, which would enable more fine-grained event understanding. Moreover, introducing additional modalities (e.g., audio) may provide complementary cues that enhance both the robustness and contextual depth of anomaly reasoning.</p>

<h2 class="relative group">E Potential Societal Impact
    <div id="e-potential-societal-impact" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#e-potential-societal-impact" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>We propose a new method and benchmark for video anomaly understanding. Accurate and interpretable anomaly understanding systems can contribute to a wide range of safety-critical applications, such as disaster early warning, fire prevention, fall detection, and public safety monitoring. By enabling models to reason about abnormal events, our approach can assist first responders in identifying urgent situations earlier and more reliably.</p>
<p>However, this research inevitably involves scenarios that depict violent or chaotic abnormal behaviors. We strictly follow established ethical guidelines throughout our study. The datasets used in this study are publicly available and have been processed in accordance with the guidelines provided by their original publishers. We strictly adhere to these terms of use and employ the data solely for academic research purposes. To ensure privacy protection, the datasets include safeguards such as reduced video resolution and facial blurring, effectively preventing the identification of individuals. Looking ahead, we plan to explore anomaly understanding methods that incorporate privacy preservation as a core design principle.</p>
<p>Figure 6: Qualitative case of the Anomaly Reasoning task. All correct description and analysis are highlighted in orange. The evaluation results are presented on the right of the answer respectively. Both SFT and VAU-R1 perform inference using the same CoT prompt. VAU-R1&rsquo;s output correctly identifies the anomaly with high fluency but lacks reasoning for the core event, whereas SFT&rsquo;s output is inaccurate and tends to produce repetitive responses.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000005_e7a2f284436da621883332c5742c1e6aa0c9a20566597b2cd03f4911647e194e.png"
    ></figure>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000006_eca921733253d817d650241f19bc50eadc6bbe8f4e0042fd6b5166a80767bcf5.png"
    ></figure>
<p>Figure 7: Example of VAU-Bench. An explosion case in an outdoor backyard, highlighting complex anomaly detection and dynamic scene understanding, labeled with a question-answer pair, key visual evidence, anomaly type, and a multi-part reasoning chain that includes location, cause and effect, and a high-level conclusion.</p>

<h2 class="relative group">Key Object: The perpetrators
    <div id="key-object-the-perpetrators" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#key-object-the-perpetrators" aria-label="Anchor">#</a>
    </span>
    
</h2>
<p>Figure 8: Example of VAU-Bench. A stealing incident, demonstrating capabilities in human activity recognition and intent analysis.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000007_0a8642bddd32605f9bec5c31531f3edfa6dea1b4bb4aba6d4bf754cad6c40666.png"
    ></figure>
<p>Figure 9: Example of VAU-Bench. A normal scene, used to evaluate model robustness against false positives and to enhance dataset diversity.</p>
<figure><img
    class="my-0 rounded-md"
    loading="lazy"
    decoding="async"
    fetchpriority="low"
    alt="Image"
    src="artifacts/image_000008_bdad07224af9b85427e7309135fb630e4508737fccd452fc9c95f41531b722b0.png"
    ></figure>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_papers/VAU-R1 Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning.md"
          data-oid-likes="likes_papers/VAU-R1 Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/video-anomaly-detection-and-explanation-via-large-language-models/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/papers/vane-bench-video-anomaly-evaluation/">
              <span class="leading-6">
                &ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="0001-01-01T00:00:00&#43;00:00">1 January 0001</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      
      
        
      
      
      
      
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400 overflow-x-auto py-2">
        <ul class="flex list-none flex-row">
          
            <li class=" flex mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0 me-4">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href=""
                title="">
                
                
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
